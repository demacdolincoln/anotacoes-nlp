<!DOCTYPE html>
<html prefix="" lang="pt_br">
<head>
<meta charset="utf-8">
<meta name="description" content="Uma pequena ajuda mais prática que técnica ou teórica para quem está aprendendo sobre NLP">
<meta name="viewport" content="width=device-width">
<title>Anotações sobre NLP (Posts antigos, página 1) | Anotações sobre NLP</title>
<link href="assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="assets/css/code.css" rel="stylesheet" type="text/css">
<link href="assets/css/dark.css" rel="stylesheet" type="text/css">
<link href="assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Share+Tech+Mono" rel="stylesheet">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/index-1.html">
<link rel="prev" href="." type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><script src="https://cdn.jsdelivr.net/npm/vega@4.4.0"></script><script src="https://cdn.jsdelivr.net/npm/vega-lite@2.6.0"></script><script src="https://cdn.jsdelivr.net/npm/vega-embed@3.24.2"></script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/languages/python.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/languages/julia.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body class="hack dark">

<a href="#content" class="sr-only sr-only-focusable">Pular para o conteúdo principal</a>
    <div id="container">
         
    <header id="header"><h1 id="brand"><a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/" title="Anotações sobre NLP" rel="home">

        <span id="blog-title">Anotações sobre NLP</span>
    </a></h1>

        

        
    <nav id="menu"><ul>
<li><a href="archive.html">Arquivo</a></li>
                <li><a href="categories/">Etiqueta</a></li>
                <li><a href="rss.xml">Feed RSS</a></li>

    

    
    
    </ul></nav></header><main id="content"><div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/word2vec-2-cbow/" class="u-url">Word2Vec 2: CBOW</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Lincoln de Macêdo
            </span></p>
            <p class="dateline">
            <a href="posts/word2vec-2-cbow/" rel="bookmark">
            <time class="published dt-published" datetime="2018-12-07T00:23:12-03:00" itemprop="datePublished" title="2018-12-07 00:23">2018-12-07 00:23</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/word2vec-2-cbow/#disqus_thread" data-disqus-identifier="cache/posts/word2vec-2-cbow.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Na anotação anterior vimos de forma mais ou menos prática o sentido da coisa, implementamos o Word2Vec com o objetivo de identificar a proximidade semântica entre palavras com base no uso em textos, este post é fundamentalmente teórico e a implementação do cbow aqui demonstrada está muito longe de ser algo pronto para produção, é apenas um exemplo que tenta ser didático.</p>
<div class="section" id="preparacao-dos-dados">
<h2>preparação dos dados</h2>
<p>Como nosso objetivo é fazer com que uma rede neural receba as palavras de contexto e indique a palavra central, e na anotação anterior fiz uma pequena observação dizendo que sempre teremos $2w$ palavras de contexto para cada palavra central e assim faremos, vamos modificar um pouco o código que cria os pares do word2vec:</p>
<table class="codetable"><tr>
<td class="linenos"><div class="linenodiv"><pre><a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-1"> 1</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-2"> 2</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-3"> 3</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-4"> 4</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-5"> 5</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-6"> 6</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-7"> 7</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-8"> 8</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-9"> 9</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-10">10</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-11">11</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-12">12</a>
<a href="posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-13">13</a></pre></div></td>
<td class="code"><pre class="code python"><a name="rest_code_b681d05195e54117935d1c2ab3132375-1"></a><span class="n">window</span> <span class="o">=</span> <span class="mi">2</span>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-2"></a><span class="n">pair_ids</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-3"></a>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-4"></a><span class="n">text_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus_text</span><span class="p">)</span>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-5"></a>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-6"></a><span class="n">corpus_text</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">corpus_text</span><span class="p">)</span>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-7"></a><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">window</span><span class="p">,</span> <span class="n">window</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">])</span>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-8"></a>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-9"></a><span class="k">for</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">window</span><span class="p">,</span> <span class="n">text_size</span><span class="o">-</span><span class="n">window</span><span class="p">):</span>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-10"></a>    <span class="n">center_word_id</span> <span class="o">=</span> <span class="n">word2id</span><span class="p">[</span><span class="n">corpus_text</span><span class="p">[</span><span class="n">center_word</span><span class="p">]]</span>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-11"></a>    <span class="n">context_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2id</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">corpus_text</span><span class="p">[</span><span class="n">mask</span> <span class="o">+</span> <span class="n">center_word</span><span class="p">]]</span>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-12"></a>
<a name="rest_code_b681d05195e54117935d1c2ab3132375-13"></a>    <span class="n">pair_ids</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">context_words</span><span class="p">,</span> <span class="n">center_word_id</span><span class="p">])</span>
</pre></td>
</tr></table>
<p>Assim feito, teremos algo como:</p>
<table border="1" class="docutils">
<colgroup>
<col width="25%">
<col width="25%">
<col width="25%">
<col width="25%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">contexto</th>
<th class="head">central</th>
<th class="head">contexto</th>
<th class="head">central</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>[155, 77, 577, 495]</td>
<td>544</td>
<td>['armazenado', 'ace', 'turing', 'interessou']</td>
<td>posteriormente</td>
</tr>
<tr>
<td>[77, 544, 495, 233]</td>
<td>577</td>
<td>['ace', 'posteriormente', 'interessou', 'química']</td>
<td>turing</td>
</tr>
<tr>
<td>[544, 577, 233, 308]</td>
<td>495</td>
<td>['posteriormente', 'turing', 'química', 'escreveu']</td>
<td>interessou</td>
</tr>
<tr>
<td>[577, 495, 308, 446]</td>
<td>233</td>
<td>['turing', 'interessou', 'escreveu', 'artigo']</td>
<td>química</td>
</tr>
<tr>
<td>[495, 233, 446, 537]</td>
<td>308</td>
<td>['interessou', 'química', 'artigo', 'sobre']</td>
<td>escreveu</td>
</tr>
<tr>
<td>[233, 308, 537, 323]</td>
<td>446</td>
<td>['química', 'escreveu', 'sobre', 'base']</td>
<td>artigo</td>
</tr>
<tr>
<td>[308, 446, 323, 233]</td>
<td>537</td>
<td>['escreveu', 'artigo', 'base', 'química']</td>
<td>sobre</td>
</tr>
<tr>
<td>[446, 537, 233, 504]</td>
<td>323</td>
<td>['artigo', 'sobre', 'química', 'morfogênese']</td>
<td>base</td>
</tr>
<tr>
<td>[537, 323, 504, 506]</td>
<td>233</td>
<td>['sobre', 'base', 'morfogênese', 'previu']</td>
<td>química</td>
</tr>
<tr>
<td>[323, 233, 506, 492]</td>
<td>504</td>
<td>['base', 'química', 'previu', 'reações']</td>
<td>morfogênese</td>
</tr>
<tr>
<td>[233, 504, 492, 8]</td>
<td>506</td>
<td>['química', 'morfogênese', 'reações', 'químicas']</td>
<td>previu</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="a-rede-neural">
<h2>A rede neural</h2>
<p>O que importa na rede neural neste método e no skip-gram é a camada <em>Embedding</em></p>
<table class="codetable"><tr>
<td class="linenos"><div class="linenodiv"><pre><a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-1"> 1</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-2"> 2</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-3"> 3</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-4"> 4</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-5"> 5</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-6"> 6</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-7"> 7</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-8"> 8</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-9"> 9</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-10">10</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-11">11</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-12">12</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-13">13</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-14">14</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-15">15</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-16">16</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-17">17</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-18">18</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-19">19</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-20">20</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-21">21</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-22">22</a>
<a href="posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-23">23</a></pre></div></td>
<td class="code"><pre class="code python"><a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-1"></a><span class="k">class</span> <span class="nc">CBOW</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-2"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">context_size</span><span class="p">):</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-3"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">CBOW</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-4"></a>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-5"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-6"></a>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-7"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear0</span> <span class="o">=</span>  <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">emb_size</span><span class="o">*</span><span class="n">context_size</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-8"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-9"></a>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-10"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">log_softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-11"></a>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-12"></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-13"></a>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-14"></a>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-15"></a>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear0</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-16"></a>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-17"></a>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-18"></a>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-19"></a>        <span class="k">return</span> <span class="n">out</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-20"></a>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-21"></a>    <span class="k">def</span> <span class="nf">get_word_emb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_id</span><span class="p">):</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-22"></a>        <span class="n">word</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">word_id</span><span class="p">])</span>
<a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-23"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></td>
</tr></table>
<p>O treinamento será demorado, afinal como já dito, este não é um código para produção, é apenas um código didático, então enquanto ocorre o treinamento, não é má idéia ir tomar um chá e caminhar um pouco.</p>
<p>Algo que preciso ressaltar aqui é que predizer a palavra central corretamente não importa tanto, o importante é que esteja ocorrendo o aprendizado já queo que nos interessa é que os valores da camada incorporada se aproximem em palavras próximas e se distanciem para palavras distantes, então é de se esperar um gráfico horrível mostrando a evolução da perda.</p>
<img alt="/images/word2vec-cbow-loss.png" src="images/word2vec-cbow-loss.png"><p>Para visualizar a distribuição das palavras num plano cartesiano, faremos o mesmo que com o Gensim, usaremos a implementação do PCA disponível no slearn.</p>
<img alt="/images/word2vec-cbow-1.png" src="images/word2vec-cbow-1.png"><p>Observando a similaridade, que não é lá tão boa neste caso devido a total falta de otimização em tudo no código:</p>
<table border="1" class="docutils">
<colgroup>
<col width="25%">
<col width="25%">
<col width="25%">
<col width="25%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">rank sim cos</th>
<th class="head"> </th>
<th class="head">rank dist eucl</th>
<th class="head"> </th>
</tr></thead>
<tbody valign="top">
<tr>
<td>novas</td>
<td>0.28059</td>
<td>novas</td>
<td>0.09326</td>
</tr>
<tr>
<td>equivalia</td>
<td>0.31309</td>
<td>polonesa</td>
<td>0.09989</td>
</tr>
<tr>
<td>pioneiro</td>
<td>0.31798</td>
<td>neve</td>
<td>0.10029</td>
</tr>
<tr>
<td>afirma</td>
<td>0.32445</td>
<td>andrew</td>
<td>0.10191</td>
</tr>
<tr>
<td>neve</td>
<td>0.33447</td>
<td>pioneiro</td>
<td>0.10310</td>
</tr>
<tr>
<td>polonesa</td>
<td>0.33585</td>
<td>afirma</td>
<td>0.10484</td>
</tr>
<tr>
<td>massachusetts</td>
<td>0.34675</td>
<td>conduzida</td>
<td>0.10508</td>
</tr>
<tr>
<td>conduzida</td>
<td>0.34768</td>
<td>bombas</td>
<td>0.10641</td>
</tr>
<tr>
<td>andrew</td>
<td>0.35143</td>
<td>manipular</td>
<td>0.10718</td>
</tr>
<tr>
<td>hastings</td>
<td>0.35665</td>
<td>homossexuais</td>
<td>0.11074</td>
</tr>
</tbody>
</table>
<p>Observando onde cada termo está com as dimensões da camada incorporada da rede neural reduzida a 2d temos:</p>
<img alt="/images/word2vec-cbow-rank.png" src="images/word2vec-cbow-rank.png"><p>É compreensível ver estas distâncias tão em desarcodo pelo fato das distorções da redução de dimensões, de 10 para 2.</p>
<div class="notebook">
    <a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/word2vec-2-cbow.ipynb">code</a>
</div>
</div>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/word2vec-1-introducao/" class="u-url">Word2Vec 1: Introdução</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Lincoln de Macêdo
            </span></p>
            <p class="dateline">
            <a href="posts/word2vec-1-introducao/" rel="bookmark">
            <time class="published dt-published" datetime="2018-12-06T06:13:12-03:00" itemprop="datePublished" title="2018-12-06 06:13">2018-12-06 06:13</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/word2vec-1-introducao/#disqus_thread" data-disqus-identifier="cache/posts/word2vec-1-introducao.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>O Word2Vec parte de uma idéia muito simples e até certo ponto bastante lógica: relacionar uma palavra com as que estão em sua volta num texto. A partir desse conceito tão básico o Word2Vec acaba sendo uma base para outros algoritmos e não necessariamente um fim em si, a partir dele vamos implementar o cbow e o skip-gram nas anotações seguintes, por hora, vamos entender como funciona a criação dos pares que são a base do Word2Vec.</p>
<div class="section" id="pares">
<h2>Pares</h2>
<p>vamos imaginar que já tenhamos feito todo o processo descrito no post de introdução a esta série. O que buscamos nesta etapa é apenas definir uma "janela" que será a quantidade de palavras vizinhas à uma palavra que chamaremos de central e criar pares ligando essa palavra central às vizinhas, lógico que no código real trabalharemos com ids que representam palavras e não com as palavras em si.</p>
<p>ex.:</p>
<p><cite>O cachorro comeu o trabalho da faculdade de novo</cite></p>
<p>considerando a janela <cite>w = 2</cite> teríamos:</p>
<pre class="code python"><a name="rest_code_9505efc5503e42e496cdc16448991b1a-1"></a><span class="p">[</span>
<a name="rest_code_9505efc5503e42e496cdc16448991b1a-2"></a>    <span class="p">(</span><span class="s2">"comeu"</span><span class="p">,</span> <span class="s2">"o"</span><span class="p">),</span>
<a name="rest_code_9505efc5503e42e496cdc16448991b1a-3"></a>    <span class="p">(</span><span class="s2">"comeu"</span><span class="p">,</span> <span class="s2">"cachorro"</span><span class="p">),</span>
<a name="rest_code_9505efc5503e42e496cdc16448991b1a-4"></a>    <span class="p">(</span><span class="s2">"comeu"</span><span class="p">,</span> <span class="s2">"o"</span><span class="p">),</span>
<a name="rest_code_9505efc5503e42e496cdc16448991b1a-5"></a>    <span class="p">(</span><span class="s2">"comeu"</span><span class="p">,</span> <span class="s2">"trabalho"</span><span class="p">),</span>
<a name="rest_code_9505efc5503e42e496cdc16448991b1a-6"></a>    <span class="o">...</span>
<a name="rest_code_9505efc5503e42e496cdc16448991b1a-7"></a><span class="p">]</span>
</pre>
<p>Coisas óbvias a se deduzir: a partir da palavra central, as vezes que ela aparece é sempre <cite>2*w</cite> e em relação às vizinhas, que chamamos de palavras de contexto, a proporção sempre será de <cite>2*w</cite> para cada palavra central, isso será importante para o cbow e para o skip-gram.</p>
<p>Traduzindo esse procedimento bem básico em código, teremos:</p>
<pre class="code python"><a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-1"></a><span class="n">w</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># janela (window)</span>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-2"></a><span class="n">pair_ids</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-3"></a>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-4"></a><span class="n">text_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-5"></a>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-6"></a><span class="n">corpus_text</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-7"></a><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">])</span>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-8"></a>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-9"></a><span class="k">for</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">text_size</span><span class="o">-</span><span class="n">w</span><span class="p">):</span>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-10"></a>    <span class="n">center_word_id</span> <span class="o">=</span> <span class="n">word2id</span><span class="p">[</span><span class="n">corpus_text</span><span class="p">[</span><span class="n">center_word</span><span class="p">]]</span>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-11"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[</span><span class="n">mask</span> <span class="o">+</span> <span class="n">center_word</span><span class="p">]:</span>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-12"></a>        <span class="n">context_word_id</span> <span class="o">=</span> <span class="n">word2id</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-13"></a>        <span class="n">pair_ids</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">center_word_id</span><span class="p">,</span> <span class="n">context_word_id</span><span class="p">])</span>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-14"></a>
<a name="rest_code_f966f8c8a16343d194788b6b79b1ad53-15"></a><span class="n">pair_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span>
</pre>
<p>Esse será exatamente o código que teremos no método skip-gram. Mas por enquanto vamos aproveitar os métodos que usam o word2vec já implementados e vamos ver o que podemos extrair deles.</p>
</div>
<div class="section" id="gensim">
<h2>Gensim</h2>
<p>No Gensim as operações são muito simples, basta passar para ele o texto processado de acordo com a introdução a este material:</p>
<pre class="code python"><a name="rest_code_872b268f01ef429cb73a3bcb995bb778-1"></a><span class="n">model_sg</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_872b268f01ef429cb73a3bcb995bb778-2"></a><span class="n">model_cb</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre>
<p>No momento de criar o objeto, a única diferença nos parâmetros usados é no <cite>sg</cite> que a essa altura já está claro que signfica skip-gram e em vez de usar True ou False, usamos 1 ou 0 para definir qual método será usado.</p>
<p>A diferença real deles está no input e output pois ambos, cbow e skip-gram, são apenas redes neurais com pouquíssima diferença entre si como será visto posteiormente.</p>
<p>No cbow buscamos predizer a palavra central a partir das palavras de contexto e no skip-gram fazemos o contrário, a partir da palavra central buscamos prever as palavras de contexto.</p>
<img alt="/images/skip-gram_cbow.png" src="images/skip-gram_cbow.png"><pre class="code python"><a name="rest_code_57344aa7a3dc4082ad5a46d65ae12923-1"></a><span class="n">model_sg</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<a name="rest_code_57344aa7a3dc4082ad5a46d65ae12923-2"></a><span class="n">model_cb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre>
<p>Na prática, a função do treinamento é, a partir da proximidade entre as palavras, as camadas da rede neural vão se ajustando o que acaba indicando a proximidade de sentido entre elas, indo para um exemplo clássico queremos que seja possível, através de uma distribuição no plano cartesiano que o meio do caminho entre as palavras "rei" e "mulher" seja "rainha".</p>
<p>## visualizando</p>
<p>Primeiro vamos ver as dimensões na saída para cada palavra:</p>
<pre class="code python"><a name="rest_code_b61ac146ea09431e8db88a2326cd6dd4-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">model_sg</span><span class="p">[</span><span class="s2">"turing"</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<a name="rest_code_b61ac146ea09431e8db88a2326cd6dd4-2"></a><span class="p">(</span><span class="mi">100</span><span class="p">,)</span>
</pre>
<p>Como podemos perceber, nos é impossível fazer uma visualização de algo em 100 dimensões, para reduzi para 2 dimensões vamos usar o sklearn com a classe PCA, como o sklearn mantém o mesmo procedimento para praticamente tudo, vou me abster de colocar o código aqui que pode ser visto no jupyter notebook com o código completo. O importante é que ao final teremos esses gráficos para cada método:</p>
<p>obs: queria fazer algo mais interativo mas não consegui no momento</p>
<img alt="/images/word2vec-1.png" src="images/word2vec-1.png"><p>O Gensim já tem métodos nos objetos formados para encontrar as palavras mais próximas usando a similaridade de cossenos:</p>
<pre class="code python"><a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-1"></a><span class="c1"># repare que quanto mais próximo de 1, mais similar</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-2"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">w</span> <span class="o">=</span> <span class="s2">"cianeto"</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-3"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">model_sg</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-4"></a><span class="p">[(</span><span class="s1">'corpo'</span><span class="p">,</span> <span class="mf">0.9956434965133667</span><span class="p">),</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-5"></a> <span class="p">(</span><span class="s1">'envenenamento'</span><span class="p">,</span> <span class="mf">0.9950364828109741</span><span class="p">),</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-6"></a> <span class="p">(</span><span class="s1">'apesar'</span><span class="p">,</span> <span class="mf">0.9946295022964478</span><span class="p">),</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-7"></a> <span class="p">(</span><span class="s1">'aparente'</span><span class="p">,</span> <span class="mf">0.9940468668937683</span><span class="p">),</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-8"></a> <span class="p">(</span><span class="s1">'presença'</span><span class="p">,</span> <span class="mf">0.9939732551574707</span><span class="p">),</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-9"></a> <span class="p">(</span><span class="s1">'descoberto'</span><span class="p">,</span> <span class="mf">0.9937050342559814</span><span class="p">),</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-10"></a> <span class="p">(</span><span class="s1">'níveis'</span><span class="p">,</span> <span class="mf">0.9936593770980835</span><span class="p">),</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-11"></a> <span class="p">(</span><span class="s1">'quanto'</span><span class="p">,</span> <span class="mf">0.993450403213501</span><span class="p">),</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-12"></a> <span class="p">(</span><span class="s1">'testada'</span><span class="p">,</span> <span class="mf">0.9933900833129883</span><span class="p">),</span>
<a name="rest_code_0b53f65ef7ab4616af68d153a0d15180-13"></a> <span class="p">(</span><span class="s1">'determinar'</span><span class="p">,</span> <span class="mf">0.9930295944213867</span><span class="p">)]</span>
</pre>
<p>Agora comparando o CBOW e o Skip-Gram:</p>
<pre class="code python"><a name="rest_code_13e13d41ecf944d9b5e6853da20117fe-1"></a><span class="n">w</span> <span class="o">=</span> <span class="s2">"morte"</span>
<a name="rest_code_13e13d41ecf944d9b5e6853da20117fe-2"></a>
<a name="rest_code_13e13d41ecf944d9b5e6853da20117fe-3"></a><span class="n">sg_similar</span> <span class="o">=</span> <span class="n">model_sg</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<a name="rest_code_13e13d41ecf944d9b5e6853da20117fe-4"></a><span class="n">cb_similar</span> <span class="o">=</span> <span class="n">model_cb</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<a name="rest_code_13e13d41ecf944d9b5e6853da20117fe-5"></a>
<a name="rest_code_13e13d41ecf944d9b5e6853da20117fe-6"></a><span class="n">md</span> <span class="o">=</span> <span class="s2">"| skip-gram | cbow |</span><span class="se">\n</span><span class="s2">|--|--|</span><span class="se">\n</span><span class="s2">"</span>
<a name="rest_code_13e13d41ecf944d9b5e6853da20117fe-7"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sg_similar</span><span class="p">,</span> <span class="n">cb_similar</span><span class="p">):</span>
<a name="rest_code_13e13d41ecf944d9b5e6853da20117fe-8"></a>    <span class="n">md</span> <span class="o">+=</span> <span class="n">f</span><span class="s2">"| {i[0][0]} |  {i[1][0]} |</span><span class="se">\n</span><span class="s2">"</span>
<a name="rest_code_13e13d41ecf944d9b5e6853da20117fe-9"></a>
<a name="rest_code_13e13d41ecf944d9b5e6853da20117fe-10"></a><span class="n">Markdown</span><span class="p">(</span><span class="n">md</span><span class="p">)</span>
</pre>
<table border="1" class="docutils">
<colgroup>
<col width="59%">
<col width="41%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">skip-gram</th>
<th class="head">cbow</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>causa</td>
<td>turing</td>
</tr>
<tr>
<td>defende</td>
<td>maçã</td>
</tr>
<tr>
<td>setembro</td>
<td>suicídio</td>
</tr>
<tr>
<td>acidental</td>
<td>após</td>
</tr>
<tr>
<td>estabeleceu</td>
<td>cianeto</td>
</tr>
<tr>
<td>campanha</td>
<td>computador</td>
</tr>
<tr>
<td>necessariamente</td>
<td>onde</td>
</tr>
<tr>
<td>copeland</td>
<td>ser</td>
</tr>
<tr>
<td>suicídio</td>
<td>anos</td>
</tr>
<tr>
<td>resultado</td>
<td>ter</td>
</tr>
</tbody>
</table>
<div class="notebook">
    <a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/word2vec-1-introducao.ipynb">code</a>
</div>
</div>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/pre-processamento-de-textos/" class="u-url">Pré-processamento de textos</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Lincoln de Macêdo
            </span></p>
            <p class="dateline">
            <a href="posts/pre-processamento-de-textos/" rel="bookmark">
            <time class="published dt-published" datetime="2018-12-06T03:03:53-03:00" itemprop="datePublished" title="2018-12-06 03:03">2018-12-06 03:03</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/pre-processamento-de-textos/#disqus_thread" data-disqus-identifier="cache/posts/pre-processamento-de-textos.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Este é o processo padrão usado em praticamente todas as anotações relacionadas à NLP:</p>
<ol class="arabic simple">
<li>limpar o texto:<ul>
<li>remover pontuação, acentos, e stop-words <a class="footnote-reference" href="posts/pre-processamento-de-textos/#id2" id="id1">[1]</a>
</li>
<li>colocar tudo em minúsculas</li>
</ul>
</li>
<li>converter numa lista de termos usados.</li>
</ol>
<p>A única excessão é com o TF-IDF e LSA e comparo quando o processamento é feito dividindo em parágrafos e com o texto inteiro de uma vez.</p>
<p>Sempre usarei textos da wikipédia, pelo simples motivo de ser muito prático e inteiramente legal.</p>
<div class="section" id="bibliotecas-usadas">
<h2>bibliotecas usadas</h2>
<pre class="code python"><a name="rest_code_ba7c2f45a518477ead457b090ef36a07-1"></a><span class="o">%</span><span class="n">pylab</span> <span class="n">inline</span>
<a name="rest_code_ba7c2f45a518477ead457b090ef36a07-2"></a><span class="kn">import</span> <span class="nn">nltk</span>
<a name="rest_code_ba7c2f45a518477ead457b090ef36a07-3"></a><span class="kn">import</span> <span class="nn">gensim</span>
<a name="rest_code_ba7c2f45a518477ead457b090ef36a07-4"></a><span class="kn">import</span> <span class="nn">wikipedia</span>
</pre>
<p>No Jupyter notebook o comando <strong>%pylab</strong> importa o matplotlib e numpy e configura o modo como os gráficos serão apresentados, ocasionalmente também usarei o Altair junto com o Pandas para visualizar os dados.</p>
</div>
<div class="section" id="pre-processamento">
<h2>Pré-processamento</h2>
<table class="codetable"><tr>
<td class="linenos"><div class="linenodiv"><pre><a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-1"> 1</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-2"> 2</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-3"> 3</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-4"> 4</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-5"> 5</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-6"> 6</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-7"> 7</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-8"> 8</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-9"> 9</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-10">10</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-11">11</a>
<a href="posts/pre-processamento-de-textos/#rest_code_086bb275912d488faebb0a62db90bd0e-12">12</a></pre></div></td>
<td class="code"><pre class="code python"><a name="rest_code_086bb275912d488faebb0a62db90bd0e-1"></a><span class="n">wikipedia</span><span class="o">.</span><span class="n">set_lang</span><span class="p">(</span><span class="s2">"pt"</span><span class="p">)</span>
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-2"></a><span class="n">text</span> <span class="o">=</span> <span class="n">wikipedia</span><span class="o">.</span><span class="n">page</span><span class="p">(</span><span class="s2">"Alan_Turing"</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-3"></a>
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-4"></a><span class="n">sentences</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-5"></a>
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-6"></a><span class="n">stop_words</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">"portuguese"</span><span class="p">)</span> <span class="o">+</span>\
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-7"></a>             <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">"english"</span><span class="p">)</span>
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-8"></a>
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-9"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">splitlines</span><span class="p">():</span>
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-10"></a>    <span class="n">clean_text</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">simple_preprocess</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-11"></a>    <span class="n">clean_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">clean_text</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
<a name="rest_code_086bb275912d488faebb0a62db90bd0e-12"></a>    <span class="n">sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
</pre></td>
</tr></table>
<p>Explicando as etapas do código acima:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name">
<col class="field-body">
<tbody valign="top">
<tr class="field">
<th class="field-name">linhas 6 e 7:</th>
<td class="field-body">lista de stop-words, como no texto há termos em inglês, juntei as duas listas (termos em português e em inglês) numa só.</td>
</tr>
<tr class="field">
<th class="field-name">for:</th>
<td class="field-body">
<strong>splitlines</strong> vai dividir o texto em parágrafod e a função <strong>simple_preprocess()</strong> do Gensim remove pontuação e converte tudo para minúsculas, em seguida removo as stop words e por último adiciono o parágrafo à lista de sentenças usadas no texto.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="facilitando-as-coisas">
<h2>Facilitando as coisas</h2>
<p>Para dar maior foco ao que importa, salvei a lista de termos usando o pickle:</p>
<pre class="code python"><a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-1"></a><span class="kn">import</span> <span class="nn">pickle</span>
<a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-2"></a>
<a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-3"></a><span class="c1"># salvando em arquivo</span>
<a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-4"></a><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"sentences.pickle"</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-5"></a>   <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
<a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-6"></a>   <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-7"></a>
<a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-8"></a><span class="c1"># lendo do arquivo</span>
<a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-9"></a><span class="n">sentences</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
<a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-10"></a>   <span class="nb">open</span><span class="p">(</span><span class="s2">"sentences.pickle"</span><span class="p">,</span> <span class="s2">"rb"</span><span class="p">)</span>
<a name="rest_code_ab5c2cc3b9be462f94e3beb74ecd9d9d-11"></a><span class="p">)</span>
</pre>
</div>
<div class="section" id="footnotes">
<h2>footnotes</h2>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="posts/pre-processamento-de-textos/#id1">[1]</a></td>
<td>stop words são as palavras sem valor semântico ao que pretendemos fazer, são palavras como "eu", "está", "era", "têm", etc. São palavras de uso tão comum e frequente que acabaria por ofuscar a presença de palavras mais relevantes no processo de classificação de textos por exemplo, afinal para saber o sentido de frases como "Alan Turing é o pai da ciência da computação" basta apenas as palavras ["Alan", "Turing", "pai", "ciência", "computação"], isso é o que basta para uma máquina.</td>
</tr></tbody>
</table>
<div class="notebook">
    <a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/preprocessing.py">code</a>
</div>
</div>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/" class="u-url">README</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Lincoln de Macêdo
            </span></p>
            <p class="dateline">
            <a href="posts/" rel="bookmark">
            <time class="published dt-published" datetime="2018-12-06T02:46:15-03:00" itemprop="datePublished" title="2018-12-06 02:46">2018-12-06 02:46</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/#disqus_thread" data-disqus-identifier="cache/posts/index.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Pretendo fazer uma longa série de posts sobre NLP, não sou especialista nisso e podemos considerar os posts mais como anotações de estudo do que tutoriais ou manuais. O Índice abaixo será atualizado à medida que eu for publicando novos conteúdos, a idéia é seguir o andamento histórico de cada parte, na 1ª parte começaremos com o tf-idf para depois seguirmos para o word2vec e glove:</p>
<ul class="simple">
<li><dl class="first docutils">
<dt>parte 1: vetorização</dt>
<dd><ul class="first last">
<li><a class="reference external" href="posts/estatistica-tf-idf-e-lsa">estatística: tf-idf</a></li>
<li><a class="reference external" href="posts/word2vec-1-introducao">word2vec 1: introdução</a></li>
<li><a class="reference external" href="posts/word2vec-2-cbow">word2vec 2: cbow</a></li>
<li><a class="reference external" href="posts/word2vec-3-skip-gram">word2vec 3: skip-gram</a></li>
<li><em>glove</em></li>
<li><a class="reference external" href="posts/seq2seq-introducao">seq2seq: introdução</a></li>
<li><em>notas finais e comparações entre métodos</em></li>
</ul></dd>
</dl></li>
<li><dl class="first docutils">
<dt>parte 2: classificação</dt>
<dd><ul class="first last">
<li><a class="reference external" href="posts/classificacao-1">classificação 1: introdução</a></li>
<li><em>classificacao 2: CNN</em></li>
</ul></dd>
</dl></li>
<li><dl class="first docutils">
<dt>parte 3: modelagem</dt>
<dd><ul class="first last">
<li><a class="reference external" href="posts/resumos-0-pagerank">resumos 0: pagerank</a></li>
<li><a class="reference external" href="posts/seq2seq-introducao">seq2seq: introdução</a></li>
<li><em>seq2seq: implementação</em></li>
</ul></dd>
</dl></li>
<li><dl class="first docutils">
<dt>utils</dt>
<dd><ul class="first last">
<li>
<a class="reference external" href="posts/pre-processamento-de-textos">Pré-processamento de textos</a>. (<em>muito importante</em>)</li>
<li><a class="reference external" href="posts/svd-vs-pca">SVD vs PCA</a></li>
<li><a class="reference external" href="posts/distancia-euclidiama-vs-similaridade-de-cossenos">distância euclidiana vs similaridade de cossenos</a></li>
<li><a class="reference external" href="posts/gru-e-lstm">GRU e LSTM</a></li>
</ul></dd>
</dl></li>
</ul>
<p>Obs1.: O pré-processamento é a etapa inicial de praticamente todos os conteúdos aqui escritos, é realmente muito importante, por isso antes de partir para qualquer outro conteúdo, leia ele primeiro.</p>
<p>Obs2.: O que estiver em itálico é que ainda não escrevi mas devo fazer ao longo dessas semanas.</p>
<p>obs3.: Com excessão da parte 1, usarei o cbow, skip-gram e glove já computados, fontes recomendadas:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc">http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc</a></li>
<li><a class="reference external" href="https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md">https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md</a></li>
<li><a class="reference external" href="https://sites.google.com/site/rmyeid/projects/polyglot">https://sites.google.com/site/rmyeid/projects/polyglot</a></li>
</ul>
</div>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="." rel="prev">Posts mais recentes</a>
            </li>
        </ul></nav><script>var disqus_shortname="demacdolincoln";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script></main><footer id="footer"><p>Contents © 2018         <a href="mailto:demacdolincoln@gmail.com">Lincoln de Macêdo</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer>
</div>
    
    

    
    
    
</body>
</html>
