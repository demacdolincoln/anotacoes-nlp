<p>Pretendo fazer uma longa série de posts sobre NLP, não sou especialista nisso e podemos considerar os posts mais como anotações de estudo do que tutoriais ou manuais. O Índice abaixo será atualizado à medida que eu for publicando novos conteúdos, a idéia é seguir o andamento histórico de cada parte, na 1ª parte começaremos com o tf-idf para depois seguirmos para o word2vec e glove:</p>
<ul class="simple">
<li><dl class="first docutils">
<dt>parte 1: vetorização</dt>
<dd><ul class="first last">
<li><a class="reference external" href="link://filename/posts/estatistica-tf-idf-e-lsa.rst">estatística: tf-idf</a></li>
<li><a class="reference external" href="link://filename/posts/word2vec-1-introducao.rst">word2vec 1: introdução</a></li>
<li><a class="reference external" href="link://filename/posts/word2vec-2-cbow.rst">word2vec 2: cbow</a></li>
<li><a class="reference external" href="link://filename/posts/word2vec-3-skip-gram.rst">word2vec 3: skip-gram</a></li>
<li><em>glove</em></li>
<li><em>seq2seq</em></li>
<li><em>notas finais e comparações entre métodos</em></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>utils</dt>
<dd><ul class="first last">
<li><a class="reference external" href="link://filename/posts/pre-processamento-de-textos.rst">Pré-processamento de textos</a>. (<em>muito importante</em>)</li>
<li><a class="reference external" href="link://filename/posts/svd-vs-pca.rst">SVD vs PCA</a></li>
<li><a class="reference external" href="link://filename/posts/distancia-eucliciana-vs-similaridade-de-cossenos.rst">distância euclidiana vs similaridade de cossenos</a></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Obs1.: O pré-processamento é a etapa inicial de praticamente todos os conteúdos aqui escritos, é realmente muito importante, por isso antes de partir para qualquer outro conteúdo, leia ele primeiro.</p>
<p>Obs2.: O que estiver em itálico é que ainda não escrevi mas devo fazer ao longo dessas semanas. As partes 2 e 3 possivelmente serão sobre classificação de textos e modelagem com cadeias de markov e RNN.</p>
