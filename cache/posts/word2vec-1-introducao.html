<p>O Word2Vec parte de uma idéia muito simples e até certo ponto bastante lógica: relacionar uma palavra com as que estão em sua volta num texto. A partir desse conceito tão básico o Word2Vec acaba sendo uma base para outros algoritmos e não necessariamente um fim em si, a partir dele vamos implementar o cbow e o skip-gram nas anotações seguintes, por hora, vamos entender como funciona a criação dos pares que são a base do Word2Vec.</p>
<div class="section" id="pares">
<h1>Pares</h1>
<p>vamos imaginar que já tenhamos feito todo o processo descrito no post de introdução a esta série. O que buscamos nesta etapa é apenas definir uma &quot;janela&quot; que será a quantidade de palavras vizinhas à uma palavra que chamaremos de central e criar pares ligando essa palavra central às vizinhas, lógico que no código real trabalharemos com ids que representam palavras e não com as palavras em si.</p>
<p>ex.:</p>
<p><cite>O cachorro comeu o trabalho da faculdade de novo</cite></p>
<p>considerando a janela <cite>w = 2</cite> teríamos:</p>
<pre class="code python"><a name="rest_code_34ee859340754c91be8a05ac4157a1c0-1"></a><span class="p">[</span>
<a name="rest_code_34ee859340754c91be8a05ac4157a1c0-2"></a>    <span class="p">(</span><span class="s2">&quot;comeu&quot;</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">),</span>
<a name="rest_code_34ee859340754c91be8a05ac4157a1c0-3"></a>    <span class="p">(</span><span class="s2">&quot;comeu&quot;</span><span class="p">,</span> <span class="s2">&quot;cachorro&quot;</span><span class="p">),</span>
<a name="rest_code_34ee859340754c91be8a05ac4157a1c0-4"></a>    <span class="p">(</span><span class="s2">&quot;comeu&quot;</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">),</span>
<a name="rest_code_34ee859340754c91be8a05ac4157a1c0-5"></a>    <span class="p">(</span><span class="s2">&quot;comeu&quot;</span><span class="p">,</span> <span class="s2">&quot;trabalho&quot;</span><span class="p">),</span>
<a name="rest_code_34ee859340754c91be8a05ac4157a1c0-6"></a>    <span class="o">...</span>
<a name="rest_code_34ee859340754c91be8a05ac4157a1c0-7"></a><span class="p">]</span>
</pre><p>Coisas óbvias a se deduzir: a partir da palavra central, as vezes que ela aparece é sempre <cite>2*w</cite> e em relação às vizinhas, que chamamos de palavras de contexto, a proporção sempre será de <cite>2*w</cite> para cada palavra central, isso será importante para o cbow e para o skip-gram.</p>
<p>Traduzindo esse procedimento bem básico em código, teremos:</p>
<pre class="code python"><a name="rest_code_be29c7f15d944043b0378438b8c5c18e-1"></a><span class="n">w</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># janela (window)</span>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-2"></a><span class="n">pair_ids</span> <span class="o">=</span> <span class="p">[]</span>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-3"></a>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-4"></a><span class="n">text_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-5"></a>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-6"></a><span class="n">corpus_text</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-7"></a><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">])</span>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-8"></a>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-9"></a><span class="k">for</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">text_size</span><span class="o">-</span><span class="n">w</span><span class="p">):</span>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-10"></a>    <span class="n">center_word_id</span> <span class="o">=</span> <span class="n">word2id</span><span class="p">[</span><span class="n">corpus_text</span><span class="p">[</span><span class="n">center_word</span><span class="p">]]</span>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-11"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[</span><span class="n">mask</span> <span class="o">+</span> <span class="n">center_word</span><span class="p">]:</span>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-12"></a>        <span class="n">context_word_id</span> <span class="o">=</span> <span class="n">word2id</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-13"></a>        <span class="n">pair_ids</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">center_word_id</span><span class="p">,</span> <span class="n">context_word_id</span><span class="p">])</span>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-14"></a>
<a name="rest_code_be29c7f15d944043b0378438b8c5c18e-15"></a><span class="n">pair_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span>
</pre><p>Esse será exatamente o código que teremos no método skip-gram. Mas por enquanto vamos aproveitar os métodos que usam o word2vec já implementados e vamos ver o que podemos extrair deles:</p>
</div>
<div class="section" id="gensim">
<h1>Gensim</h1>
<p>No Gensim as operações são muito simples, basta passar para ele o texto processado de acordo com a introdução a este material:</p>
<pre class="code python"><a name="rest_code_c65c28e3083e4496b547220f1120a1a9-1"></a><span class="n">model_sg</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_c65c28e3083e4496b547220f1120a1a9-2"></a><span class="n">model_cb</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre><p>No momento de criar o objeto, a única diferença nos parâmetros usados é no <cite>sg</cite> que a essa altura já está claro que signfica skip-gram e em vez de usar True ou False, usamos 1 ou 0 para definir qual método será usado.</p>
<p>A diferença real deles está no input e output pois ambos, cbow e skip-gram, são apenas redes neurais com pouquíssima diferença entre si como será visto posteiormente.</p>
<p>No cbow buscamos predizer a palavra central a partir das palavras de contexto e no skip-gram fazemos o contrário, a partir da palavra central buscamos prever as palavras de contexto.</p>
<pre class="code python"><a name="rest_code_a58a3936846244aea4a855d21199a20e-1"></a><span class="n">model_sg</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<a name="rest_code_a58a3936846244aea4a855d21199a20e-2"></a><span class="n">model_cb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre><p>Na prática, a função do treinamento é, a partir da proximidade entre as palavras, as camadas da rede neural vão se ajustando o que acaba indicando a proximidade de sentido entre elas, indo para um exemplo clássico queremos que seja possível, através de uma distribuição no plano cartesiano que o meio do caminho entre as palavras &quot;rei&quot; e &quot;mulher&quot; seja &quot;rainha&quot;.</p>
<p>## visualizando</p>
<p>Primeiro vamos ver as dimensões na saída para cada palavra:</p>
<pre class="code python"><a name="rest_code_b92604287c114e4e859942fa68ee0cdb-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">model_sg</span><span class="p">[</span><span class="s2">&quot;turing&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<a name="rest_code_b92604287c114e4e859942fa68ee0cdb-2"></a><span class="p">(</span><span class="mi">100</span><span class="p">,)</span>
</pre><p>Como podemos perceber, nos é impossível fazer uma visualização de algo em 100 dimensões, para reduzi para 2 dimensões vamos usar o sklearn com a classe PCA, como o sklearn mantém o mesmo procedimento para praticamente tudo, vou me abster de colocar o código aqui que pode ser visto no jupyter notebook com o código completo. O importante é que ao final teremos esses gráficos para cada método:</p>
<p>obs: queria fazer algo mais interativo mas não consegui no momento</p>
<img alt="/images/word2vec-1.png" src="/images/word2vec-1.png" />
<p>O Gensim já tem métodos nos objetos formados para encontrar as palavras mais próximas usando a similaridade de cossenos:</p>
<pre class="code python"><a name="rest_code_a1552e49f79541c793afae2626141435-1"></a><span class="c1"># repare que quanto mais próximo de 1, mais similar</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-2"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">w</span> <span class="o">=</span> <span class="s2">&quot;cianeto&quot;</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-3"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">model_sg</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-4"></a><span class="p">[(</span><span class="s1">&#39;corpo&#39;</span><span class="p">,</span> <span class="mf">0.9956434965133667</span><span class="p">),</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-5"></a> <span class="p">(</span><span class="s1">&#39;envenenamento&#39;</span><span class="p">,</span> <span class="mf">0.9950364828109741</span><span class="p">),</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-6"></a> <span class="p">(</span><span class="s1">&#39;apesar&#39;</span><span class="p">,</span> <span class="mf">0.9946295022964478</span><span class="p">),</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-7"></a> <span class="p">(</span><span class="s1">&#39;aparente&#39;</span><span class="p">,</span> <span class="mf">0.9940468668937683</span><span class="p">),</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-8"></a> <span class="p">(</span><span class="s1">&#39;presença&#39;</span><span class="p">,</span> <span class="mf">0.9939732551574707</span><span class="p">),</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-9"></a> <span class="p">(</span><span class="s1">&#39;descoberto&#39;</span><span class="p">,</span> <span class="mf">0.9937050342559814</span><span class="p">),</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-10"></a> <span class="p">(</span><span class="s1">&#39;níveis&#39;</span><span class="p">,</span> <span class="mf">0.9936593770980835</span><span class="p">),</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-11"></a> <span class="p">(</span><span class="s1">&#39;quanto&#39;</span><span class="p">,</span> <span class="mf">0.993450403213501</span><span class="p">),</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-12"></a> <span class="p">(</span><span class="s1">&#39;testada&#39;</span><span class="p">,</span> <span class="mf">0.9933900833129883</span><span class="p">),</span>
<a name="rest_code_a1552e49f79541c793afae2626141435-13"></a> <span class="p">(</span><span class="s1">&#39;determinar&#39;</span><span class="p">,</span> <span class="mf">0.9930295944213867</span><span class="p">)]</span>
</pre><p>Agora comparando o CBOW e o Skip-Gram:</p>
<pre class="code python"><a name="rest_code_6a9c544dd659482e9eaca4d438f08c33-1"></a><span class="n">w</span> <span class="o">=</span> <span class="s2">&quot;morte&quot;</span>
<a name="rest_code_6a9c544dd659482e9eaca4d438f08c33-2"></a>
<a name="rest_code_6a9c544dd659482e9eaca4d438f08c33-3"></a><span class="n">sg_similar</span> <span class="o">=</span> <span class="n">model_sg</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<a name="rest_code_6a9c544dd659482e9eaca4d438f08c33-4"></a><span class="n">cb_similar</span> <span class="o">=</span> <span class="n">model_cb</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<a name="rest_code_6a9c544dd659482e9eaca4d438f08c33-5"></a>
<a name="rest_code_6a9c544dd659482e9eaca4d438f08c33-6"></a><span class="n">md</span> <span class="o">=</span> <span class="s2">&quot;| skip-gram | cbow |</span><span class="se">\n</span><span class="s2">|--|--|</span><span class="se">\n</span><span class="s2">&quot;</span>
<a name="rest_code_6a9c544dd659482e9eaca4d438f08c33-7"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sg_similar</span><span class="p">,</span> <span class="n">cb_similar</span><span class="p">):</span>
<a name="rest_code_6a9c544dd659482e9eaca4d438f08c33-8"></a>    <span class="n">md</span> <span class="o">+=</span> <span class="n">f</span><span class="s2">&quot;| {i[0][0]} |  {i[1][0]} |</span><span class="se">\n</span><span class="s2">&quot;</span>
<a name="rest_code_6a9c544dd659482e9eaca4d438f08c33-9"></a>
<a name="rest_code_6a9c544dd659482e9eaca4d438f08c33-10"></a><span class="n">Markdown</span><span class="p">(</span><span class="n">md</span><span class="p">)</span>
</pre><table border="1" class="docutils">
<colgroup>
<col width="59%" />
<col width="41%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">skip-gram</th>
<th class="head">cbow</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>causa</td>
<td>turing</td>
</tr>
<tr><td>defende</td>
<td>maçã</td>
</tr>
<tr><td>setembro</td>
<td>suicídio</td>
</tr>
<tr><td>acidental</td>
<td>após</td>
</tr>
<tr><td>estabeleceu</td>
<td>cianeto</td>
</tr>
<tr><td>campanha</td>
<td>computador</td>
</tr>
<tr><td>necessariamente</td>
<td>onde</td>
</tr>
<tr><td>copeland</td>
<td>ser</td>
</tr>
<tr><td>suicídio</td>
<td>anos</td>
</tr>
<tr><td>resultado</td>
<td>ter</td>
</tr>
</tbody>
</table>
<div class="notebook">
    <a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/word2vec-1-introducao.ipynb">code</a>
</div></div>
