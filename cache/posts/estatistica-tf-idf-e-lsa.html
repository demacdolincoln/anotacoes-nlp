<p>Antes da popularidade de métodos baseados em IA, muito também devido à capacidade dos computadores da época, o que restava para análises de texto era quantificar as palavras e buscar extrair estatísticas, o mais básico e fundamental talvez seja o TF-IDF e por isso este post.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">tf-idf:</th><td class="field-body"><em>frequency-inverse document frequency</em></td>
</tr>
</tbody>
</table>
<p>Este método se resume a contar a frequência de uso de palavras e realizar um cálculo que gere uma estimativa de uso/importância da palavra no texto, de certa forma ele se conecta à <a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law">Lei de Zipf</a> que trata justamente de uma análise da frequência de palavras.</p>
<div class="math">
\begin{equation*}
TF(t) = \frac{nº\ de\ vezes\ que\ t\ aparece\ no\ texto}{total\ de\ termos\ no\ texto}
\end{equation*}
</div>
<div class="math">
\begin{equation*}
IDF(t) = log_e(\frac{quantidade\ total\ de\ textos}{numero\ de\ textos\ em\ que\ t\ aparece})
\end{equation*}
</div>
<p>Recomendo bastante a wikipédia em inglês, há bastante exemplos de cálculos variantes: <a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a></p>
<p>Logicamente há inconsistências, afinal apenas a frequência não alcança o uso das palavras, não indica necessariamente as mais significativas se uma pessoa em vez de fazer referências a uma palavra ficar repetindo a mesma coisa o tempo todo. ex.:</p>
<!--  -->
<blockquote>
<p>&quot;há filmes bons, ruins e medianos, mas o filme em questão é o pior de todos, o filme é tão chato e cansativo que todos dormem assistindo os primeiros minutos do filme&quot;</p>
<p>&quot;há filmes bons, ruins e medianos, mas este em questão é o pior de todos, tão chato e cansativo que todos dormem aos primeiros minutos&quot;</p>
</blockquote>
<p>É bem claro que apesar do sentido do texto ser o mesmo, a importância dada à palavra &quot;filme&quot; seria diferente. E de fato, o TF-IDF funciona melhor para textos que seguem as regras de coesão e coerência, então vamos usar publicações da wikipédia.</p>
<p>Apesar do cálculo ser bastante simples, vou preferir usar o sklearn pois neste caso o mais importante é ter uma ideia geral sobre um recurso básico e servir como uma introdução básica sobre NLP, especialmente sobre vertorização de textos</p>
<div class="section" id="tf-idf">
<h1>TF-IDF</h1>
<p>Como quase tudo no sklearn...</p>
<table class="codetable"><tr><td class="linenos"><div class="linenodiv"><pre><a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-1"> 1</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-2"> 2</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-3"> 3</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-4"> 4</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-5"> 5</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-6"> 6</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-7"> 7</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-8"> 8</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-9"> 9</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-10">10</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-11">11</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-12">12</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-13">13</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-14">14</a></pre></div></td><td class="code"><pre class="code python"><a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-1"></a><span class="kn">import</span> <span class="nn">wikipedia</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-2"></a><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-3"></a><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-4"></a>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-5"></a><span class="n">stopw</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;portuguese&quot;</span><span class="p">)</span> <span class="o">+</span>\
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-6"></a>        <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-7"></a>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-8"></a><span class="n">wikipedia</span><span class="o">.</span><span class="n">set_lang</span><span class="p">(</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-9"></a><span class="n">text</span> <span class="o">=</span> <span class="n">wikipedia</span><span class="o">.</span><span class="n">page</span><span class="p">(</span><span class="s2">&quot;Alan_Turing&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-10"></a>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-11"></a><span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="n">stopw</span><span class="p">)</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-12"></a>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-13"></a><span class="n">X</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">splitlines</span><span class="p">())</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-14"></a><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></td></tr></table><p>Na penúltima linha usei o <cite>splitlines</cite> para dividir o texto em parágrafos, assim podemos posteriormente coletar informações sobre os termos relevantes para cada parágrafo, mas admito esta forma ser demasiadamente simplista pois neste caso acabo considerando subtítulos como parágrafos.</p>
<p>Internamente, o objeto que criamos, durante o treinamento, armazena um dicionário com as palavras e um &quot;id&quot;, vamos usar isso para converter os termos:</p>
<pre class="code python"><a name="rest_code_1c0c7dd8f0f34b4382bcf82ab32598f5-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">X</span>
<a name="rest_code_1c0c7dd8f0f34b4382bcf82ab32598f5-2"></a><span class="o">&lt;</span><span class="mi">61</span><span class="n">x664</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s1">&#39;&lt;class &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="s1">&#39;&gt;&#39;</span>
<a name="rest_code_1c0c7dd8f0f34b4382bcf82ab32598f5-3"></a>    <span class="k">with</span> <span class="mi">862</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Row</span> <span class="n">format</span><span class="o">&gt;</span>
</pre><p>A matriz esparsa tem diversas vantagens quando tratamos com longos arrays rechados de zeros, talvez o produto principal nessa implementação seja exatamente essa matriz que indica em cada parágrafo quais os termos presentes e a sua frequência, que é o ponto principal do TF-IDF.</p>
<img alt="visualização da matriz resultante" src="/images/lsa.png" />
<p>E é exatamente sobre essa matriz que chegamos no LSA (Latent Semantic Analysis), mas antes vamos ver quais as palavras mais relevantes do primeiro parágrafo:</p>
<pre class="code python"><a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">ft_name</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-2"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">top_tfidf</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-3"></a><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_tfidf</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-4"></a>        <span class="k">print</span><span class="p">(</span><span class="n">ft_name</span><span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-5"></a>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-6"></a><span class="n">computação</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-7"></a><span class="n">cheshire</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-8"></a><span class="n">junho</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-9"></a><span class="n">ciência</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-10"></a><span class="n">influente</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-11"></a><span class="n">algoritmo</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-12"></a><span class="n">east</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-13"></a><span class="n">lógico</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-14"></a><span class="n">desenvolvimento</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-15"></a><span class="n">desempenhando</span>
</pre><p>O ft_name é a lista de termos que irá converter para string a posição do termo indicada quando ordenamos o array comtendo o valor calculado para cada termo devolvendo as respectivas posições.</p>
</div>
<div class="section" id="lsa">
<h1>LSA</h1>
<p>O LSA é nada mais que usar o <a class="reference external" href="link://filename/posts/svd-vs-pca.rst">SVD</a> mas em vez de diminuir as dimensões vamos manter o tamanho da matriz:</p>
<pre class="code python"><a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-2"></a><span class="p">(</span><span class="mi">61</span><span class="p">,</span> <span class="mi">664</span><span class="p">)</span>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-3"></a>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-4"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">lsa</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">61</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-5"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">lsa</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-6"></a><span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;randomized&#39;</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">61</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-7"></a>   <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre><p>O real poder do LSA vem desse tratamento dado à matriz formada a partir do TF-IDF, o código abaixo indica as palavras mais relevantes para cada parágrafo:</p>
<pre class="code python"><a name="rest_code_83e0731111324c66915b3bc47e48b73f-1"></a><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">comp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lsa</span><span class="o">.</span><span class="n">components_</span><span class="p">):</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-2"></a>    <span class="n">terms_in_comp</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ft_name</span><span class="p">,</span> <span class="n">comp</span><span class="p">)</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-3"></a>    <span class="n">sorted_terms</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">terms_in_comp</span><span class="p">,</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-4"></a>                          <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-5"></a>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-6"></a>    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;paragrafo: {i}&quot;</span><span class="p">)</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-7"></a>    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">sorted_terms</span><span class="p">:</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-8"></a>        <span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-9"></a>    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
</pre><p>Pegando apenas o parágrafo 0, o resultado que temos é:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">paragrafo 0:</th><td class="field-body"><ul class="first last simple">
<li>turing</li>
<li>máquina</li>
<li>alan</li>
<li>prêmio</li>
<li>memorial</li>
<li>guerra</li>
<li>enigma</li>
<li>bletchley</li>
<li>park</li>
<li>computação</li>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="off-topic">
<h1>off-topic</h1>
<ol class="arabic simple">
<li>E para gerar estatísticas de relevância de um texto inteiro, basta não dividir em parágrafos</li>
<li>E para gerarmos aquele bag of words que está na moda temos algumas opções, dependendo do caso aplicamos só o <strong>TF</strong> para gerar um ranking, para outros casos o <strong>TF-IDF</strong> funciona melhor, especialmente quando juntamos vários textos como uma análise geral de várias páginas de blogs, o LSA tende a ser melhor em usos mais específicos porém nada impede de usa-lo para gerar o ranking de termos para um livro, por exemplo.</li>
</ol>
<div class="notebook">
    <a class="notebook-link" href="/files/estatistica-tf-idf-e-lsa.ipynb">code</a>
</div></div>
