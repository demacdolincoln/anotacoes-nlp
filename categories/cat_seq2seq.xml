<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Anotações sobre NLP (Posts sobre seq2seq)</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/</link><description></description><atom:link href="http://demacdolincoln.github.io/anotacoes-nlp/posts/categories/cat_seq2seq.xml" rel="self" type="application/rss+xml"></atom:link><language>pt_br</language><copyright>Contents © 2018 &lt;a href="mailto:demacdolincoln@gmail.com"&gt;Lincoln de Macêdo&lt;/a&gt; </copyright><lastBuildDate>Mon, 31 Dec 2018 17:59:19 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Seq2Seq - Implementação</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-implementacao/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;span&gt;&lt;/span&gt;</description><category>modelagem</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-implementacao/</guid><pubDate>Mon, 24 Dec 2018 05:13:25 GMT</pubDate></item><item><title>Seq2Seq - Introdução</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-introducao/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Tenho certeza que todos ao menos uma vez se perguntaram, pelo menos nas primeiras vezes que usaram o google translate, "como é que isso funciona? é magica?", até mesmo pelo que escrevi aqui até agora, todos os conteúdos estão bastante distantes de algo que trate tão intensamente com linguagem do que o desta anotação. O seq2seq nos permite criar redes que aprendam a sequência em que as palavras estão dispostas num texto de modo que fique fácil gerar textos, por hora, para simplificar esse assunto bastante extendo, traterei aqui apenas de explicar cada passo praticamente sem o código e na próxima anotação terá uma implementação completa.&lt;/p&gt;
&lt;div class="section" id="encoder-decoder"&gt;
&lt;h2&gt;encoder - decoder&lt;/h2&gt;
&lt;p&gt;O grande "truque" está no mecanismo de codificação-decodificação, na prática são 2 redes neurais recorrentes bem simples que compartilham uma mesma camada oculta e não tem camada de ativação, só uma célula &lt;cite&gt;GRU ou LSTM &amp;lt;link://filename/posts/gru-e-lstm.rst&amp;gt;_&lt;/cite&gt; que realiza o processamento.&lt;/p&gt;
&lt;p&gt;A informação que dá sentido à ambas as redes é a camada oculta, é sobre ela que incide o treinamento, portanto essa é a camada responsável por fazer a relação entre as saídas de cada rede neural.&lt;/p&gt;
&lt;p&gt;Mas por não haver uma classificação, precisaremos de mais uma rede neural (3 até agora) que será como a rede de decodificação mas com funções de ativação para que sobre ela seja realizado o treinamento da camada oculta.&lt;/p&gt;
&lt;p&gt;Então o que temos até o momento é:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;encoder:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;gru&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;decoder:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;gru&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;att decoder:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;gru&lt;/li&gt;
&lt;li&gt;função linear&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="treinamento"&gt;
&lt;h2&gt;Treinamento&lt;/h2&gt;
&lt;p&gt;Como já disse que tudo está em torno da camada oculta compartilhada, é criando este array que se inicia o treinamento, que incide mais sobre a camada de decodificação que sobre a de encodificação, é feito desse modo pela camada de decodificação ser a usada para calcular a perda já que é ela que nos fornecerá a saída final do algoritmo.&lt;/p&gt;
&lt;p&gt;Procedimento:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;encoder_output, hidden_layer = encoder(input, hidden_layer)&lt;/li&gt;
&lt;li&gt;decoder_output, hidden_layer = decoder(encoder_output, hidden_layer)&lt;/li&gt;
&lt;li&gt;loss(decoder_output, target)&lt;/li&gt;
&lt;li&gt;backward&lt;/li&gt;
&lt;li&gt;step&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Na próxima anotação sobre o seq2seq, diante do código, tudo ficará mais claro.&lt;/p&gt;
&lt;p&gt;Resolvi não colocar imagens ilustrativas aqui pois no 1º link das leituras recomendadas há um monte de animações explicando bem detalhadamente todo o processo, das 2 leituras recomendadas, essa é a que mais recomendo.&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="leituras-recomendadas"&gt;
&lt;h2&gt;leituras recomendadas&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"&gt;http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://google.github.io/seq2seq/"&gt;https://google.github.io/seq2seq/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>modelagem</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-introducao/</guid><pubDate>Mon, 24 Dec 2018 05:13:03 GMT</pubDate></item></channel></rss>