<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Anotações sobre NLP</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/</link><description>Uma pequena ajuda mais prática que técnica ou teórica para quem está aprendendo sobre NLP</description><atom:link href="http://demacdolincoln.github.io/anotacoes-nlp/posts/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>pt_br</language><copyright>Contents © 2018 &lt;a href="mailto:demacdolincoln@gmail.com"&gt;Lincoln de Macêdo&lt;/a&gt; </copyright><lastBuildDate>Mon, 31 Dec 2018 20:34:09 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Resumos 0: PageRank</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/resumos-0-pagerank/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Esta é uma anotação introdutória ao problema de resumir textos, o ponto principal abordado aqui será a dificuldade de identificar o que é relevante, para isso usei o textrank, não entrarei em muitos detalhes sobre esse algoritmo, tratando de forma intuitiva a idéia geral que será mais aprofundada em anotações posteriores.&lt;/p&gt;
&lt;p&gt;PageRank foi o primeiro algoritmo usado pelo google para rankear os links de sua busca, logicamente o google evoluiu neste tempo todo e usa uma combinação de vários algoritmos e não o pagerank puro, aqui o usaremos para rankear os parágrafos de um texto da wikipedia.&lt;/p&gt;
&lt;div class="section" id="funcionamento"&gt;
&lt;h2&gt;Funcionamento&lt;/h2&gt;
&lt;p&gt;Não entrarei em muitos detalhes sobre o algoritmo, então explicando de forma superficial temos o fato do pagerank se valer de um grafo, e ao considerar o grau de cada nó, ou seja a quantidade de conexões de cada nó, e um peso atribuído a cada conexão, teremos um ranking de importância. Até mesmo explicando desse modo já imaginamos como o algoritmo se aplica bem a links entre páginas na internet, mas para textos ele realmente não é tão adequado porém é didático como algo introdutório.&lt;/p&gt;
&lt;p&gt;Os passos do "resumo" que na verdade é um rankeamento:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;extrair estatísticas do texto (&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa"&gt;tf-idf&lt;/a&gt; ou &lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-1-introducao"&gt;word2vec&lt;/a&gt;, enfim, qualquer coisa que nos diga algo sobre o texto)&lt;/li&gt;
&lt;li&gt;gerar uma matriz de similaridade, que na verdade servirá como matriz adjacente&lt;/li&gt;
&lt;li&gt;converter a matriz adjacente num grafo&lt;/li&gt;
&lt;li&gt;aplicar o PageRank&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="implementacao"&gt;
&lt;h2&gt;Implementação&lt;/h2&gt;
&lt;p&gt;Resolvi o skip-gram já treinado[1]_ e a página da wikipédia sobre Alan Turing como já feito antes, o processamento realmente começa criando listas com os valores correspondentes a cada palavra indicado pelo skip-gram.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_7f655bc01807478aaf8481b70fbbbc42-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;sentences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_7f655bc01807478aaf8481b70fbbbc42-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_7f655bc01807478aaf8481b70fbbbc42-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;paragraph&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7f655bc01807478aaf8481b70fbbbc42-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;paragraph&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;id2word&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;O passo seguinte é criar uma matriz quadrada onde cada lado tem o nº de parágrafos, preenchi a matriz da seguinte forma:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_d8741e6eb8c448659e1aecdaddf604bc-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_d8741e6eb8c448659e1aecdaddf604bc-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_d8741e6eb8c448659e1aecdaddf604bc-3"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_d8741e6eb8c448659e1aecdaddf604bc-4"&gt;&lt;/a&gt;            &lt;span class="n"&gt;similarity_matrix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_d8741e6eb8c448659e1aecdaddf604bc-5"&gt;&lt;/a&gt;                                          &lt;span class="n"&gt;cosine_similarity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_d8741e6eb8c448659e1aecdaddf604bc-6"&gt;&lt;/a&gt;                                      &lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;O que é feito acima é apenas comparar a similaridade de cossenos entre cada parágrafo, indicando de alguma forma algum nível de similaridade, de modo que o parágrafo com maior &lt;strong&gt;índice de similaridade&lt;/strong&gt; em relação aos demais será aquele que melhor representa o conjunto.&lt;/p&gt;
&lt;img alt="/images/similarity_matrix-classificacao-1.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/similarity_matrix-classificacao-1.png"&gt;
&lt;p&gt;Essa é uma matriz simétrica que seŕá lida como uma matriz adjacente de um grafo, cada linha e coluna serão nós e cada corrdenada indica o peso do vértice que liga cada nó, um dos problemas dessa estratégia é que todos os nós terão o mesmo grau, já que todos se ligam a todos, isso acaba inutilizando o uso do grau de cada nó para o pagerank, tendo como único parâmetro a considerar o peso dos vértices.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_numpy_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;similarity_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pagerank&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;original&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-5"&gt;&lt;/a&gt;    &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"original_text-Alan_Turing.pickle"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"rb"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-6"&gt;&lt;/a&gt;    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;word_rank&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-9"&gt;&lt;/a&gt;            &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-10"&gt;&lt;/a&gt;            &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-11"&gt;&lt;/a&gt;            &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-12"&gt;&lt;/a&gt;    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-13"&gt;&lt;/a&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-14"&gt;&lt;/a&gt;    &lt;span class="n"&gt;qnt_lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-15"&gt;&lt;/a&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-16"&gt;&lt;/a&gt;    &lt;span class="n"&gt;top&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_rank&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;qnt_lines&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-18"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qnt_lines&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-19"&gt;&lt;/a&gt;        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"-- parágrafo do resumo: {i} | parágrafo original: {top[i][1]}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_987e3760b0b646bb83d961a7d99e45cd-20"&gt;&lt;/a&gt;        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Na saída do código acima podemos reparar que a ordem de importância dada a cada parágrafo não necessariamente está relacionado ao seu tamanho ou à sua posição no texto:
.. epigraph:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
-- parágrafo do resumo: 0 | parágrafo original: 19
Por muitos anos, foram feitas campanhas que envolveram ativistas da tecnologia da informação, do meio político e do público LGBT. Em 11 de setembro de 2009, 55 anos após sua morte, o primeiro-ministro do Reino Unido, Gordon Brown, seguindo um pedido feito através de uma petição direcionada ao governo britânico, pediu desculpas formais em nome do governo pelo tratamento preconceituoso e desumano dado a Turing, que o levou ao suicídio. Em 24 de dezembro de 2013, passou a ter efeito a Real Prerrogativa do Perdão, concedida a Turing pela Rainha Elizabeth II, a pedido do ministro da justiça do Reino Unido, Chirs Grayling, depois que uma petição criada em 2012 obteve mais de 37.000 assinaturas solicitando o devido perdão.

-- parágrafo do resumo: 1 | parágrafo original: 3
A homossexualidade de Turing resultou em um processo criminal em 1952, pois atos homossexuais eram ilegais no Reino Unido na época, e ele aceitou o tratamento com hormônios femininos e castração química, como alternativa à prisão. Morreu em 1954, algumas semanas antes de seu aniversário de 42 anos, devido a um aparente autoadministrado envenenamento por cianeto, apesar de sua mãe (e alguns outros) terem considerado sua morte acidental. Em 10 de setembro de 2009, após uma campanha de internet, o primeiro-ministro britânico Gordon Brown fez um pedido oficial de desculpas público, em nome do governo britânico, devido à maneira pela qual Turing foi tratado após a guerra. Em 24 de dezembro de 2013, Alan Turing recebeu o perdão real da rainha Elizabeth II, da condenação por homossexualidade.

-- parágrafo do resumo: 2 | parágrafo original: 12
Em 1938, Turing se uniu ao GC&amp;amp;CS, o braço de decodificação de mensagens da inteligência britânica, para efetuar a Criptoanálise da Máquina Enigma. O Enigma era uma máquina de codificação que mudava seus códigos diariamente, obrigando a que o projeto de decifração se tornasse bastante rápido. Após o Reino Unido iniciar a Segunda Guerra Mundial ao declarar guerra à Alemanha em 1939, Turing foi direcionado para o quartel da GC&amp;amp;CS em Bletchley Park. A partir de uma máquina decodificadora polonesa, Turing projetou a Bomba eletromecânica ("Bombe"),  um equipamento eletromecânico que ajudaria a decriptar as mensagens do Enigma e foi montada em 1940. Novas Bombas foram construídas após Turing e sua equipe pedirem apoio a Winston Churchill, e mais de duzentas operavam ao fim da Guerra em 1945. Turing também introduziu sua equipe em Bletchley Park ao matemático Tommy Flowers, que em 1943 projetou o Colossus, um computador primitivo que ajudou a decodificar outra máquina criptográfica alemã, o Lorenz.
&lt;/pre&gt;
&lt;p&gt;Logicamente eu poderia ter usado frases em vez de parágrafos para fazer o resumo, talvez até fizesse mais sentido chamar a saída do código de resumo, mas resolvi usar parágrafos inteiros por considerar que a idéia fica mais clara assim e ao comparar com o texto original, fica mais visualmente evidente como se deu o trabalho do pagerank, nos próximos posts sobre este tópico serão mostradas redes neurais que fazem um trabalho bem mais coerente, logicamente usarei redes neurais recorrentes e o seq2seq, portanto recomendo que veja as anotações que escrevi sobre esses temas:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm"&gt;GRU e LSTM&lt;/a&gt;
&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-introducao"&gt;seq2seq: introdução&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;p&gt;_[1] &lt;a class="reference external" href="http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc"&gt;http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>modelagem</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/resumos-0-pagerank/</guid><pubDate>Tue, 25 Dec 2018 16:31:18 GMT</pubDate></item><item><title>Classificação 2: CNN</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-2-cnn/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Como falado &lt;cite&gt;anteriormente &amp;lt;link://filename/posts/classificacao-1.rst&amp;gt;_&lt;/cite&gt;, classificar um texto é algo que vai além do vocabulário ainda que a gente utilize o word2vec ou o glove, e como a ordem das palavras importa muito, vamos dar aqui o 1º passo neste sentido, vamos ver como funciona uma rede neural convolucional (CNN) aplicada à classificação de textos.&lt;/p&gt;
&lt;p&gt;Habitualmente elas são usadas essencialmente em processamento de imagens, a idéia é bastante simples: ter uma matriz maior e calcular uma matriz menor equivalente, neste processo há perda de dados e portanto é irreversível, porém tem se demonstrado muito útil em muitos casos.&lt;/p&gt;
&lt;div class="section" id="implementacao"&gt;
&lt;h2&gt;Implementação:&lt;/h2&gt;
&lt;p&gt;O primeiro passo é transformar um texto numa matriz, para isso vamos recordar o que temos:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;texto ~&amp;gt; sequência de ids de palavras&lt;/li&gt;
&lt;li&gt;skip-gram, cbow, glove, etc. ~&amp;gt; representação cartesiana de palavras segundo o sentido compreendido pelo seu uso&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Diante disso se torna meio lógico fazer uma matriz no formato &lt;cite&gt;words x embedding dims&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;Um dos problemas dessa abordagem é que frases tem tamanhos variáveis enquanto a matriz de entrada na camada convolucional da CNN precisa ter tamanho fixo, então temos de lidar sempre com o pior caso, frases grandes, e preencher o espaço restante das frases menores com zeros, isso acaba nos obrigando ter um custo computacional extra já que teremos muitos espaços em branco só para que sempre tenhamos matrizes do mesmo tamanho.&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="leituras-recomendadas"&gt;
&lt;h2&gt;Leituras recomendadas:&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://arxiv.org/abs/1408.5882"&gt;https://arxiv.org/abs/1408.5882&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>classificação</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-2-cnn/</guid><pubDate>Tue, 25 Dec 2018 08:17:55 GMT</pubDate></item><item><title>GRU e LSTM</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;obs: Não vou me demorar tratando de questões teóricas sobre as RNN em si, já que o foco dessas anotações é NLP, futuramente, ao concluir essas anotações talvez eu inicie uma série mais abrangente sobre machine learning, mas por enquanto apenas traterei de assuntos teóricos relativos a NLP e ao resto apenas uma abordagem prática.&lt;/p&gt;
&lt;p&gt;Parece meio óbvio dizer mas o que define uma rede neural recorrente é exatamente a recorrência, isto é, informações que são armazenadas e depois reutilizadas, a forma mais simples de implementação consiste em criar uma camada (um array) que armazena os resultados e é utilizado normalmente no processamento dos dados que passam por uma rede neural, só que com 1 caracérística distinta: haver uma condição ou não para atualizar esses dados aprendidos ao longo do treinamento e uma condição que a informação armazenada seja utilizada.&lt;/p&gt;
&lt;p&gt;Devido a esse mecanismo ser claramente um gerenciamento de memória e estarmos tratando de redes neurais, não demora muito para começarmos a associar ao modo como nosso cérebro gerencia a memória, desta forma vem ao mundo em 1997 o LSTM (Long Short-Term Memory) &lt;a class="footnote-reference" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/#id4" id="id1"&gt;[1]&lt;/a&gt; &lt;a class="footnote-reference" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/#id5" id="id2"&gt;[2]&lt;/a&gt;, e como o nome bem indica, se trata de gerenciamento de memória de curto e longo prazo, posteriormente, em 2014 nasce o GRU (Gated Recurrent Unit) &lt;a class="footnote-reference" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/#id6" id="id3"&gt;[3]&lt;/a&gt;, mantendo diversas semelhanças com o LSTM porém com um custo computacional um pouco menor mas que no geral tem um desempenho semelhante ainda que é comum encontrar artigos falando que um ou outro método funcionou bem melhor em algum caso específico. Mas aqui estamos tratando de entender por alto como funciona esse gerenciamento de memória, o que precisamos ter noção é de como eles fazem isso?&lt;/p&gt;
&lt;div class="section" id="portoes"&gt;
&lt;h2&gt;Portões&lt;/h2&gt;
&lt;p&gt;Portões == funções&lt;/p&gt;
&lt;img alt="https://upload.wikimedia.org/wikipedia/commons/3/3b/The_LSTM_cell.png" src="https://upload.wikimedia.org/wikipedia/commons/3/3b/The_LSTM_cell.png"&gt;
&lt;object data="https://upload.wikimedia.org/wikipedia/commons/3/37/Gated_Recurrent_Unit%2C_base_type.svg" type="image/svg+xml"&gt;
https://upload.wikimedia.org/wikipedia/commons/3/37/Gated_Recurrent_Unit%2C_base_type.svg&lt;/object&gt;
&lt;p&gt;Explicando as imagens acima: a primeira mostra como funciona o LSTM e a segunda mostra como funciona o GRU, ainda que os diagramas sejam diferentes, vemos que as funções usadas, os "portões" são os mesmos embora aplicados de diferentes formas, como são as funções que importam para entender aidéia geral, vou direto à explicação sobre as funções.&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\sigma = \frac{1}{1 + exp(-x)}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
tanh = \frac{e^x - e^-x}{e^x + e^{-x}}
\end{equation*}
&lt;/div&gt;
&lt;img alt="/images/lstm_gru-tanh-sigmoid.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/lstm_gru-tanh-sigmoid.png"&gt;
&lt;p&gt;Vemos que a diferença entre os gráficos dessas funções é essencialmente o limite quando tende a menos infinito, e isso faz toda a diferença, pois um limite que tende a 0 significa que posteriormente qualquer número multiplicado por 0 será 0, neste caso a função sigmoide indica a relevância de cada dimensão de entrada, se a dimensão for próxima a zero, ela vai perdendo relevância até desaparecer ou ser substituída por outra informação mais relevante (decidida pela função tanh).&lt;/p&gt;
&lt;p&gt;Todo o mecanismo de preservação e esquecimento desses métodos se baseia nesses "portões" que é como são chamadas as camadas com a função sigmoide, enquanto a função tanh tem o dever de fazer as escolhas finais, repare nas somas e multiplicações que unem o fluxo da saída com a camada oculta que armazena a memória, como a última estapa das operações com o estado da célula é sempre uma soma e os anteriores são multiplicações, isso revela a alteração dos pesos para definir a importância de cada dimensão e posteriormente a atualização mantendo assim para a época atual do treinamento da rede neural, a memória de curto prazo (os espaços próximos a zero que após a soma se mantém próximos aos resultados mais recentes) e a memória de longo prazo (o conteúdo mais relevante deixado mais próximo de 1)&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="artigos-e-links-recomendados"&gt;
&lt;h2&gt;artigos e links recomendados&lt;/h2&gt;
&lt;p&gt;Uma das melhores explicações que já encontrei sobre LSTM e GRU: &lt;a class="reference external" href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"&gt;Illustrated Guide to LSTM’s and GRU’s: A step by step explanation&lt;/a&gt;&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.5709"&gt;Learning to Forget: Continual Prediction with LSTM ( Felix A. Gers , Jürgen Schmidhuber , Fred Cummins )&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/#id2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://www.researchgate.net/profile/Sepp_Hochreiter/publication/13853244_Long_Short-term_Memory/links/5700e75608aea6b7746a0624/Long-Short-term-Memory.pdf?origin=publication_detail"&gt;Long short-term memory (Sepp Hochreiter; Jürgen Schmidhuber)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/#id3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://arxiv.org/pdf/1406.1078v3.pdf"&gt;Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation ( Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;&lt;/div&gt;</description><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/</guid><pubDate>Mon, 24 Dec 2018 05:13:54 GMT</pubDate></item><item><title>Seq2Seq - Implementação</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-implementacao/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;span&gt;&lt;/span&gt;</description><category>modelagem</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-implementacao/</guid><pubDate>Mon, 24 Dec 2018 05:13:25 GMT</pubDate></item><item><title>Seq2Seq - Introdução</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-introducao/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Tenho certeza que todos ao menos uma vez se perguntaram, pelo menos nas primeiras vezes que usaram o google translate, "como é que isso funciona? é magica?", até mesmo pelo que escrevi aqui até agora, todos os conteúdos estão bastante distantes de algo que trate tão intensamente com linguagem do que o desta anotação. O seq2seq nos permite criar redes que aprendam a sequência em que as palavras estão dispostas num texto de modo que fique fácil gerar textos, por hora, para simplificar esse assunto bastante extendo, traterei aqui apenas de explicar cada passo praticamente sem o código e na próxima anotação terá uma implementação completa.&lt;/p&gt;
&lt;div class="section" id="encoder-decoder"&gt;
&lt;h2&gt;encoder - decoder&lt;/h2&gt;
&lt;p&gt;O grande "truque" está no mecanismo de codificação-decodificação, na prática são 2 redes neurais recorrentes bem simples que compartilham uma mesma camada oculta e não tem camada de ativação, só uma célula &lt;cite&gt;GRU ou LSTM &amp;lt;link://filename/posts/gru-e-lstm.rst&amp;gt;_&lt;/cite&gt; que realiza o processamento.&lt;/p&gt;
&lt;p&gt;A informação que dá sentido à ambas as redes é a camada oculta, é sobre ela que incide o treinamento, portanto essa é a camada responsável por fazer a relação entre as saídas de cada rede neural.&lt;/p&gt;
&lt;p&gt;Mas por não haver uma classificação, precisaremos de mais uma rede neural (3 até agora) que será como a rede de decodificação mas com funções de ativação para que sobre ela seja realizado o treinamento da camada oculta.&lt;/p&gt;
&lt;p&gt;Então o que temos até o momento é:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;encoder:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;gru&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;decoder:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;gru&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;att decoder:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;gru&lt;/li&gt;
&lt;li&gt;função linear&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="treinamento"&gt;
&lt;h2&gt;Treinamento&lt;/h2&gt;
&lt;p&gt;Como já disse que tudo está em torno da camada oculta compartilhada, é criando este array que se inicia o treinamento, que incide mais sobre a camada de decodificação que sobre a de encodificação, é feito desse modo pela camada de decodificação ser a usada para calcular a perda já que é ela que nos fornecerá a saída final do algoritmo.&lt;/p&gt;
&lt;p&gt;Procedimento:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;encoder_output, hidden_layer = encoder(input, hidden_layer)&lt;/li&gt;
&lt;li&gt;decoder_output, hidden_layer = decoder(encoder_output, hidden_layer)&lt;/li&gt;
&lt;li&gt;loss(decoder_output, target)&lt;/li&gt;
&lt;li&gt;backward&lt;/li&gt;
&lt;li&gt;step&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Na próxima anotação sobre o seq2seq, diante do código, tudo ficará mais claro.&lt;/p&gt;
&lt;p&gt;Resolvi não colocar imagens ilustrativas aqui pois no 1º link das leituras recomendadas há um monte de animações explicando bem detalhadamente todo o processo, das 2 leituras recomendadas, essa é a que mais recomendo.&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="leituras-recomendadas"&gt;
&lt;h2&gt;leituras recomendadas&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"&gt;http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://google.github.io/seq2seq/"&gt;https://google.github.io/seq2seq/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>modelagem</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-introducao/</guid><pubDate>Mon, 24 Dec 2018 05:13:03 GMT</pubDate></item><item><title>Classificação 1</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-1/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;O problema fundamental da classificação de textos reside no fato da dificuldade de representar o texto a nível numérico, ainda que o word2vec, o glove, o seq2seq sejam realmente úteis assim como vários outros algoritmos que não incluí aqui mas que são facilmente encontrados em buscas no google, eles por si só não conseguem ir além da representação semântica ou de alguma outra lógica sobre as palavras, tanto para gerar textos quanto para classificá-los precisamos do auxílio de outros algoritmos. O objetivo desta anotação é identificar os desafions inerentes a esta tarefa.&lt;/p&gt;
&lt;div class="section" id="apenas-vocabulario"&gt;
&lt;h2&gt;Apenas vocabulário&lt;/h2&gt;
&lt;p&gt;Esse seria o caminho mais óbvio, tendo em vista que temos uma representação espacial da disposição das palavras num hiperplano, então faz sentido imaginar que textos sobre diferentes assuntos necessariamente tem diferentes vocabulários. Façamos um teste:&lt;/p&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;peguei 2 textos da wikipedia:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol class="loweralpha simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://pt.wikipedia.org/wiki/Lutefisk"&gt;Lutefisk&lt;/a&gt; - um prato da culinária norueguesa&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://pt.wikipedia.org/wiki/Erhu"&gt;Erhu&lt;/a&gt; - um instrumento tradicional chinês&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;fiz o pré-processamento das palavras como já descrito em outro post, ficando apenas com o vocabulário&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;peguei as posições correspondentes a cada palavra no skip-gram já treinado&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;reduzi as dimensões e plotei o gráfico&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;img alt="/images/classificacao_1_scatter_voc.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/classificacao_1_scatter_voc.png"&gt;
&lt;p&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-1/#id1"&gt;&lt;span class="problematic" id="id2"&gt;**&lt;/span&gt;&lt;/a&gt;explicando as cores:&lt;/p&gt;
&lt;div class="system-message" id="id1"&gt;
&lt;p class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;&amp;lt;string&amp;gt;&lt;/tt&gt;, line 19); &lt;em&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-1/#id2"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
Inline strong start-string without end-string.&lt;/div&gt;
&lt;!-- vermelho: vocabulário do texto 1 --&gt;
&lt;!-- ciano: vocabulário do texto 2 --&gt;
&lt;!-- branco: vocabulário em comum a ambos --&gt;
&lt;p&gt;Como é possível perceber acima, não identificamos um nível de separação consistente entre os vocabulários, olhando a densidade de concentração do vocabulário nos dois textos, descartando as palavras em comum, temos a imagem abaixo:&lt;/p&gt;
&lt;img alt="/images/classificação_1_kde_voc.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/classifica%C3%A7%C3%A3o_1_kde_voc.png"&gt;
&lt;p&gt;Os centros estão muito próximos, ou seja, mesmo identificando as regiões mais densas nos vocabulários, ao tentar usar esta região como métrica, a similaridade de sentido entre os termos para o texto 1 e para o texto 2 continuam muito próximas tornando essa estratégia bem ineficaz.&lt;/p&gt;
&lt;p&gt;A solução está na compreensão que um texto não são apenas palavras soltas, mas o sentifo extrapola a simples junção de palavras, então a ordem do que está escrito importa, a disposição das palavras no texto importa muito.&lt;/p&gt;
&lt;p&gt;Nas próximas anotações abordarei sobre o uso de redes convolucionais e redes neurais recorrentes, diferentes formas de tentar burlar as dificuldades aqui apresentadas.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-1/</guid><pubDate>Mon, 24 Dec 2018 05:12:26 GMT</pubDate></item><item><title>Distância Euclidiama vs Similaridade de Cossenos</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Indo direto ao ponto a principal diferença entre os cálculos é que enquanto na distância euclidiana é como se fizéssemos uma medição com uma régua entre 2 pontos, na similaridade de cossenos analisamos a distância angular entre 2 pontos a partir da origem, isso ficará mais claro no gráfico perto do final desta anotação.&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
dist\_eucl = \sqrt{\sum{(a-b)^2}}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
cosine\_sim = \frac{\sqrt{\sum{a * b}}}{\sqrt{\sum{a^2}} * \sqrt{\sum{b^2}}}
\end{equation*}
&lt;/div&gt;
&lt;div class="section" id="comparando-resultados"&gt;
&lt;h2&gt;Comparando resultados&lt;/h2&gt;
&lt;p&gt;Primeiro vamos implementar cada cálculo e depois uma função que receba uma matriz, normalize os dados, e indike os "k" pontos mais próximos a alguma coordenada que a gente escolher. Como usaremos em outras anotações, escrevi mais linhas do que um código simples e didático deveria ter:&lt;/p&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-12"&gt;12&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-13"&gt;13&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-14"&gt;14&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-15"&gt;15&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-16"&gt;16&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-17"&gt;17&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-18"&gt;18&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-19"&gt;19&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-20"&gt;20&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-21"&gt;21&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-22"&gt;22&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-23"&gt;23&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-24"&gt;24&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-25"&gt;25&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-26"&gt;26&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-27"&gt;27&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-28"&gt;28&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-1"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.spatial.distance&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;euclidean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cosine&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"cos"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s2"&gt;"coord"&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"coord"&lt;/span&gt;&lt;span class="p"&gt;]])))&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-12"&gt;&lt;/a&gt;        &lt;span class="n"&gt;ata_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;coord_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-14"&gt;&lt;/a&gt;    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;data_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-16"&gt;&lt;/a&gt;        &lt;span class="n"&gt;coord_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"pos"&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-18"&gt;&lt;/a&gt;    &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-19"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-20"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s2"&gt;"cos"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-21"&gt;&lt;/a&gt;            &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cosine&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-22"&gt;&lt;/a&gt;        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-23"&gt;&lt;/a&gt;            &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;euclidean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-24"&gt;&lt;/a&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-25"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s2"&gt;"cos"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-26"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-27"&gt;&lt;/a&gt;    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-28"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Visualizando a diferença de resultados entre as medições, gerei esse gráfico abaixo:&lt;/p&gt;
&lt;a class="reference external image-reference" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/eucl_vs_cos.png"&gt;&lt;img alt="/images/eucl_vs_cos.thumbnail.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/eucl_vs_cos.thumbnail.png" style="width: 500px;"&gt;&lt;/a&gt;
&lt;p&gt;explicando: os pontos vermelhos representam os pontos mais próximos desse ponto amarelo cortado por uma seta são os pontos mais próximos considerando a distância euclidiana, os pontos azuis é pela similaridade de cossenos e os roxos são os que as duas métricas coincidem ao listar os mais próximos, a seta indica a inclinação do ponto amarelo em relação a origem, e é isso que a similaridade de cossenos leva em consideração, perceba que um dos pontos azuis ficou bem distante mas projetando a seta vemos que se mantém mais próximo ao ângulo do ponto amarelo que o ponto vemelho.&lt;/p&gt;
&lt;p&gt;O motivo de preferirmos usar a similaridade de cossenos a usar distância euclidiana ou outras métricas para medir distâncias é que quando trabalhamos com NLP e ainda mais quando fazemos uma redução de dimensionalidade (onde ficou claro que há rotação e distorção) os ângulos ficam mais bem preservados que as distâncias.&lt;/p&gt;
&lt;p&gt;obs: É muito comum a similaridade é calculada com 1 passo a mais do que o demonstrado aqui, a distância angular é dada por:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
dist\_angular = \frac{cos^-1(cos\_similarity)}{\pi}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
angular\_similarity = 1-dist\_angular
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Outras vezes apenas fazem &lt;strong&gt;1-similaridade&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/knn_eucl_cos.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>utils</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/</guid><pubDate>Fri, 07 Dec 2018 07:04:17 GMT</pubDate></item><item><title>Estatística: TF-IDF e LSA</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Antes da popularidade de métodos baseados em IA, muito também devido à capacidade dos computadores da época, o que restava para análises de texto era quantificar as palavras e buscar extrair estatísticas, o mais básico e fundamental talvez seja o TF-IDF e por isso este post.&lt;/p&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name"&gt;
&lt;col class="field-body"&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;tf-idf:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;em&gt;frequency-inverse document frequency&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Este método se resume a contar a frequência de uso de palavras e realizar um cálculo que gere uma estimativa de uso/importância da palavra no texto, de certa forma ele se conecta à &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law"&gt;Lei de Zipf&lt;/a&gt; que trata justamente de uma análise da frequência de palavras.&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
TF(t) = \frac{nº\ de\ vezes\ que\ t\ aparece\ no\ texto}{total\ de\ termos\ no\ texto}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
IDF(t) = log_e(\frac{quantidade\ total\ de\ textos}{numero\ de\ textos\ em\ que\ t\ aparece})
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Recomendo bastante a wikipédia em inglês, há bastante exemplos de cálculos variantes: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Logicamente há inconsistências, afinal apenas a frequência não alcança o uso das palavras, não indica necessariamente as mais significativas se uma pessoa em vez de fazer referências a uma palavra ficar repetindo a mesma coisa o tempo todo. ex.:&lt;/p&gt;
&lt;!--  --&gt;
&lt;blockquote&gt;
&lt;p&gt;"há filmes bons, ruins e medianos, mas o filme em questão é o pior de todos, o filme é tão chato e cansativo que todos dormem assistindo os primeiros minutos do filme"&lt;/p&gt;
&lt;p&gt;"há filmes bons, ruins e medianos, mas este em questão é o pior de todos, tão chato e cansativo que todos dormem aos primeiros minutos"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;É bem claro que apesar do sentido do texto ser o mesmo, a importância dada à palavra "filme" seria diferente. E de fato, o TF-IDF funciona melhor para textos que seguem as regras de coesão e coerência, então vamos usar publicações da wikipédia.&lt;/p&gt;
&lt;p&gt;Apesar do cálculo ser bastante simples, vou preferir usar o sklearn pois neste caso o mais importante é ter uma ideia geral sobre um recurso básico e servir como uma introdução básica sobre NLP, especialmente sobre vertorização de textos&lt;/p&gt;
&lt;div class="section" id="tf-idf"&gt;
&lt;h2&gt;TF-IDF&lt;/h2&gt;
&lt;p&gt;Como quase tudo no sklearn...&lt;/p&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-12"&gt;12&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-13"&gt;13&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-14"&gt;14&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-1"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;wikipedia&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.corpus&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;stopwords&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-3"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;stopw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stopwords&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"portuguese"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;\
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stopwords&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"english"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-8"&gt;&lt;/a&gt;&lt;span class="n"&gt;wikipedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_lang&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"pt"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-9"&gt;&lt;/a&gt;&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;wikipedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Alan_Turing"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;tfidf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stop_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;stopw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-13"&gt;&lt;/a&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tfidf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitlines&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-14"&gt;&lt;/a&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Na penúltima linha usei o &lt;cite&gt;splitlines&lt;/cite&gt; para dividir o texto em parágrafos, assim podemos posteriormente coletar informações sobre os termos relevantes para cada parágrafo, mas admito esta forma ser demasiadamente simplista pois neste caso acabo considerando subtítulos como parágrafos.&lt;/p&gt;
&lt;p&gt;Internamente, o objeto que criamos, durante o treinamento, armazena um dicionário com as palavras e um "id", vamos usar isso para converter os termos:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_13bdd8d59ba844dcbc93ff2b1ac1af6c-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;
&lt;a name="rest_code_13bdd8d59ba844dcbc93ff2b1ac1af6c-2"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="n"&gt;x664&lt;/span&gt; &lt;span class="n"&gt;sparse&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt; &lt;span class="s1"&gt;'&amp;lt;class '&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="s1"&gt;'&amp;gt;'&lt;/span&gt;
&lt;a name="rest_code_13bdd8d59ba844dcbc93ff2b1ac1af6c-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mi"&gt;862&lt;/span&gt; &lt;span class="n"&gt;stored&lt;/span&gt; &lt;span class="n"&gt;elements&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;Compressed&lt;/span&gt; &lt;span class="n"&gt;Sparse&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt; &lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;A matriz esparsa tem diversas vantagens quando tratamos com longos arrays rechados de zeros, talvez o produto principal nessa implementação seja exatamente essa matriz que indica em cada parágrafo quais os termos presentes e a sua frequência, que é o ponto principal do TF-IDF.&lt;/p&gt;
&lt;img alt="visualização da matriz resultante" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/lsa.png"&gt;
&lt;p&gt;E é exatamente sobre essa matriz que chegamos no LSA (Latent Semantic Analysis), mas antes vamos ver quais as palavras mais relevantes do primeiro parágrafo:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;ft_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tfidf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_feature_names&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-2"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;top_tfidf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toarray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)[::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;top_tfidf&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-4"&gt;&lt;/a&gt;        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ft_name&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;computação&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;cheshire&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-8"&gt;&lt;/a&gt;&lt;span class="n"&gt;junho&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-9"&gt;&lt;/a&gt;&lt;span class="n"&gt;ciência&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-10"&gt;&lt;/a&gt;&lt;span class="n"&gt;influente&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;algoritmo&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-12"&gt;&lt;/a&gt;&lt;span class="n"&gt;east&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-13"&gt;&lt;/a&gt;&lt;span class="n"&gt;lógico&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-14"&gt;&lt;/a&gt;&lt;span class="n"&gt;desenvolvimento&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-15"&gt;&lt;/a&gt;&lt;span class="n"&gt;desempenhando&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;O ft_name é a lista de termos que irá converter para string a posição do termo indicada quando ordenamos o array comtendo o valor calculado para cada termo devolvendo as respectivas posições.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lsa"&gt;
&lt;h2&gt;LSA&lt;/h2&gt;
&lt;p&gt;O LSA é nada mais que usar o &lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/svd-vs-pca"&gt;SVD&lt;/a&gt; mas em vez de diminuir as dimensões vamos manter o tamanho da matriz:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-2"&gt;&lt;/a&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;664&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-4"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;lsa&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TruncatedSVD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-5"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;lsa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;TruncatedSVD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;algorithm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'randomized'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-7"&gt;&lt;/a&gt;   &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;O real poder do LSA vem desse tratamento dado à matriz formada a partir do TF-IDF, o código abaixo indica as palavras mais relevantes para cada parágrafo:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;comp&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lsa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;components_&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;terms_in_comp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ft_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;comp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;sorted_terms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;terms_in_comp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-4"&gt;&lt;/a&gt;                          &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-6"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"paragrafo: {i}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sorted_terms&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"-"&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Pegando apenas o parágrafo 0, o resultado que temos é:&lt;/p&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name"&gt;
&lt;col class="field-body"&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;paragrafo 0:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;turing&lt;/li&gt;
&lt;li&gt;máquina&lt;/li&gt;
&lt;li&gt;alan&lt;/li&gt;
&lt;li&gt;prêmio&lt;/li&gt;
&lt;li&gt;memorial&lt;/li&gt;
&lt;li&gt;guerra&lt;/li&gt;
&lt;li&gt;enigma&lt;/li&gt;
&lt;li&gt;bletchley&lt;/li&gt;
&lt;li&gt;park&lt;/li&gt;
&lt;li&gt;computação&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="off-topic"&gt;
&lt;h2&gt;off-topic&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;E para gerar estatísticas de relevância de um texto inteiro, basta não dividir em parágrafos&lt;/li&gt;
&lt;li&gt;E para gerarmos aquele bag of words que está na moda temos algumas opções, dependendo do caso aplicamos só o &lt;strong&gt;TF&lt;/strong&gt; para gerar um ranking, para outros casos o &lt;strong&gt;TF-IDF&lt;/strong&gt; funciona melhor, especialmente quando juntamos vários textos como uma análise geral de várias páginas de blogs, o LSA tende a ser melhor em usos mais específicos porém nada impede de usa-lo para gerar o ranking de termos para um livro, por exemplo.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/estatistica-tf-idf-e-lsa.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>utils</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/</guid><pubDate>Fri, 07 Dec 2018 04:47:59 GMT</pubDate></item><item><title>word2vec 3: skip-gram</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-3-skip-gram/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Como já dito antes, o skip-gram faz um treinamento meio que ao contrário do cbow, no treinamento a rede neural recebe as palavras centrais para tentar prever as palavras de contexto e assim ajusta os pesos das camadas da rede neural aproximando valores para palavras semelhantes no hiperplano.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;text_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-8"&gt;&lt;/a&gt;           &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-9"&gt;&lt;/a&gt;       &lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-11"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;center_word_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-14"&gt;&lt;/a&gt;        &lt;span class="n"&gt;context_word_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;pair_ids&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;center_word_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context_word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-16"&gt;&lt;/a&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;A única diferença do código acima para criar os pares de ids está na ordem: primeiro a palavra central e depois a palavra de contexto:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;central&lt;/th&gt;
&lt;th class="head"&gt;contexto&lt;/th&gt;
&lt;th class="head"&gt;central&lt;/th&gt;
&lt;th class="head"&gt;contexto&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;máquina&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;302&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;computação&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;máquina&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;máquina&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;409&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;importante&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;O modelo da rede neural não se difere muito da usada no cbow, a única diferença fica por conta do tamanho da entrada da primeira função linear, já que passaremos 1 id por vez e não 4 como no cbow.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-3"&gt;&lt;/a&gt;        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-5"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-7"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# única diferença aqui&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-8"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-10"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LogSoftmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-16"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-19"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-20"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-21"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_word_emb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word_id&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-22"&gt;&lt;/a&gt;        &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LongTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-23"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;De modo geral o nível de erro (ou perda, nunca sei ao certo como traduzir "loss" neste contexto) no skip-gram é maior que no cbow, mas repito que o importante é que esteja havendo um aprendizado e não que a rede neural se adapte ao ponto de prever todas as palavras relacionadas ainda que ocasionalmente isso ocorra, para nós interessa o seguinte movimento: numa época a rede neural elevar os valores das palavras próximas na saída e afastar as mais distantes, assim naturalmente ela vai aprendendo a agrupar palavras em regiões de um hiperplano aproximando ou afastando de acordo com o modo como as palavras são usadas, tendendo a manter um distanciamento relacionado ao seu valor semântico.&lt;/p&gt;
&lt;img alt="/images/word2vec-skipgram-loss.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-skipgram-loss.png"&gt;
&lt;p&gt;Reduzindo as dimensões para visualizar a distribuição...&lt;/p&gt;
&lt;img alt="/images/word2vec-skipgram-1.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-skipgram-1.png" style="width: 500px;"&gt;
&lt;p&gt;Logicamente dessa forma como implementei, o custo/perda/loss é mais alto que na implementação feita do cbow, afinal vamos aos poucos ajustando 4 resultados possíveis para cada termo. Neste exemplo aumentei a quantidade de épocas para 2500 e ainda assim ficou imensamente distante do resultado da implementação do cbow neste aspecto, porém a relação entre as palavras se mostrou um pouco melhor ainda que longe do ideal.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;rank sim cos&lt;/th&gt;
&lt;th class="head"&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;
&lt;/li&gt;&lt;/ul&gt;
&lt;/th&gt;
&lt;th class="head"&gt;rank dist eucl&lt;/th&gt;
&lt;th class="head"&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;
&lt;/li&gt;&lt;/ul&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;muitos&lt;/td&gt;
&lt;td&gt;0.14544&lt;/td&gt;
&lt;td&gt;muitos&lt;/td&gt;
&lt;td&gt;0.07375&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;poderia&lt;/td&gt;
&lt;td&gt;0.26087&lt;/td&gt;
&lt;td&gt;code&lt;/td&gt;
&lt;td&gt;0.08692&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;ceruzzi&lt;/td&gt;
&lt;td&gt;0.28141&lt;/td&gt;
&lt;td&gt;ceruzzi&lt;/td&gt;
&lt;td&gt;0.08939&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;code&lt;/td&gt;
&lt;td&gt;0.28206&lt;/td&gt;
&lt;td&gt;condados&lt;/td&gt;
&lt;td&gt;0.09595&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;britânica&lt;/td&gt;
&lt;td&gt;0.28430&lt;/td&gt;
&lt;td&gt;mortem&lt;/td&gt;
&lt;td&gt;0.09709&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;mortem&lt;/td&gt;
&lt;td&gt;0.33544&lt;/td&gt;
&lt;td&gt;atos&lt;/td&gt;
&lt;td&gt;0.10284&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;condenado&lt;/td&gt;
&lt;td&gt;0.33660&lt;/td&gt;
&lt;td&gt;teórica&lt;/td&gt;
&lt;td&gt;0.10357&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;comerciantes&lt;/td&gt;
&lt;td&gt;0.33929&lt;/td&gt;
&lt;td&gt;condenado&lt;/td&gt;
&lt;td&gt;0.10376&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;cabeceira&lt;/td&gt;
&lt;td&gt;0.34548&lt;/td&gt;
&lt;td&gt;rápido&lt;/td&gt;
&lt;td&gt;0.10433&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;condados&lt;/td&gt;
&lt;td&gt;0.36041&lt;/td&gt;
&lt;td&gt;prazer&lt;/td&gt;
&lt;td&gt;0.10648&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;img alt="/images/word2vec-skipgram-rank.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-skipgram-rank.png"&gt;
&lt;p&gt;Só lembrando que segui o mesmo padrão de cores:&lt;/p&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name"&gt;
&lt;col class="field-body"&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;amarelo:&lt;/th&gt;&lt;td class="field-body"&gt;Palavra escolhida&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;vermelho:&lt;/th&gt;&lt;td class="field-body"&gt;Termos mais próximos pela similaridade de cossenos&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;azul:&lt;/th&gt;&lt;td class="field-body"&gt;Termos mais próximos pela distância euclidiana&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;roxo:&lt;/th&gt;&lt;td class="field-body"&gt;Termos que ambas as métricas concordam&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/word2vec-3-skipgram.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>word2vec</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-3-skip-gram/</guid><pubDate>Fri, 07 Dec 2018 04:43:36 GMT</pubDate></item><item><title>SVD vs PCA</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/svd-vs-pca/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Não vou tratar aqui de como se implementa o PCA e o SVD, prefiro indicar esses tutoriais abaixo, eles foram muito bem escritos e são muito claros sobre como são os cálculos usados:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://sebastianraschka.com/Articles/2014_pca_step_by_step.html"&gt;tutorial PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/"&gt;tutorial SVD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Embora esses métodos possam ser usados para compressão de dados, análises populacionais e uma infinidade de análises envolvendo dados organizados em matrizes, aqui prefiro comparar cada método e discutir o uso voltado à redução de dimensões a fim que possamos visualizar os dados dessas anotações,&lt;/p&gt;
&lt;p&gt;mas antes de chegar nas discussões, vamos ver alguns gráficos mostrando o que o SVD e o PCA retornaram quando os usamos para reduzir dimensões de matrizes:&lt;/p&gt;
&lt;img alt="/images/svd_pca_0_3d.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_0_3d.png"&gt;
&lt;img alt="/images/svd_pca_1_3dreduction.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_1_3dreduction.png"&gt;
&lt;p&gt;curiosamente vemos que ocorreu uma rotação no gráfico do PCA e que o gráfico do SVD mantém uma certa similaridade visual com o gráfico original em 3D. Só compreendi melhor vendo &lt;a class="reference external" href="https://www.quora.com/What-is-the-difference-between-PCA-and-SVD/answer/Adarsh-131"&gt;esta resposta no Quora&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Geometrically PCA corresponds to “centering the dataset”, and then rotating it to align the axis of highest variance with the principle axis."&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Geometricamente, PCA corresponde a "centralização do dataset", e depois rotaciona para alinhar o eixo de maior variância com o eixo principal&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Lógico que nem sempre acontece de ambos as representações ficarem tão diferentes, para observar melhor isso resolvi seguir um &lt;a class="reference external" href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py"&gt;exemplo da documentação do sklearn&lt;/a&gt;&lt;/p&gt;
&lt;img alt="/images/svd_pca_2_64reduction.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_2_64reduction.png"&gt;
&lt;p&gt;A imagem acima mostra que deve ter coincidido a forma como o SVD reduziu as dimensões e a rotação feita pelo PCA, só lembrando o que está de forma muito explícita no link para o Quora: o PCA usa o SVD para criar um ranking, afinal PCA significa "análise do componente principal" e o SVD fornece um dos passos para chegar ao componente pricipal.&lt;/p&gt;
&lt;p&gt;Mas o KMeans realiza um aprendizado não supervisionado, e ainda especialmente neste caso onde a redução de 64 dimensões para 2 com certeza não deu margem para que os dados fossem linearmente separáveis, resolvi usar o SVM para desenhar o espaço para cada classe.&lt;/p&gt;
&lt;img alt="/images/svd_pca_3_svm.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_3_svm.png"&gt;
&lt;p&gt;Algo que se deve ressaltar no gráfico acima é que os pontos semi-transparentes que adicionei ao gráfico são os que os classificadores treinados erraram, sobre isso repare no resultado abaixo:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-1"&gt;&lt;/a&gt;erros SVD: 704 de 1797
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-2"&gt;&lt;/a&gt;erros PCA: 704 de 1797
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-3"&gt;&lt;/a&gt;erros normal: 0 de 1797
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-4"&gt;&lt;/a&gt;-----------------------
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-5"&gt;&lt;/a&gt;percentuais de acertos:
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-6"&gt;&lt;/a&gt;&amp;gt; SVD: 60.824%
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-7"&gt;&lt;/a&gt;&amp;gt; pca: 60.824%
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-8"&gt;&lt;/a&gt;&amp;gt; normal: 100.000%
&lt;/pre&gt;&lt;p&gt;Considerei "normal" como a aplicação do SVM sem reduzir as dimensões. Estes resultados mostram que a sobreposição de dados na redução de dimensões assim como a distorção que ocorre nas transformações feitas com as matrizes, tende a dificultar o trabalho dos algoritmos, mesmo mantendo um certo nível de fidelidade com a distribuição original dos dadosm o melhor é usar essa redução mais para visualizar do que para aplicar métricas ou classificadores, e por isso também que nas notas onde uso distância euclidiana e similaridade de cossenos, ao reduzir as dimensões os resultados parecem errados ainda que nas dimensões originais esteja correto.&lt;/p&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/SVD-PCA.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>utils</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/svd-vs-pca/</guid><pubDate>Fri, 07 Dec 2018 04:26:29 GMT</pubDate></item></channel></rss>