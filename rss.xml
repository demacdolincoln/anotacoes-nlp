<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Anotações sobre NLP</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/</link><description>Uma pequena ajuda mais prática que técnica ou teórica para quem está aprendendo sobre NLP</description><atom:link href="http://demacdolincoln.github.io/anotacoes-nlp/posts/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>pt_br</language><copyright>Contents © 2018 &lt;a href="mailto:demacdolincoln@gmail.com"&gt;Lincoln de Macêdo&lt;/a&gt; </copyright><lastBuildDate>Tue, 11 Dec 2018 05:35:21 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Distância Euclidiama vs Similaridade de Cossenos</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Indo direto ao ponto a principal diferença entre os cálculos é que enquanto na distância euclidiana é como se fizéssemos uma medição com uma régua entre 2 pontos, na similaridade de cossenos analisamos a distância angular entre 2 pontos a partir da origem, isso ficará mais claro no gráfico perto do final desta anotação.&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
dist\_eucl = \sqrt{\sum{(a-b)^2}}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
cosine\_sim = \frac{\sqrt{\sum{a * b}}}{\sqrt{\sum{a^2}} * \sqrt{\sum{b^2}}}
\end{equation*}
&lt;/div&gt;
&lt;div class="section" id="comparando-resultados"&gt;
&lt;h2&gt;Comparando resultados&lt;/h2&gt;
&lt;p&gt;Primeiro vamos implementar cada cálculo e depois uma função que receba uma matriz, normalize os dados, e indike os "k" pontos mais próximos a alguma coordenada que a gente escolher. Como usaremos em outras anotações, escrevi mais linhas do que um código simples e didático deveria ter:&lt;/p&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-12"&gt;12&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-13"&gt;13&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-14"&gt;14&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-15"&gt;15&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-16"&gt;16&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-17"&gt;17&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-18"&gt;18&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-19"&gt;19&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-20"&gt;20&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-21"&gt;21&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-22"&gt;22&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-23"&gt;23&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-24"&gt;24&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-25"&gt;25&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-26"&gt;26&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-27"&gt;27&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_26923e0007a54c26ade0ce5edaa2de95-28"&gt;28&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-1"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.spatial.distance&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;euclidean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cosine&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"cos"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s2"&gt;"coord"&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"coord"&lt;/span&gt;&lt;span class="p"&gt;]])))&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-12"&gt;&lt;/a&gt;        &lt;span class="n"&gt;ata_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;coord_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-14"&gt;&lt;/a&gt;    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;data_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-16"&gt;&lt;/a&gt;        &lt;span class="n"&gt;coord_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"pos"&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-18"&gt;&lt;/a&gt;    &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-19"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-20"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s2"&gt;"cos"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-21"&gt;&lt;/a&gt;            &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cosine&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-22"&gt;&lt;/a&gt;        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-23"&gt;&lt;/a&gt;            &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;euclidean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-24"&gt;&lt;/a&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-25"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s2"&gt;"cos"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-26"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-27"&gt;&lt;/a&gt;    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_26923e0007a54c26ade0ce5edaa2de95-28"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Visualizando a diferença de resultados entre as medições, gerei esse gráfico abaixo:&lt;/p&gt;
&lt;a class="reference external image-reference" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/eucl_vs_cos.png"&gt;&lt;img alt="/images/eucl_vs_cos.thumbnail.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/eucl_vs_cos.thumbnail.png" style="width: 500px;"&gt;&lt;/a&gt;
&lt;p&gt;explicando: os pontos vermelhos representam os pontos mais próximos desse ponto amarelo cortado por uma seta são os pontos mais próximos considerando a distância euclidiana, os pontos azuis é pela similaridade de cossenos e os roxos são os que as duas métricas coincidem ao listar os mais próximos, a seta indica a inclinação do ponto amarelo em relação a origem, e é isso que a similaridade de cossenos leva em consideração, perceba que um dos pontos azuis ficou bem distante mas projetando a seta vemos que se mantém mais próximo ao ângulo do ponto amarelo que o ponto vemelho.&lt;/p&gt;
&lt;p&gt;O motivo de preferirmos usar a similaridade de cossenos a usar distância euclidiana ou outras métricas para medir distâncias é que quando trabalhamos com NLP e ainda mais quando fazemos uma redução de dimensionalidade (onde ficou claro que há rotação e distorção) os ângulos ficam mais bem preservados que as distâncias.&lt;/p&gt;
&lt;p&gt;obs: É muito comum a similaridade é calculada com 1 passo a mais do que o demonstrado aqui, a distância angular é dada por:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
dist\_angular = \frac{cos^-1(cos\_similarity)}{\pi}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
angular\_similarity = 1-dist\_angular
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Outras vezes apenas fazem &lt;strong&gt;1-similaridade&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/files/knn_eucl_cos.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>utils</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/</guid><pubDate>Fri, 07 Dec 2018 07:04:17 GMT</pubDate></item><item><title>Estatística: TF-IDF e LSA</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Antes da popularidade de métodos baseados em IA, muito também devido à capacidade dos computadores da época, o que restava para análises de texto era quantificar as palavras e buscar extrair estatísticas, o mais básico e fundamental talvez seja o TF-IDF e por isso este post.&lt;/p&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name"&gt;
&lt;col class="field-body"&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;tf-idf:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;em&gt;frequency-inverse document frequency&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Este método se resume a contar a frequência de uso de palavras e realizar um cálculo que gere uma estimativa de uso/importância da palavra no texto, de certa forma ele se conecta à &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law"&gt;Lei de Zipf&lt;/a&gt; que trata justamente de uma análise da frequência de palavras.&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
TF(t) = \frac{nº\ de\ vezes\ que\ t\ aparece\ no\ texto}{total\ de\ termos\ no\ texto}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
IDF(t) = log_e(\frac{quantidade\ total\ de\ textos}{numero\ de\ textos\ em\ que\ t\ aparece})
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Recomendo bastante a wikipédia em inglês, há bastante exemplos de cálculos variantes: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Logicamente há inconsistências, afinal apenas a frequência não alcança o uso das palavras, não indica necessariamente as mais significativas se uma pessoa em vez de fazer referências a uma palavra ficar repetindo a mesma coisa o tempo todo. ex.:&lt;/p&gt;
&lt;!--  --&gt;
&lt;blockquote&gt;
&lt;p&gt;"há filmes bons, ruins e medianos, mas o filme em questão é o pior de todos, o filme é tão chato e cansativo que todos dormem assistindo os primeiros minutos do filme"&lt;/p&gt;
&lt;p&gt;"há filmes bons, ruins e medianos, mas este em questão é o pior de todos, tão chato e cansativo que todos dormem aos primeiros minutos"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;É bem claro que apesar do sentido do texto ser o mesmo, a importância dada à palavra "filme" seria diferente. E de fato, o TF-IDF funciona melhor para textos que seguem as regras de coesão e coerência, então vamos usar publicações da wikipédia.&lt;/p&gt;
&lt;p&gt;Apesar do cálculo ser bastante simples, vou preferir usar o sklearn pois neste caso o mais importante é ter uma ideia geral sobre um recurso básico e servir como uma introdução básica sobre NLP, especialmente sobre vertorização de textos&lt;/p&gt;
&lt;div class="section" id="tf-idf"&gt;
&lt;h2&gt;TF-IDF&lt;/h2&gt;
&lt;p&gt;Como quase tudo no sklearn...&lt;/p&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-12"&gt;12&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-13"&gt;13&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_d64834f8a0e44d0bbca6b89fe692778a-14"&gt;14&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-1"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;wikipedia&lt;/span&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.corpus&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;stopwords&lt;/span&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-3"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;stopw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stopwords&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"portuguese"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;\
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stopwords&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"english"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-8"&gt;&lt;/a&gt;&lt;span class="n"&gt;wikipedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_lang&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"pt"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-9"&gt;&lt;/a&gt;&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;wikipedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Alan_Turing"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;tfidf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stop_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;stopw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-13"&gt;&lt;/a&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tfidf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitlines&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-14"&gt;&lt;/a&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Na penúltima linha usei o &lt;cite&gt;splitlines&lt;/cite&gt; para dividir o texto em parágrafos, assim podemos posteriormente coletar informações sobre os termos relevantes para cada parágrafo, mas admito esta forma ser demasiadamente simplista pois neste caso acabo considerando subtítulos como parágrafos.&lt;/p&gt;
&lt;p&gt;Internamente, o objeto que criamos, durante o treinamento, armazena um dicionário com as palavras e um "id", vamos usar isso para converter os termos:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_1c0c7dd8f0f34b4382bcf82ab32598f5-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;
&lt;a name="rest_code_1c0c7dd8f0f34b4382bcf82ab32598f5-2"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="n"&gt;x664&lt;/span&gt; &lt;span class="n"&gt;sparse&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt; &lt;span class="s1"&gt;'&amp;lt;class '&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="s1"&gt;'&amp;gt;'&lt;/span&gt;
&lt;a name="rest_code_1c0c7dd8f0f34b4382bcf82ab32598f5-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mi"&gt;862&lt;/span&gt; &lt;span class="n"&gt;stored&lt;/span&gt; &lt;span class="n"&gt;elements&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;Compressed&lt;/span&gt; &lt;span class="n"&gt;Sparse&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt; &lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;A matriz esparsa tem diversas vantagens quando tratamos com longos arrays rechados de zeros, talvez o produto principal nessa implementação seja exatamente essa matriz que indica em cada parágrafo quais os termos presentes e a sua frequência, que é o ponto principal do TF-IDF.&lt;/p&gt;
&lt;img alt="visualização da matriz resultante" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/lsa.png"&gt;
&lt;p&gt;E é exatamente sobre essa matriz que chegamos no LSA (Latent Semantic Analysis), mas antes vamos ver quais as palavras mais relevantes do primeiro parágrafo:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;ft_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tfidf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_feature_names&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-2"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;top_tfidf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toarray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)[::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;top_tfidf&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-4"&gt;&lt;/a&gt;        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ft_name&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;computação&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;cheshire&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-8"&gt;&lt;/a&gt;&lt;span class="n"&gt;junho&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-9"&gt;&lt;/a&gt;&lt;span class="n"&gt;ciência&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-10"&gt;&lt;/a&gt;&lt;span class="n"&gt;influente&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;algoritmo&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-12"&gt;&lt;/a&gt;&lt;span class="n"&gt;east&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-13"&gt;&lt;/a&gt;&lt;span class="n"&gt;lógico&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-14"&gt;&lt;/a&gt;&lt;span class="n"&gt;desenvolvimento&lt;/span&gt;
&lt;a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-15"&gt;&lt;/a&gt;&lt;span class="n"&gt;desempenhando&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;O ft_name é a lista de termos que irá converter para string a posição do termo indicada quando ordenamos o array comtendo o valor calculado para cada termo devolvendo as respectivas posições.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lsa"&gt;
&lt;h2&gt;LSA&lt;/h2&gt;
&lt;p&gt;O LSA é nada mais que usar o &lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/svd-vs-pca"&gt;SVD&lt;/a&gt; mas em vez de diminuir as dimensões vamos manter o tamanho da matriz:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-2"&gt;&lt;/a&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;664&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-4"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;lsa&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TruncatedSVD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-5"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;lsa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;TruncatedSVD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;algorithm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'randomized'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-7"&gt;&lt;/a&gt;   &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;O real poder do LSA vem desse tratamento dado à matriz formada a partir do TF-IDF, o código abaixo indica as palavras mais relevantes para cada parágrafo:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_83e0731111324c66915b3bc47e48b73f-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;comp&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lsa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;components_&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_83e0731111324c66915b3bc47e48b73f-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;terms_in_comp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ft_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;comp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_83e0731111324c66915b3bc47e48b73f-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;sorted_terms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;terms_in_comp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_83e0731111324c66915b3bc47e48b73f-4"&gt;&lt;/a&gt;                          &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_83e0731111324c66915b3bc47e48b73f-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_83e0731111324c66915b3bc47e48b73f-6"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"paragrafo: {i}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_83e0731111324c66915b3bc47e48b73f-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sorted_terms&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_83e0731111324c66915b3bc47e48b73f-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_83e0731111324c66915b3bc47e48b73f-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"-"&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Pegando apenas o parágrafo 0, o resultado que temos é:&lt;/p&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name"&gt;
&lt;col class="field-body"&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;paragrafo 0:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;turing&lt;/li&gt;
&lt;li&gt;máquina&lt;/li&gt;
&lt;li&gt;alan&lt;/li&gt;
&lt;li&gt;prêmio&lt;/li&gt;
&lt;li&gt;memorial&lt;/li&gt;
&lt;li&gt;guerra&lt;/li&gt;
&lt;li&gt;enigma&lt;/li&gt;
&lt;li&gt;bletchley&lt;/li&gt;
&lt;li&gt;park&lt;/li&gt;
&lt;li&gt;computação&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="off-topic"&gt;
&lt;h2&gt;off-topic&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;E para gerar estatísticas de relevância de um texto inteiro, basta não dividir em parágrafos&lt;/li&gt;
&lt;li&gt;E para gerarmos aquele bag of words que está na moda temos algumas opções, dependendo do caso aplicamos só o &lt;strong&gt;TF&lt;/strong&gt; para gerar um ranking, para outros casos o &lt;strong&gt;TF-IDF&lt;/strong&gt; funciona melhor, especialmente quando juntamos vários textos como uma análise geral de várias páginas de blogs, o LSA tende a ser melhor em usos mais específicos porém nada impede de usa-lo para gerar o ranking de termos para um livro, por exemplo.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/files/estatistica-tf-idf-e-lsa.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>utils</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/</guid><pubDate>Fri, 07 Dec 2018 04:47:59 GMT</pubDate></item><item><title>word2vec 3: skip-gram</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-3-skip-gram/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Como já dito antes, o skip-gram faz um treinamento meio que ao contrário do cbow, no treinamento a rede neural recebe as palavras centrais para tentar prever as palavras de contexto e assim ajusta os pesos das camadas da rede neural aproximando valores para palavras semelhantes no hiperplano.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;text_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-8"&gt;&lt;/a&gt;           &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-9"&gt;&lt;/a&gt;       &lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-11"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;center_word_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-14"&gt;&lt;/a&gt;        &lt;span class="n"&gt;context_word_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;pair_ids&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;center_word_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context_word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_1c02ba04c5d940ec8d8697f8662ba7c7-16"&gt;&lt;/a&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;A única diferença do código acima para criar os pares de ids está na ordem: primeiro a palavra central e depois a palavra de contexto:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;central&lt;/th&gt;
&lt;th class="head"&gt;contexto&lt;/th&gt;
&lt;th class="head"&gt;central&lt;/th&gt;
&lt;th class="head"&gt;contexto&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;máquina&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;302&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;computação&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;máquina&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;máquina&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;409&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;importante&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;O modelo da rede neural não se difere muito da usada no cbow, a única diferença fica por conta do tamanho da entrada da primeira função linear, já que passaremos 1 id por vez e não 4 como no cbow.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-3"&gt;&lt;/a&gt;        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-5"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-7"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# única diferença aqui&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-8"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-10"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LogSoftmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-16"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-19"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-20"&gt;&lt;/a&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-21"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_word_emb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word_id&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-22"&gt;&lt;/a&gt;        &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LongTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_96bac182964241b6a0d4397244b62b38-23"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;De modo geral o nível de erro (ou perda, nunca sei ao certo como traduzir "loss" neste contexto) no skip-gram é maior que no cbow, mas repito que o importante é que esteja havendo um aprendizado e não que a rede neural se adapte ao ponto de prever todas as palavras relacionadas ainda que ocasionalmente isso ocorra, para nós interessa o seguinte movimento: numa época a rede neural elevar os valores das palavras próximas na saída e afastar as mais distantes, assim naturalmente ela vai aprendendo a agrupar palavras em regiões de um hiperplano aproximando ou afastando de acordo com o modo como as palavras são usadas, tendendo a manter um distanciamento relacionado ao seu valor semântico.&lt;/p&gt;
&lt;img alt="/images/word2vec-skipgram-loss.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-skipgram-loss.png"&gt;
&lt;p&gt;Reduzindo as dimensões para visualizar a distribuição...&lt;/p&gt;
&lt;img alt="/images/word2vec-skipgram-1.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-skipgram-1.png" style="width: 500px;"&gt;
&lt;p&gt;Logicamente dessa forma como implementei, o custo/perda/loss é mais alto que na implementação feita do cbow, afinal vamos aos poucos ajustando 4 resultados possíveis para cada termo. Neste exemplo aumentei a quantidade de épocas para 2500 e ainda assim ficou imensamente distante do resultado da implementação do cbow neste aspecto, porém a relação entre as palavras se mostrou um pouco melhor ainda que longe do ideal.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;rank sim cos&lt;/th&gt;
&lt;th class="head"&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;
&lt;/li&gt;&lt;/ul&gt;
&lt;/th&gt;
&lt;th class="head"&gt;rank dist eucl&lt;/th&gt;
&lt;th class="head"&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;
&lt;/li&gt;&lt;/ul&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;muitos&lt;/td&gt;
&lt;td&gt;0.14544&lt;/td&gt;
&lt;td&gt;muitos&lt;/td&gt;
&lt;td&gt;0.07375&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;poderia&lt;/td&gt;
&lt;td&gt;0.26087&lt;/td&gt;
&lt;td&gt;code&lt;/td&gt;
&lt;td&gt;0.08692&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;ceruzzi&lt;/td&gt;
&lt;td&gt;0.28141&lt;/td&gt;
&lt;td&gt;ceruzzi&lt;/td&gt;
&lt;td&gt;0.08939&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;code&lt;/td&gt;
&lt;td&gt;0.28206&lt;/td&gt;
&lt;td&gt;condados&lt;/td&gt;
&lt;td&gt;0.09595&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;britânica&lt;/td&gt;
&lt;td&gt;0.28430&lt;/td&gt;
&lt;td&gt;mortem&lt;/td&gt;
&lt;td&gt;0.09709&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;mortem&lt;/td&gt;
&lt;td&gt;0.33544&lt;/td&gt;
&lt;td&gt;atos&lt;/td&gt;
&lt;td&gt;0.10284&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;condenado&lt;/td&gt;
&lt;td&gt;0.33660&lt;/td&gt;
&lt;td&gt;teórica&lt;/td&gt;
&lt;td&gt;0.10357&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;comerciantes&lt;/td&gt;
&lt;td&gt;0.33929&lt;/td&gt;
&lt;td&gt;condenado&lt;/td&gt;
&lt;td&gt;0.10376&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;cabeceira&lt;/td&gt;
&lt;td&gt;0.34548&lt;/td&gt;
&lt;td&gt;rápido&lt;/td&gt;
&lt;td&gt;0.10433&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;condados&lt;/td&gt;
&lt;td&gt;0.36041&lt;/td&gt;
&lt;td&gt;prazer&lt;/td&gt;
&lt;td&gt;0.10648&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;img alt="/images/word2vec-skipgram-rank.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-skipgram-rank.png"&gt;
&lt;p&gt;Só lembrando que segui o mesmo padrão de cores:&lt;/p&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name"&gt;
&lt;col class="field-body"&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;amarelo:&lt;/th&gt;&lt;td class="field-body"&gt;Palavra escolhida&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;vermelho:&lt;/th&gt;&lt;td class="field-body"&gt;Termos mais próximos pela similaridade de cossenos&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;azul:&lt;/th&gt;&lt;td class="field-body"&gt;Termos mais próximos pela distância euclidiana&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;roxo:&lt;/th&gt;&lt;td class="field-body"&gt;Termos que ambas as métricas concordam&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/files/word2vec-3-skipgram.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>word2vec</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-3-skip-gram/</guid><pubDate>Fri, 07 Dec 2018 04:43:36 GMT</pubDate></item><item><title>SVD vs PCA</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/svd-vs-pca/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Não vou tratar aqui de como se implementa o PCA e o SVD, prefiro indicar esses tutoriais abaixo, eles foram muito bem escritos e são muito claros sobre como são os cálculos usados:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://sebastianraschka.com/Articles/2014_pca_step_by_step.html"&gt;tutorial PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/"&gt;tutorial SVD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Embora esses métodos possam ser usados para compressão de dados, análises populacionais e uma infinidade de análises envolvendo dados organizados em matrizes, aqui prefiro comparar cada método e discutir o uso voltado à redução de dimensões a fim que possamos visualizar os dados dessas anotações,&lt;/p&gt;
&lt;p&gt;mas antes de chegar nas discussões, vamos ver alguns gráficos mostrando o que o SVD e o PCA retornaram quando os usamos para reduzir dimensões de matrizes:&lt;/p&gt;
&lt;img alt="/images/svd_pca_0_3d.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_0_3d.png"&gt;
&lt;img alt="/images/svd_pca_1_3dreduction.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_1_3dreduction.png"&gt;
&lt;p&gt;curiosamente vemos que ocorreu uma rotação no gráfico do PCA e que o gráfico do SVD mantém uma certa similaridade visual com o gráfico original em 3D. Só compreendi melhor vendo &lt;a class="reference external" href="https://www.quora.com/What-is-the-difference-between-PCA-and-SVD/answer/Adarsh-131"&gt;esta resposta no Quora&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Geometrically PCA corresponds to “centering the dataset”, and then rotating it to align the axis of highest variance with the principle axis."&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Geometricamente, PCA corresponde a "centralização do dataset", e depois rotaciona para alinhar o eixo de maior variância com o eixo principal&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Lógico que nem sempre acontece de ambos as representações ficarem tão diferentes, para observar melhor isso resolvi seguir um &lt;a class="reference external" href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py"&gt;exemplo da documentação do sklearn&lt;/a&gt;&lt;/p&gt;
&lt;img alt="/images/svd_pca_2_64reduction.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_2_64reduction.png"&gt;
&lt;p&gt;A imagem acima mostra que deve ter coincidido a forma como o SVD reduziu as dimensões e a rotação feita pelo PCA, só lembrando o que está de forma muito explícita no link para o Quora: o PCA usa o SVD para criar um ranking, afinal PCA significa "análise do componente principal" e o SVD fornece um dos passos para chegar ao componente pricipal.&lt;/p&gt;
&lt;p&gt;Mas o KMeans realiza um aprendizado não supervisionado, e ainda especialmente neste caso onde a redução de 64 dimensões para 2 com certeza não deu margem para que os dados fossem linearmente separáveis, resolvi usar o SVM para desenhar o espaço para cada classe.&lt;/p&gt;
&lt;img alt="/images/svd_pca_3_svm.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_3_svm.png"&gt;
&lt;p&gt;Algo que se deve ressaltar no gráfico acima é que os pontos semi-transparentes que adicionei ao gráfico são os que os classificadores treinados erraram, sobre isso repare no resultado abaixo:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_7aa4db47292d401e82ab919229f5dbb5-1"&gt;&lt;/a&gt;erros SVD: 704 de 1797
&lt;a name="rest_code_7aa4db47292d401e82ab919229f5dbb5-2"&gt;&lt;/a&gt;erros PCA: 704 de 1797
&lt;a name="rest_code_7aa4db47292d401e82ab919229f5dbb5-3"&gt;&lt;/a&gt;erros normal: 0 de 1797
&lt;a name="rest_code_7aa4db47292d401e82ab919229f5dbb5-4"&gt;&lt;/a&gt;-----------------------
&lt;a name="rest_code_7aa4db47292d401e82ab919229f5dbb5-5"&gt;&lt;/a&gt;percentuais de acertos:
&lt;a name="rest_code_7aa4db47292d401e82ab919229f5dbb5-6"&gt;&lt;/a&gt;&amp;gt; SVD: 60.824%
&lt;a name="rest_code_7aa4db47292d401e82ab919229f5dbb5-7"&gt;&lt;/a&gt;&amp;gt; pca: 60.824%
&lt;a name="rest_code_7aa4db47292d401e82ab919229f5dbb5-8"&gt;&lt;/a&gt;&amp;gt; normal: 100.000%
&lt;/pre&gt;&lt;p&gt;Considerei "normal" como a aplicação do SVM sem reduzir as dimensões. Estes resultados mostram que a sobreposição de dados na redução de dimensões assim como a distorção que ocorre nas transformações feitas com as matrizes, tende a dificultar o trabalho dos algoritmos, mesmo mantendo um certo nível de fidelidade com a distribuição original dos dadosm o melhor é usar essa redução mais para visualizar do que para aplicar métricas ou classificadores, e por isso também que nas notas onde uso distância euclidiana e similaridade de cossenos, ao reduzir as dimensões os resultados parecem errados ainda que nas dimensões originais esteja correto.&lt;/p&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/files/SVD-PCA.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>utils</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/svd-vs-pca/</guid><pubDate>Fri, 07 Dec 2018 04:26:29 GMT</pubDate></item><item><title>Word2Vec 2: CBOW</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Na anotação anterior vimos de forma mais ou menos prática o sentido da coisa, implementamos o Word2Vec com o objetivo de identificar a proximidade semântica entre palavras com base no uso em textos, este post é fundamentalmente teórico e a implementação do cbow aqui demonstrada está muito longe de ser algo pronto para produção, é apenas um exemplo que tenta ser didático.&lt;/p&gt;
&lt;div class="section" id="preparacao-dos-dados"&gt;
&lt;h2&gt;preparação dos dados&lt;/h2&gt;
&lt;p&gt;Como nosso objetivo é fazer com que uma rede neural receba as palavras de contexto e indique a palavra central, e na anotação anterior fiz uma pequena observação dizendo que sempre teremos $2w$ palavras de contexto para cada palavra central e assim faremos, vamos modificar um pouco o código que cria os pares do word2vec:&lt;/p&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-12"&gt;12&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-13"&gt;13&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;text_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;center_word_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;context_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_a67276fcd0734d1eaed76a2f0cdc26fd-13"&gt;&lt;/a&gt;    &lt;span class="n"&gt;pair_ids&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;context_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;center_word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Assim feito, teremos algo como:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;contexto&lt;/th&gt;
&lt;th class="head"&gt;central&lt;/th&gt;
&lt;th class="head"&gt;contexto&lt;/th&gt;
&lt;th class="head"&gt;central&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;[155, 77, 577, 495]&lt;/td&gt;
&lt;td&gt;544&lt;/td&gt;
&lt;td&gt;['armazenado', 'ace', 'turing', 'interessou']&lt;/td&gt;
&lt;td&gt;posteriormente&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[77, 544, 495, 233]&lt;/td&gt;
&lt;td&gt;577&lt;/td&gt;
&lt;td&gt;['ace', 'posteriormente', 'interessou', 'química']&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[544, 577, 233, 308]&lt;/td&gt;
&lt;td&gt;495&lt;/td&gt;
&lt;td&gt;['posteriormente', 'turing', 'química', 'escreveu']&lt;/td&gt;
&lt;td&gt;interessou&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[577, 495, 308, 446]&lt;/td&gt;
&lt;td&gt;233&lt;/td&gt;
&lt;td&gt;['turing', 'interessou', 'escreveu', 'artigo']&lt;/td&gt;
&lt;td&gt;química&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[495, 233, 446, 537]&lt;/td&gt;
&lt;td&gt;308&lt;/td&gt;
&lt;td&gt;['interessou', 'química', 'artigo', 'sobre']&lt;/td&gt;
&lt;td&gt;escreveu&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[233, 308, 537, 323]&lt;/td&gt;
&lt;td&gt;446&lt;/td&gt;
&lt;td&gt;['química', 'escreveu', 'sobre', 'base']&lt;/td&gt;
&lt;td&gt;artigo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[308, 446, 323, 233]&lt;/td&gt;
&lt;td&gt;537&lt;/td&gt;
&lt;td&gt;['escreveu', 'artigo', 'base', 'química']&lt;/td&gt;
&lt;td&gt;sobre&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[446, 537, 233, 504]&lt;/td&gt;
&lt;td&gt;323&lt;/td&gt;
&lt;td&gt;['artigo', 'sobre', 'química', 'morfogênese']&lt;/td&gt;
&lt;td&gt;base&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[537, 323, 504, 506]&lt;/td&gt;
&lt;td&gt;233&lt;/td&gt;
&lt;td&gt;['sobre', 'base', 'morfogênese', 'previu']&lt;/td&gt;
&lt;td&gt;química&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[323, 233, 506, 492]&lt;/td&gt;
&lt;td&gt;504&lt;/td&gt;
&lt;td&gt;['base', 'química', 'previu', 'reações']&lt;/td&gt;
&lt;td&gt;morfogênese&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[233, 504, 492, 8]&lt;/td&gt;
&lt;td&gt;506&lt;/td&gt;
&lt;td&gt;['química', 'morfogênese', 'reações', 'químicas']&lt;/td&gt;
&lt;td&gt;previu&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="a-rede-neural"&gt;
&lt;h2&gt;A rede neural&lt;/h2&gt;
&lt;p&gt;O que importa na rede neural neste método e no skip-gram é a camada &lt;em&gt;Embedding&lt;/em&gt;&lt;/p&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-12"&gt;12&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-13"&gt;13&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-14"&gt;14&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-15"&gt;15&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-16"&gt;16&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-17"&gt;17&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-18"&gt;18&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-19"&gt;19&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-20"&gt;20&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-21"&gt;21&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-22"&gt;22&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_90fd5f98df93494996d2f7fa3d64a3a7-23"&gt;23&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-3"&gt;&lt;/a&gt;        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-5"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-7"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;context_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-8"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-10"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LogSoftmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-16"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-19"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-20"&gt;&lt;/a&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-21"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_word_emb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word_id&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-22"&gt;&lt;/a&gt;        &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LongTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_90fd5f98df93494996d2f7fa3d64a3a7-23"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;O treinamento será demorado, afinal como já dito, este não é um código para produção, é apenas um código didático, então enquanto ocorre o treinamento, não é má idéia ir tomar um chá e caminhar um pouco.&lt;/p&gt;
&lt;p&gt;Algo que preciso ressaltar aqui é que predizer a palavra central corretamente não importa tanto, o importante é que esteja ocorrendo o aprendizado já queo que nos interessa é que os valores da camada incorporada se aproximem em palavras próximas e se distanciem para palavras distantes, então é de se esperar um gráfico horrível mostrando a evolução da perda.&lt;/p&gt;
&lt;img alt="/images/word2vec-cbow-loss.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-cbow-loss.png"&gt;
&lt;p&gt;Para visualizar a distribuição das palavras num plano cartesiano, faremos o mesmo que com o Gensim, usaremos a implementação do PCA disponível no slearn.&lt;/p&gt;
&lt;img alt="/images/word2vec-cbow-1.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-cbow-1.png"&gt;
&lt;p&gt;Observando a similaridade, que não é lá tão boa neste caso devido a total falta de otimização em tudo no código:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;rank sim cos&lt;/th&gt;
&lt;th class="head"&gt; &lt;/th&gt;
&lt;th class="head"&gt;rank dist eucl&lt;/th&gt;
&lt;th class="head"&gt; &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;novas&lt;/td&gt;
&lt;td&gt;0.28059&lt;/td&gt;
&lt;td&gt;novas&lt;/td&gt;
&lt;td&gt;0.09326&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;equivalia&lt;/td&gt;
&lt;td&gt;0.31309&lt;/td&gt;
&lt;td&gt;polonesa&lt;/td&gt;
&lt;td&gt;0.09989&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;pioneiro&lt;/td&gt;
&lt;td&gt;0.31798&lt;/td&gt;
&lt;td&gt;neve&lt;/td&gt;
&lt;td&gt;0.10029&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;afirma&lt;/td&gt;
&lt;td&gt;0.32445&lt;/td&gt;
&lt;td&gt;andrew&lt;/td&gt;
&lt;td&gt;0.10191&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;neve&lt;/td&gt;
&lt;td&gt;0.33447&lt;/td&gt;
&lt;td&gt;pioneiro&lt;/td&gt;
&lt;td&gt;0.10310&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;polonesa&lt;/td&gt;
&lt;td&gt;0.33585&lt;/td&gt;
&lt;td&gt;afirma&lt;/td&gt;
&lt;td&gt;0.10484&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;massachusetts&lt;/td&gt;
&lt;td&gt;0.34675&lt;/td&gt;
&lt;td&gt;conduzida&lt;/td&gt;
&lt;td&gt;0.10508&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;conduzida&lt;/td&gt;
&lt;td&gt;0.34768&lt;/td&gt;
&lt;td&gt;bombas&lt;/td&gt;
&lt;td&gt;0.10641&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;andrew&lt;/td&gt;
&lt;td&gt;0.35143&lt;/td&gt;
&lt;td&gt;manipular&lt;/td&gt;
&lt;td&gt;0.10718&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;hastings&lt;/td&gt;
&lt;td&gt;0.35665&lt;/td&gt;
&lt;td&gt;homossexuais&lt;/td&gt;
&lt;td&gt;0.11074&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Observando onde cada termo está com as dimensões da camada incorporada da rede neural reduzida a 2d temos:&lt;/p&gt;
&lt;img alt="/images/word2vec-cbow-rank.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-cbow-rank.png"&gt;
&lt;p&gt;É compreensível ver estas distâncias tão em desarcodo pelo fato das distorções da redução de dimensões, de 10 para 2.&lt;/p&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/files/word2vec-2-cbow.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>word2vec</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/</guid><pubDate>Fri, 07 Dec 2018 03:23:12 GMT</pubDate></item><item><title>Word2Vec 1: Introdução</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-1-introducao/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;O Word2Vec parte de uma idéia muito simples e até certo ponto bastante lógica: relacionar uma palavra com as que estão em sua volta num texto. A partir desse conceito tão básico o Word2Vec acaba sendo uma base para outros algoritmos e não necessariamente um fim em si, a partir dele vamos implementar o cbow e o skip-gram nas anotações seguintes, por hora, vamos entender como funciona a criação dos pares que são a base do Word2Vec.&lt;/p&gt;
&lt;div class="section" id="pares"&gt;
&lt;h2&gt;Pares&lt;/h2&gt;
&lt;p&gt;vamos imaginar que já tenhamos feito todo o processo descrito no post de introdução a esta série. O que buscamos nesta etapa é apenas definir uma "janela" que será a quantidade de palavras vizinhas à uma palavra que chamaremos de central e criar pares ligando essa palavra central às vizinhas, lógico que no código real trabalharemos com ids que representam palavras e não com as palavras em si.&lt;/p&gt;
&lt;p&gt;ex.:&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;O cachorro comeu o trabalho da faculdade de novo&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;considerando a janela &lt;cite&gt;w = 2&lt;/cite&gt; teríamos:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_526626d3c7ba4139898d9ed63345cd8b-1"&gt;&lt;/a&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;a name="rest_code_526626d3c7ba4139898d9ed63345cd8b-2"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"comeu"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"o"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_526626d3c7ba4139898d9ed63345cd8b-3"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"comeu"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"cachorro"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_526626d3c7ba4139898d9ed63345cd8b-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"comeu"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"o"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_526626d3c7ba4139898d9ed63345cd8b-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"comeu"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"trabalho"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_526626d3c7ba4139898d9ed63345cd8b-6"&gt;&lt;/a&gt;    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;a name="rest_code_526626d3c7ba4139898d9ed63345cd8b-7"&gt;&lt;/a&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Coisas óbvias a se deduzir: a partir da palavra central, as vezes que ela aparece é sempre &lt;cite&gt;2*w&lt;/cite&gt; e em relação às vizinhas, que chamamos de palavras de contexto, a proporção sempre será de &lt;cite&gt;2*w&lt;/cite&gt; para cada palavra central, isso será importante para o cbow e para o skip-gram.&lt;/p&gt;
&lt;p&gt;Traduzindo esse procedimento bem básico em código, teremos:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="c1"&gt;# janela (window)&lt;/span&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;text_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;center_word_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-11"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-12"&gt;&lt;/a&gt;        &lt;span class="n"&gt;context_word_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;pair_ids&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;center_word_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context_word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_0be0045f8c264a74b819777352ed8f76-15"&gt;&lt;/a&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Esse será exatamente o código que teremos no método skip-gram. Mas por enquanto vamos aproveitar os métodos que usam o word2vec já implementados e vamos ver o que podemos extrair deles:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="gensim"&gt;
&lt;h2&gt;Gensim&lt;/h2&gt;
&lt;p&gt;No Gensim as operações são muito simples, basta passar para ele o texto processado de acordo com a introdução a este material:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_d0cdd617174b4940954cc31c2f6f0a22-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;model_sg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gensim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Word2Vec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;compute_loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sg&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d0cdd617174b4940954cc31c2f6f0a22-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;model_cb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gensim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Word2Vec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;compute_loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sg&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;No momento de criar o objeto, a única diferença nos parâmetros usados é no &lt;cite&gt;sg&lt;/cite&gt; que a essa altura já está claro que signfica skip-gram e em vez de usar True ou False, usamos 1 ou 0 para definir qual método será usado.&lt;/p&gt;
&lt;p&gt;A diferença real deles está no input e output pois ambos, cbow e skip-gram, são apenas redes neurais com pouquíssima diferença entre si como será visto posteiormente.&lt;/p&gt;
&lt;p&gt;No cbow buscamos predizer a palavra central a partir das palavras de contexto e no skip-gram fazemos o contrário, a partir da palavra central buscamos prever as palavras de contexto.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_a82057057918435989831cfafe22a7ae-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;model_sg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_examples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_a82057057918435989831cfafe22a7ae-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;model_cb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_examples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Na prática, a função do treinamento é, a partir da proximidade entre as palavras, as camadas da rede neural vão se ajustando o que acaba indicando a proximidade de sentido entre elas, indo para um exemplo clássico queremos que seja possível, através de uma distribuição no plano cartesiano que o meio do caminho entre as palavras "rei" e "mulher" seja "rainha".&lt;/p&gt;
&lt;p&gt;## visualizando&lt;/p&gt;
&lt;p&gt;Primeiro vamos ver as dimensões na saída para cada palavra:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_c5bedb88ad364abf92cec5c13f9eb077-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;model_sg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"turing"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;a name="rest_code_c5bedb88ad364abf92cec5c13f9eb077-2"&gt;&lt;/a&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Como podemos perceber, nos é impossível fazer uma visualização de algo em 100 dimensões, para reduzi para 2 dimensões vamos usar o sklearn com a classe PCA, como o sklearn mantém o mesmo procedimento para praticamente tudo, vou me abster de colocar o código aqui que pode ser visto no jupyter notebook com o código completo. O importante é que ao final teremos esses gráficos para cada método:&lt;/p&gt;
&lt;p&gt;obs: queria fazer algo mais interativo mas não consegui no momento&lt;/p&gt;
&lt;img alt="/images/word2vec-1.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-1.png"&gt;
&lt;p&gt;O Gensim já tem métodos nos objetos formados para encontrar as palavras mais próximas usando a similaridade de cossenos:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# repare que quanto mais próximo de 1, mais similar&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-2"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"cianeto"&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;model_sg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_similar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s1"&gt;'corpo'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9956434965133667&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-5"&gt;&lt;/a&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'envenenamento'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9950364828109741&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-6"&gt;&lt;/a&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'apesar'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9946295022964478&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-7"&gt;&lt;/a&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'aparente'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9940468668937683&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-8"&gt;&lt;/a&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'presença'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9939732551574707&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-9"&gt;&lt;/a&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'descoberto'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9937050342559814&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-10"&gt;&lt;/a&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'níveis'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9936593770980835&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-11"&gt;&lt;/a&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'quanto'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.993450403213501&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-12"&gt;&lt;/a&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'testada'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9933900833129883&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_bc40e2e6e9494a90b2c258f0ec39f150-13"&gt;&lt;/a&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'determinar'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9930295944213867&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Agora comparando o CBOW e o Skip-Gram:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_a6cd946b1b774f6a90a6200fdcaa2eec-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"morte"&lt;/span&gt;
&lt;a name="rest_code_a6cd946b1b774f6a90a6200fdcaa2eec-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_a6cd946b1b774f6a90a6200fdcaa2eec-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;sg_similar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_sg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;similar_by_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_a6cd946b1b774f6a90a6200fdcaa2eec-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;cb_similar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_cb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;similar_by_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_a6cd946b1b774f6a90a6200fdcaa2eec-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_a6cd946b1b774f6a90a6200fdcaa2eec-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;md&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"| skip-gram | cbow |&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;|--|--|&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
&lt;a name="rest_code_a6cd946b1b774f6a90a6200fdcaa2eec-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sg_similar&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cb_similar&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_a6cd946b1b774f6a90a6200fdcaa2eec-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;md&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"| {i[0][0]} |  {i[1][0]} |&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
&lt;a name="rest_code_a6cd946b1b774f6a90a6200fdcaa2eec-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_a6cd946b1b774f6a90a6200fdcaa2eec-10"&gt;&lt;/a&gt;&lt;span class="n"&gt;Markdown&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="59%"&gt;
&lt;col width="41%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;skip-gram&lt;/th&gt;
&lt;th class="head"&gt;cbow&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;causa&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;defende&lt;/td&gt;
&lt;td&gt;maçã&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;setembro&lt;/td&gt;
&lt;td&gt;suicídio&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;acidental&lt;/td&gt;
&lt;td&gt;após&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;estabeleceu&lt;/td&gt;
&lt;td&gt;cianeto&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;campanha&lt;/td&gt;
&lt;td&gt;computador&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;necessariamente&lt;/td&gt;
&lt;td&gt;onde&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;copeland&lt;/td&gt;
&lt;td&gt;ser&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;suicídio&lt;/td&gt;
&lt;td&gt;anos&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;resultado&lt;/td&gt;
&lt;td&gt;ter&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/files/word2vec-1-introducao.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>word2vec</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-1-introducao/</guid><pubDate>Thu, 06 Dec 2018 09:13:12 GMT</pubDate></item><item><title>Pré-processamento de textos</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Este é o processo padrão usado em praticamente todas as anotações relacionadas à NLP:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;limpar o texto:&lt;ul&gt;
&lt;li&gt;remover pontuação, acentos, e stop-words &lt;a class="footnote-reference" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#id2" id="id1"&gt;[1]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;colocar tudo em minúsculas&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;converter numa lista de termos usados.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A única excessão é com o TF-IDF e LSA e comparo quando o processamento é feito dividindo em parágrafos e com o texto inteiro de uma vez.&lt;/p&gt;
&lt;p&gt;Sempre usarei textos da wikipédia, pelo simples motivo de ser muito prático e inteiramente legal.&lt;/p&gt;
&lt;div class="section" id="bibliotecas-usadas"&gt;
&lt;h2&gt;bibliotecas usadas&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_3bc60b40828349ffbb1fac68228055ea-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;pylab&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;a name="rest_code_3bc60b40828349ffbb1fac68228055ea-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;nltk&lt;/span&gt;
&lt;a name="rest_code_3bc60b40828349ffbb1fac68228055ea-3"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;gensim&lt;/span&gt;
&lt;a name="rest_code_3bc60b40828349ffbb1fac68228055ea-4"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;wikipedia&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;No Jupyter notebook o comando &lt;strong&gt;%pylab&lt;/strong&gt; importa o matplotlib e numpy e configura o modo como os gráficos serão apresentados, ocasionalmente também usarei o Altair junto com o Pandas para visualizar os dados.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pre-processamento"&gt;
&lt;h2&gt;Pré-processamento&lt;/h2&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#rest_code_6400b3ee4fb2448490e9e3400744b5bf-12"&gt;12&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;wikipedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_lang&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"pt"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;wikipedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Alan_Turing"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;sentences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;stop_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corpus&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stopwords&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"portuguese"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;\
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-7"&gt;&lt;/a&gt;             &lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corpus&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stopwords&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"english"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitlines&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;clean_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gensim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;simple_preprocess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;clean_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;clean_text&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;stop_words&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_6400b3ee4fb2448490e9e3400744b5bf-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clean_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Explicando as etapas do código acima:&lt;/p&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name"&gt;
&lt;col class="field-body"&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;linhas 6 e 7:&lt;/th&gt;&lt;td class="field-body"&gt;lista de stop-words, como no texto há termos em inglês, juntei as duas listas (termos em português e em inglês) numa só.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;for:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;strong&gt;splitlines&lt;/strong&gt; vai dividir o texto em parágrafod e a função &lt;strong&gt;simple_preprocess()&lt;/strong&gt; do Gensim remove pontuação e converte tudo para minúsculas, em seguida removo as stop words e por último adiciono o parágrafo à lista de sentenças usadas no texto.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="facilitando-as-coisas"&gt;
&lt;h2&gt;Facilitando as coisas&lt;/h2&gt;
&lt;p&gt;Para dar maior foco ao que importa, salvei a lista de termos usando o pickle:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-1"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pickle&lt;/span&gt;
&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-3"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# salvando em arquivo&lt;/span&gt;
&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"sentences.pickle"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"wb"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-5"&gt;&lt;/a&gt;   &lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-6"&gt;&lt;/a&gt;   &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-8"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# lendo do arquivo&lt;/span&gt;
&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-9"&gt;&lt;/a&gt;&lt;span class="n"&gt;sentences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-10"&gt;&lt;/a&gt;   &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"sentences.pickle"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"rb"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_74027c78de65465587bf568e4dad0202-11"&gt;&lt;/a&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="footnotes"&gt;
&lt;h2&gt;footnotes&lt;/h2&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;stop words são as palavras sem valor semântico ao que pretendemos fazer, são palavras como "eu", "está", "era", "têm", etc. São palavras de uso tão comum e frequente que acabaria por ofuscar a presença de palavras mais relevantes no processo de classificação de textos por exemplo, afinal para saber o sentido de frases como "Alan Turing é o pai da ciência da computação" basta apenas as palavras ["Alan", "Turing", "pai", "ciência", "computação"], isso é o que basta para uma máquina.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/"&gt;teste&lt;/a&gt;&lt;/p&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/files/preprocessing.py"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>utils</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos/</guid><pubDate>Thu, 06 Dec 2018 06:03:53 GMT</pubDate></item><item><title>README</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Pretendo fazer uma longa série de posts sobre NLP, não sou especialista nisso e podemos considerar os posts mais como anotações de estudo do que tutoriais ou manuais. O Índice abaixo será atualizado à medida que eu for publicando novos conteúdos, a idéia é seguir o andamento histórico de cada parte, na 1ª parte começaremos com o tf-idf para depois seguirmos para o word2vec e glove:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;parte 1: vetorização&lt;/dt&gt;
&lt;dd&gt;&lt;ul class="first last"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa"&gt;estatística: tf-idf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-1-introducao"&gt;word2vec 1: introdução&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow"&gt;word2vec 2: cbow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-3-skip-gram"&gt;word2vec 3: skip-gram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;glove&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;seq2seq&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;notas finais e comparações entre métodos&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;utils&lt;/dt&gt;
&lt;dd&gt;&lt;ul class="first last"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/pre-processamento-de-textos"&gt;Pré-processamento de textos&lt;/a&gt;. (&lt;em&gt;muito importante&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/svd-vs-pca"&gt;SVD vs PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/"&gt;distância euclidiana vs similaridade de cossenos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Obs1.: O pré-processamento é a etapa inicial de praticamente todos os conteúdos aqui escritos, é realmente muito importante, por isso antes de partir para qualquer outro conteúdo, leia ele primeiro.&lt;/p&gt;
&lt;p&gt;Obs2.: O que estiver em itálico é que ainda não escrevi mas devo fazer ao longo dessas semanas. As partes 2 e 3 possivelmente serão sobre classificação de textos e modelagem com cadeias de markov e RNN.&lt;/p&gt;&lt;/div&gt;</description><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/</guid><pubDate>Thu, 06 Dec 2018 05:46:15 GMT</pubDate></item></channel></rss>