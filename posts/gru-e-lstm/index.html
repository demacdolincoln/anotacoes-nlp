<!DOCTYPE html>
<html prefix="" lang="pt_br">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>GRU e LSTM | Anotações sobre NLP</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/dark.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Share+Tech+Mono" rel="stylesheet">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><script src="https://cdn.jsdelivr.net/npm/vega@4.4.0"></script><script src="https://cdn.jsdelivr.net/npm/vega-lite@2.6.0"></script><script src="https://cdn.jsdelivr.net/npm/vega-embed@3.24.2"></script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/languages/python.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/languages/julia.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script><meta name="author" content="Lincoln de Macêdo">
<link rel="prev" href="../seq2seq-implementacao/" title="Seq2Seq - Implementação" type="text/html">
<link rel="next" href="../classificacao-2-cnn/" title="Classificação 2: CNN" type="text/html">
<meta property="og:site_name" content="Anotações sobre NLP">
<meta property="og:title" content="GRU e LSTM">
<meta property="og:url" content="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/">
<meta property="og:description" content="obs: Não vou me demorar tratando de questões teóricas sobre as RNN em si, já que o foco dessas anotações é NLP, futuramente, ao concluir essas anotações talvez eu inicie uma série mais abrangente sobr">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-12-24T02:13:54-03:00">
</head>
<body class="hack dark">

<a href="#content" class="sr-only sr-only-focusable">Pular para o conteúdo principal</a>
    <div id="container">
         
    <header id="header"><h1 id="brand"><a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/" title="Anotações sobre NLP" rel="home">

        <span id="blog-title">Anotações sobre NLP</span>
    </a></h1>

        

        
    <nav id="menu"><ul>
<li><a href="../../archive.html">Arquivo</a></li>
                <li><a href="../../categories/">Etiqueta</a></li>
                <li><a href="../../rss.xml">Feed RSS</a></li>

    

    
    
    </ul></nav></header><main id="content"><article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">GRU e LSTM</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Lincoln de Macêdo
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2018-12-24T02:13:54-03:00" itemprop="datePublished" title="2018-12-24 02:13">2018-12-24 02:13</time></a>
            </p>
                <p class="commentline">
        
    <a href="#disqus_thread" data-disqus-identifier="cache/posts/gru-e-lstm.html">Comments</a>


            
        </p>
<p class="sourceline"><a href="index.rst" class="sourcelink">Código</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>obs: Não vou me demorar tratando de questões teóricas sobre as RNN em si, já que o foco dessas anotações é NLP, futuramente, ao concluir essas anotações talvez eu inicie uma série mais abrangente sobre machine learning, mas por enquanto apenas traterei de assuntos teóricos relativos a NLP e ao resto apenas uma abordagem prática.</p>
<p>Parece meio óbvio dizer mas o que define uma rede neural recorrente é exatamente a recorrência, isto é, informações que são armazenadas e depois reutilizadas, a forma mais simples de implementação consiste em criar uma camada (um array) que armazena os resultados e é utilizado normalmente no processamento dos dados que passam por uma rede neural, só que com 1 caracérística distinta: haver uma condição ou não para atualizar esses dados aprendidos ao longo do treinamento e uma condição que a informação armazenada seja utilizada.</p>
<p>Devido a esse mecanismo ser claramente um gerenciamento de memória e estarmos tratando de redes neurais, não demora muito para começarmos a associar ao modo como nosso cérebro gerencia a memória, desta forma vem ao mundo em 1997 o LSTM (Long Short-Term Memory) <a href="#id3"><span class="problematic" id="id1">[1]_</span></a> <a href="#id4"><span class="problematic" id="id2">[2]_</span></a>, e como o nome bem indica, se trata de gerenciamento de memória de curto e longo prazo, mas como ele faz isso?</p>
<div class="section" id="portoes">
<h2>Portões</h2>
<img alt="https://commons.wikimedia.org/wiki/File:The_LSTM_cell.png#/media/File:The_LSTM_cell.png" src="https://commons.wikimedia.org/wiki/File:The_LSTM_cell.png#/media/File:The_LSTM_cell.png"><p>Traduzindo as funções expostas na imagem acima disponível na wikipedia representando uma célula de memória LTSM:</p>
<div class="math">
\begin{equation*}
\sigma = \frac{1}{1 + exp(-x)}
\end{equation*}
</div>
<div class="math">
\begin{equation*}
tanh = \frac{e^x - e^-x}{e^x + e^{-x}}
\end{equation*}
</div>
<img alt="/images/lstm_gru-tanh-sigmoid.png" src="../../images/lstm_gru-tanh-sigmoid.png"><p>Vemos que a diferença entre os gráficos dessas funções é essencialmente o limite quando tende a menos infinito, e isso faz toda a diferença, pois um limite que tende a 0 significa que posteriormente qualquer número multiplicado por 0 será 0, neste caso a função sigmoide indica a relevância de cada dimensão de entrada, se a dimensão for próxima a zero, ela vai perdendo relevância até desaparecer ou ser substituída por outra informação mais relevante (decidida pela função tanh).</p>
<p>Todo o mecanismo de preservação e esquecimento desses métodos se baseia nesses "portões" que é como são chamadas as camadas com a função sigmoide, enquanto a função tanh tem o dever de fazer as escolhas finais, repare nas somas e multiplicações que unem o fluxo da saída com a camada oculta que armazena a memória, como a última estapa das operações com o estado da célula é sempre uma soma e os anteriores são multiplicações, isso revela a alteração dos pesos para definir a importância de cada dimensão e posteriormente a atualização mantendo assim para a época atual do treinamento da rede neural, a memória de curto prazo (os espaços próximos a zero que após a soma se mantém próximos aos resultados mais recentes) e a memória de longo prazo (o conteúdo mais relevante deixado mais próximo de 1)</p>
<p>---</p>
</div>
<div class="section" id="artigos-e-links-recomendados">
<h2>artigos e links recomendados</h2>
<p>[1] <cite>Learning to Forget: Continual Prediction with LSTM ( Felix A. Gers , Jürgen Schmidhuber , Fred Cummins ) &lt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.5709&gt;</cite></p>
<p>[2] <cite>Long short-term memory (Sepp Hochreiter; Jürgen Schmidhuber) &lt;https://www.researchgate.net/profile/Sepp_Hochreiter/publication/13853244_Long_Short-term_Memory/links/5700e75608aea6b7746a0624/Long-Short-term-Memory.pdf?origin=publication_detail&gt;</cite></p>
<p>[3] <cite>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation ( Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua) &lt;https://arxiv.org/pdf/1406.1078v3.pdf&gt;</cite></p>
</div>
<div class="system-messages section">
<h2>Docutils System Messages</h2>
<div class="system-message" id="id3">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">&lt;string&gt;</tt>, line 6); <em><a href="#id1">backlink</a></em></p>
Unknown target name: "1".</div>
<div class="system-message" id="id4">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">&lt;string&gt;</tt>, line 6); <em><a href="#id2">backlink</a></em></p>
Unknown target name: "2".</div>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul class="pager hidden-print">
<li class="previous">
                <a href="../seq2seq-implementacao/" rel="prev" title="Seq2Seq - Implementação">Post anterior</a>
            </li>
            <li class="next">
                <a href="../classificacao-2-cnn/" rel="next" title="Classificação 2: CNN">Próximo post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comentários</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="demacdolincoln",
            disqus_url="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/",
        disqus_title="GRU e LSTM",
        disqus_identifier="cache/posts/gru-e-lstm.html",
        disqus_config = function () {
            this.language = "pt_br";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="demacdolincoln";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script></main><footer id="footer"><p>Contents © 2018         <a href="mailto:demacdolincoln@gmail.com">Lincoln de Macêdo</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer>
</div>
    
    

    
    
    
</body>
</html>
