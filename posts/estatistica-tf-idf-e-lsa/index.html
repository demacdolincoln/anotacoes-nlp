<!DOCTYPE html>
<html prefix="" lang="pt_br">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Estatística: TF-IDF e LSA | Anotações sobre NLP</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/dark.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Share+Tech+Mono" rel="stylesheet">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><script src="https://cdn.jsdelivr.net/npm/vega@4.4.0"></script><script src="https://cdn.jsdelivr.net/npm/vega-lite@2.6.0"></script><script src="https://cdn.jsdelivr.net/npm/vega-embed@3.24.2"></script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/languages/python.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/languages/julia.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script><meta name="author" content="Lincoln de Macêdo">
<link rel="prev" href="../word2vec-3-skip-gram/" title="word2vec 3: skip-gram" type="text/html">
<link rel="next" href="../distancia-euclidiama-vs-similaridade-de-cossenos/" title="Distância Euclidiama vs Similaridade de Cossenos" type="text/html">
<meta property="og:site_name" content="Anotações sobre NLP">
<meta property="og:title" content="Estatística: TF-IDF e LSA">
<meta property="og:url" content="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/">
<meta property="og:description" content="Antes da popularidade de métodos baseados em IA, muito também devido à capacidade dos computadores da época, o que restava para análises de texto era quantificar as palavras e buscar extrair estatísti">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-12-07T01:47:59-03:00">
<meta property="article:tag" content="utils">
</head>
<body class="hack dark">

<a href="#content" class="sr-only sr-only-focusable">Pular para o conteúdo principal</a>
    <div id="container">
         
    <header id="header"><h1 id="brand"><a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/" title="Anotações sobre NLP" rel="home">

        <span id="blog-title">Anotações sobre NLP</span>
    </a></h1>

        

        
    <nav id="menu"><ul>
<li><a href="../../archive.html">Arquivo</a></li>
                <li><a href="../../categories/">Etiqueta</a></li>
                <li><a href="../../rss.xml">Feed RSS</a></li>

    

    
    
    </ul></nav></header><main id="content"><article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Estatística: TF-IDF e LSA</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Lincoln de Macêdo
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2018-12-07T01:47:59-03:00" itemprop="datePublished" title="2018-12-07 01:47">2018-12-07 01:47</time></a>
            </p>
                <p class="commentline">
        
    <a href="#disqus_thread" data-disqus-identifier="cache/posts/estatistica-tf-idf-e-lsa.html">Comments</a>


            
        </p>
<p class="sourceline"><a href="index.rst" class="sourcelink">Código</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>Antes da popularidade de métodos baseados em IA, muito também devido à capacidade dos computadores da época, o que restava para análises de texto era quantificar as palavras e buscar extrair estatísticas, o mais básico e fundamental talvez seja o TF-IDF e por isso este post.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name">
<col class="field-body">
<tbody valign="top"><tr class="field">
<th class="field-name">tf-idf:</th>
<td class="field-body"><em>frequency-inverse document frequency</em></td>
</tr></tbody>
</table>
<p>Este método se resume a contar a frequência de uso de palavras e realizar um cálculo que gere uma estimativa de uso/importância da palavra no texto, de certa forma ele se conecta à <a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law">Lei de Zipf</a> que trata justamente de uma análise da frequência de palavras.</p>
<div class="math">
\begin{equation*}
TF(t) = \frac{nº\ de\ vezes\ que\ t\ aparece\ no\ texto}{total\ de\ termos\ no\ texto}
\end{equation*}
</div>
<div class="math">
\begin{equation*}
IDF(t) = log_e(\frac{quantidade\ total\ de\ textos}{numero\ de\ textos\ em\ que\ t\ aparece})
\end{equation*}
</div>
<p>Recomendo bastante a wikipédia em inglês, há bastante exemplos de cálculos variantes: <a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a></p>
<p>Logicamente há inconsistências, afinal apenas a frequência não alcança o uso das palavras, não indica necessariamente as mais significativas se uma pessoa em vez de fazer referências a uma palavra ficar repetindo a mesma coisa o tempo todo. ex.:</p>
<!--  -->
<blockquote>
<p>"há filmes bons, ruins e medianos, mas o filme em questão é o pior de todos, o filme é tão chato e cansativo que todos dormem assistindo os primeiros minutos do filme"</p>
<p>"há filmes bons, ruins e medianos, mas este em questão é o pior de todos, tão chato e cansativo que todos dormem aos primeiros minutos"</p>
</blockquote>
<p>É bem claro que apesar do sentido do texto ser o mesmo, a importância dada à palavra "filme" seria diferente. E de fato, o TF-IDF funciona melhor para textos que seguem as regras de coesão e coerência, então vamos usar publicações da wikipédia.</p>
<p>Apesar do cálculo ser bastante simples, vou preferir usar o sklearn pois neste caso o mais importante é ter uma ideia geral sobre um recurso básico e servir como uma introdução básica sobre NLP, especialmente sobre vertorização de textos</p>
<div class="section" id="tf-idf">
<h2>TF-IDF</h2>
<p>Como quase tudo no sklearn...</p>
<table class="codetable"><tr>
<td class="linenos"><div class="linenodiv"><pre><a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-1"> 1</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-2"> 2</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-3"> 3</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-4"> 4</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-5"> 5</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-6"> 6</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-7"> 7</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-8"> 8</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-9"> 9</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-10">10</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-11">11</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-12">12</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-13">13</a>
<a href="#rest_code_d64834f8a0e44d0bbca6b89fe692778a-14">14</a></pre></div></td>
<td class="code"><pre class="code python"><a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-1"></a><span class="kn">import</span> <span class="nn">wikipedia</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-2"></a><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-3"></a><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-4"></a>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-5"></a><span class="n">stopw</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">"portuguese"</span><span class="p">)</span> <span class="o">+</span>\
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-6"></a>        <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">"english"</span><span class="p">)</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-7"></a>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-8"></a><span class="n">wikipedia</span><span class="o">.</span><span class="n">set_lang</span><span class="p">(</span><span class="s2">"pt"</span><span class="p">)</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-9"></a><span class="n">text</span> <span class="o">=</span> <span class="n">wikipedia</span><span class="o">.</span><span class="n">page</span><span class="p">(</span><span class="s2">"Alan_Turing"</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-10"></a>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-11"></a><span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="n">stopw</span><span class="p">)</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-12"></a>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-13"></a><span class="n">X</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">splitlines</span><span class="p">())</span>
<a name="rest_code_d64834f8a0e44d0bbca6b89fe692778a-14"></a><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></td>
</tr></table>
<p>Na penúltima linha usei o <cite>splitlines</cite> para dividir o texto em parágrafos, assim podemos posteriormente coletar informações sobre os termos relevantes para cada parágrafo, mas admito esta forma ser demasiadamente simplista pois neste caso acabo considerando subtítulos como parágrafos.</p>
<p>Internamente, o objeto que criamos, durante o treinamento, armazena um dicionário com as palavras e um "id", vamos usar isso para converter os termos:</p>
<pre class="code python"><a name="rest_code_1c0c7dd8f0f34b4382bcf82ab32598f5-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">X</span>
<a name="rest_code_1c0c7dd8f0f34b4382bcf82ab32598f5-2"></a><span class="o">&lt;</span><span class="mi">61</span><span class="n">x664</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s1">'&lt;class '</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="s1">'&gt;'</span>
<a name="rest_code_1c0c7dd8f0f34b4382bcf82ab32598f5-3"></a>    <span class="k">with</span> <span class="mi">862</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Row</span> <span class="n">format</span><span class="o">&gt;</span>
</pre>
<p>A matriz esparsa tem diversas vantagens quando tratamos com longos arrays rechados de zeros, talvez o produto principal nessa implementação seja exatamente essa matriz que indica em cada parágrafo quais os termos presentes e a sua frequência, que é o ponto principal do TF-IDF.</p>
<img alt="visualização da matriz resultante" src="../../images/lsa.png"><p>E é exatamente sobre essa matriz que chegamos no LSA (Latent Semantic Analysis), mas antes vamos ver quais as palavras mais relevantes do primeiro parágrafo:</p>
<pre class="code python"><a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">ft_name</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-2"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">top_tfidf</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-3"></a><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_tfidf</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-4"></a>        <span class="k">print</span><span class="p">(</span><span class="n">ft_name</span><span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-5"></a>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-6"></a><span class="n">computação</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-7"></a><span class="n">cheshire</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-8"></a><span class="n">junho</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-9"></a><span class="n">ciência</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-10"></a><span class="n">influente</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-11"></a><span class="n">algoritmo</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-12"></a><span class="n">east</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-13"></a><span class="n">lógico</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-14"></a><span class="n">desenvolvimento</span>
<a name="rest_code_a3d9e6a99fc0493bbf90fa9e051cbffe-15"></a><span class="n">desempenhando</span>
</pre>
<p>O ft_name é a lista de termos que irá converter para string a posição do termo indicada quando ordenamos o array comtendo o valor calculado para cada termo devolvendo as respectivas posições.</p>
</div>
<div class="section" id="lsa">
<h2>LSA</h2>
<p>O LSA é nada mais que usar o <a class="reference external" href="../svd-vs-pca">SVD</a> mas em vez de diminuir as dimensões vamos manter o tamanho da matriz:</p>
<pre class="code python"><a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-2"></a><span class="p">(</span><span class="mi">61</span><span class="p">,</span> <span class="mi">664</span><span class="p">)</span>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-3"></a>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-4"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">lsa</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">61</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-5"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">lsa</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-6"></a><span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="s1">'randomized'</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">61</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<a name="rest_code_1d5acbe72ad249c1818099ed5f52deb1-7"></a>   <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre>
<p>O real poder do LSA vem desse tratamento dado à matriz formada a partir do TF-IDF, o código abaixo indica as palavras mais relevantes para cada parágrafo:</p>
<pre class="code python"><a name="rest_code_83e0731111324c66915b3bc47e48b73f-1"></a><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">comp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lsa</span><span class="o">.</span><span class="n">components_</span><span class="p">):</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-2"></a>    <span class="n">terms_in_comp</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ft_name</span><span class="p">,</span> <span class="n">comp</span><span class="p">)</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-3"></a>    <span class="n">sorted_terms</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">terms_in_comp</span><span class="p">,</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-4"></a>                          <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-5"></a>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-6"></a>    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"paragrafo: {i}"</span><span class="p">)</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-7"></a>    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">sorted_terms</span><span class="p">:</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-8"></a>        <span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<a name="rest_code_83e0731111324c66915b3bc47e48b73f-9"></a>    <span class="k">print</span><span class="p">(</span><span class="s2">"-"</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
</pre>
<p>Pegando apenas o parágrafo 0, o resultado que temos é:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name">
<col class="field-body">
<tbody valign="top"><tr class="field">
<th class="field-name">paragrafo 0:</th>
<td class="field-body"><ul class="first last simple">
<li>turing</li>
<li>máquina</li>
<li>alan</li>
<li>prêmio</li>
<li>memorial</li>
<li>guerra</li>
<li>enigma</li>
<li>bletchley</li>
<li>park</li>
<li>computação</li>
</ul></td>
</tr></tbody>
</table>
</div>
<div class="section" id="off-topic">
<h2>off-topic</h2>
<ol class="arabic simple">
<li>E para gerar estatísticas de relevância de um texto inteiro, basta não dividir em parágrafos</li>
<li>E para gerarmos aquele bag of words que está na moda temos algumas opções, dependendo do caso aplicamos só o <strong>TF</strong> para gerar um ranking, para outros casos o <strong>TF-IDF</strong> funciona melhor, especialmente quando juntamos vários textos como uma análise geral de várias páginas de blogs, o LSA tende a ser melhor em usos mais específicos porém nada impede de usa-lo para gerar o ranking de termos para um livro, por exemplo.</li>
</ol>
<div class="notebook">
    <a class="notebook-link" href="../../files/estatistica-tf-idf-e-lsa.ipynb">code</a>
</div>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/utils/" rel="tag">utils</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../word2vec-3-skip-gram/" rel="prev" title="word2vec 3: skip-gram">Post anterior</a>
            </li>
            <li class="next">
                <a href="../distancia-euclidiama-vs-similaridade-de-cossenos/" rel="next" title="Distância Euclidiama vs Similaridade de Cossenos">Próximo post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comentários</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="demacdolincoln",
            disqus_url="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/",
        disqus_title="Estat\u00edstica: TF-IDF e LSA",
        disqus_identifier="cache/posts/estatistica-tf-idf-e-lsa.html",
        disqus_config = function () {
            this.language = "pt_br";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="demacdolincoln";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script></main><footer id="footer"><p>Contents © 2018         <a href="mailto:demacdolincoln@gmail.com">Lincoln de Macêdo</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer>
</div>
    
    

    
    
    
</body>
</html>
