<!DOCTYPE html>
<html prefix="" lang="pt_br">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>SVD vs PCA | Anotações sobre NLP</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="../../assets/css/screen.css">
<link rel="stylesheet" type="text/css" href="../../assets/css/nav.css">
<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,400%7CInconsolata">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://demacdolincoln.github.io/anotacoes-nlp/pages/svd-vs-pca/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><script src="https://cdn.jsdelivr.net/npm/vega@4.4.0"></script><script src="https://cdn.jsdelivr.net/npm/vega-lite@2.6.0"></script><script src="https://cdn.jsdelivr.net/npm/vega-embed@3.24.2"></script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/languages/python.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/languages/julia.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script><meta name="author" content="Lincoln de Macêdo">
<meta property="og:site_name" content="Anotações sobre NLP">
<meta property="og:title" content="SVD vs PCA">
<meta property="og:url" content="http://demacdolincoln.github.io/anotacoes-nlp/pages/svd-vs-pca/">
<meta property="og:description" content="Não vou tratar aqui de como se implementa o PCA e o SVD, prefiro indicar esses tutoriais abaixo, eles foram muito bem escritos e são muito claros sobre como são os cálculos usados:

tutorial PCA
tutor">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-12-07T01:26:29-03:00">
<meta property="article:tag" content="utils">
</head>
<body class="nav-closed">

<div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
<li class="nav-opened" role="presentation">
            <a href="../../archive.html">Arquivo</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../../categories/">Etiqueta</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../../rss.xml">Feed RSS</a>
        </li>
    
    
    </ul>
</div>
<span class="nav-cover"></span>

<div class="site-wrapper">
    <header class="main-header post-head no-cover"><nav class="main-nav overlay clearfix"><a class="blog-logo" href="http://demacdolincoln.github.io/anotacoes-nlp/"><img src="#" alt="Blog Logo"></a>
            <a class="menu-button" href="#"><span class="burger">☰</span><span class="word">Menu</span></a>
        </nav></header><main id="content" class="content" role="main"><article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">SVD vs PCA</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>Não vou tratar aqui de como se implementa o PCA e o SVD, prefiro indicar esses tutoriais abaixo, eles foram muito bem escritos e são muito claros sobre como são os cálculos usados:</p>
<ul class="simple">
<li><a class="reference external" href="https://sebastianraschka.com/Articles/2014_pca_step_by_step.html">tutorial PCA</a></li>
<li><a class="reference external" href="https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/">tutorial SVD</a></li>
</ul>
<p>Embora esses métodos possam ser usados para compressão de dados, análises populacionais e uma infinidade de análises envolvendo dados organizados em matrizes, aqui prefiro comparar cada método e discutir o uso voltado à redução de dimensões a fim que possamos visualizar os dados dessas anotações,</p>
<p>mas antes de chegar nas discussões, vamos ver alguns gráficos mostrando o que o SVD e o PCA retornaram quando os usamos para reduzir dimensões de matrizes:</p>
<img alt="/images/svd_pca_0_3d.png" src="../../images/svd_pca_0_3d.png"><img alt="/images/svd_pca_1_3dreduction.png" src="../../images/svd_pca_1_3dreduction.png"><p>curiosamente vemos que ocorreu uma rotação no gráfico do PCA e que o gráfico do SVD mantém uma certa similaridade visual com o gráfico original em 3D. Só compreendi melhor vendo <a class="reference external" href="https://www.quora.com/What-is-the-difference-between-PCA-and-SVD/answer/Adarsh-131">esta resposta no Quora</a>:</p>
<blockquote>
<p>"Geometrically PCA corresponds to “centering the dataset”, and then rotating it to align the axis of highest variance with the principle axis."</p>
<p><em>Geometricamente, PCA corresponde a "centralização do dataset", e depois rotaciona para alinhar o eixo de maior variância com o eixo principal</em></p>
</blockquote>
<p>Lógico que nem sempre acontece de ambos as representações ficarem tão diferentes, para observar melhor isso resolvi seguir um <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py">exemplo da documentação do sklearn</a></p>
<img alt="/images/svd_pca_2_64reduction.png" src="../../images/svd_pca_2_64reduction.png"><p>A imagem acima mostra que deve ter coincidido a forma como o SVD reduziu as dimensões e a rotação feita pelo PCA, só lembrando o que está de forma muito explícita no link para o Quora: o PCA usa o SVD para criar um ranking, afinal PCA significa "análise do componente principal" e o SVD fornece um dos passos para chegar ao componente pricipal.</p>
<p>Mas o KMeans realiza um aprendizado não supervisionado, e ainda especialmente neste caso onde a redução de 64 dimensões para 2 com certeza não deu margem para que os dados fossem linearmente separáveis, resolvi usar o SVM para desenhar o espaço para cada classe.</p>
<img alt="/images/svd_pca_3_svm.png" src="../../images/svd_pca_3_svm.png"><p>Algo que se deve ressaltar no gráfico acima é que os pontos semi-transparentes que adicionei ao gráfico são os que os classificadores treinados erraram, sobre isso repare no resultado abaixo:</p>
<pre class="code text"><a name="rest_code_17a82e4926234428a965d216f63a974d-1"></a>erros SVD: 704 de 1797
<a name="rest_code_17a82e4926234428a965d216f63a974d-2"></a>erros PCA: 704 de 1797
<a name="rest_code_17a82e4926234428a965d216f63a974d-3"></a>erros normal: 0 de 1797
<a name="rest_code_17a82e4926234428a965d216f63a974d-4"></a>-----------------------
<a name="rest_code_17a82e4926234428a965d216f63a974d-5"></a>percentuais de acertos:
<a name="rest_code_17a82e4926234428a965d216f63a974d-6"></a>&gt; SVD: 60.824%
<a name="rest_code_17a82e4926234428a965d216f63a974d-7"></a>&gt; pca: 60.824%
<a name="rest_code_17a82e4926234428a965d216f63a974d-8"></a>&gt; normal: 100.000%
</pre>
<p>Considerei "normal" como a aplicação do SVM sem reduzir as dimensões. Estes resultados mostram que a sobreposição de dados na redução de dimensões assim como a distorção que ocorre nas transformações feitas com as matrizes, tende a dificultar o trabalho dos algoritmos, mesmo mantendo um certo nível de fidelidade com a distribuição original dos dadosm o melhor é usar essa redução mais para visualizar do que para aplicar métricas ou classificadores, e por isso também que nas notas onde uso distância euclidiana e similaridade de cossenos, ao reduzir as dimensões os resultados parecem errados ainda que nas dimensões originais esteja correto.</p>
</div>
    </div>
    

</article></main><footer class="site-footer clearfix"><section class="poweredby">Contents © 2018         <a href="mailto:demacdolincoln@gmail.com">Lincoln de Macêdo</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </section></footer>
</div>

    <script type="text/javascript" src="../../assets/js/jquery.js"></script><script type="text/javascript" src="../../assets/js/jquery.fitvids.js"></script><script type="text/javascript" src="../../assets/js/index.js"></script>
</body>
</html>
