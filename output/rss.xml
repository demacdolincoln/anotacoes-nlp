<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Anotações sobre NLP</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/</link><description>Uma pequena ajuda mais prática que técnica ou teórica para quem está aprendendo sobre NLP</description><atom:link href="http://demacdolincoln.github.io/anotacoes-nlp/posts/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>pt_br</language><copyright>Contents © 2018 &lt;a href="mailto:demacdolincoln@gmail.com"&gt;Lincoln de Macêdo&lt;/a&gt; </copyright><lastBuildDate>Tue, 25 Dec 2018 16:19:27 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Classificação 2: CNN</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-2-cnn/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Como falado &lt;cite&gt;anteriormente &amp;lt;link://filename/posts/classificacao-1.rst&amp;gt;_&lt;/cite&gt;, classificar um texto é algo que vai além do vocabulário ainda que a gente utilize o word2vec ou o glove, e como a ordem das palavras importa muito, vamos dar aqui o 1º passo neste sentido, vamos ver como funciona uma rede neural convolucional (CNN) aplicada à classificação de textos.&lt;/p&gt;
&lt;p&gt;Habitualmente elas são usadas essencialmente em processamento de imagens, a idéia é bastante simples: ter uma matriz maior e calcular uma matriz menor equivalente, neste processo há perda de dados e portanto é irreversível, porém tem se demonstrado muito útil em muitos casos.&lt;/p&gt;
&lt;div class="section" id="implementacao"&gt;
&lt;h2&gt;Implementação:&lt;/h2&gt;
&lt;p&gt;O primeiro passo é transformar um texto numa matriz, para isso vamos recordar o que temos:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;texto ~&amp;gt; sequência de ids de palavras&lt;/li&gt;
&lt;li&gt;skip-gram, cbow, glove, etc. ~&amp;gt; representação cartesiana de palavras segundo o sentido compreendido pelo seu uso&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Diante disso se torna meio lógico fazer uma matriz no formato &lt;cite&gt;words x embedding dims&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;Um dos problemas dessa abordagem é que frases tem tamanhos variáveis enquanto a matriz de entrada na camada convolucional da CNN precisa ter tamanho fixo, então temos de lidar sempre com o pior caso, frases grandes, e preencher o espaço restante das frases menores com zeros, isso acaba nos obrigando ter um custo computacional extra já que teremos muitos espaços em branco só para que sempre tenhamos matrizes do mesmo tamanho.&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="leituras-recomendadas"&gt;
&lt;h2&gt;Leituras recomendadas:&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://arxiv.org/abs/1408.5882"&gt;https://arxiv.org/abs/1408.5882&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>classificação</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-2-cnn/</guid><pubDate>Tue, 25 Dec 2018 08:17:55 GMT</pubDate></item><item><title>GRU e LSTM</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;obs: Não vou me demorar tratando de questões teóricas sobre as RNN em si, já que o foco dessas anotações é NLP, futuramente, ao concluir essas anotações talvez eu inicie uma série mais abrangente sobre machine learning, mas por enquanto apenas traterei de assuntos teóricos relativos a NLP e ao resto apenas uma abordagem prática.&lt;/p&gt;
&lt;p&gt;Parece meio óbvio dizer mas o que define uma rede neural recorrente é exatamente a recorrência, isto é, informações que são armazenadas e depois reutilizadas, a forma mais simples de implementação consiste em criar uma camada (um array) que armazena os resultados e é utilizado normalmente no processamento dos dados que passam por uma rede neural, só que com 1 caracérística distinta: haver uma condição ou não para atualizar esses dados aprendidos ao longo do treinamento e uma condição que a informação armazenada seja utilizada.&lt;/p&gt;
&lt;p&gt;Devido a esse mecanismo ser claramente um gerenciamento de memória e estarmos tratando de redes neurais, não demora muito para começarmos a associar ao modo como nosso cérebro gerencia a memória, desta forma vem ao mundo em 1997 o LSTM (Long Short-Term Memory) &lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/#id3"&gt;&lt;span class="problematic" id="id1"&gt;[1]_&lt;/span&gt;&lt;/a&gt; &lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/#id4"&gt;&lt;span class="problematic" id="id2"&gt;[2]_&lt;/span&gt;&lt;/a&gt;, e como o nome bem indica, se trata de gerenciamento de memória de curto e longo prazo, mas como ele faz isso?&lt;/p&gt;
&lt;div class="section" id="portoes"&gt;
&lt;h2&gt;Portões&lt;/h2&gt;
&lt;img alt="https://commons.wikimedia.org/wiki/File:The_LSTM_cell.png#/media/File:The_LSTM_cell.png" src="https://commons.wikimedia.org/wiki/File:The_LSTM_cell.png#/media/File:The_LSTM_cell.png"&gt;
&lt;p&gt;Traduzindo as funções expostas na imagem acima disponível na wikipedia representando uma célula de memória LTSM:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\sigma = \frac{1}{1 + exp(-x)}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
tanh = \frac{e^x - e^-x}{e^x + e^{-x}}
\end{equation*}
&lt;/div&gt;
&lt;img alt="/images/lstm_gru-tanh-sigmoid.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/lstm_gru-tanh-sigmoid.png"&gt;
&lt;p&gt;Vemos que a diferença entre os gráficos dessas funções é essencialmente o limite quando tende a menos infinito, e isso faz toda a diferença, pois um limite que tende a 0 significa que posteriormente qualquer número multiplicado por 0 será 0, neste caso a função sigmoide indica a relevância de cada dimensão de entrada, se a dimensão for próxima a zero, ela vai perdendo relevância até desaparecer ou ser substituída por outra informação mais relevante (decidida pela função tanh).&lt;/p&gt;
&lt;p&gt;Todo o mecanismo de preservação e esquecimento desses métodos se baseia nesses "portões" que é como são chamadas as camadas com a função sigmoide, enquanto a função tanh tem o dever de fazer as escolhas finais, repare nas somas e multiplicações que unem o fluxo da saída com a camada oculta que armazena a memória, como a última estapa das operações com o estado da célula é sempre uma soma e os anteriores são multiplicações, isso revela a alteração dos pesos para definir a importância de cada dimensão e posteriormente a atualização mantendo assim para a época atual do treinamento da rede neural, a memória de curto prazo (os espaços próximos a zero que após a soma se mantém próximos aos resultados mais recentes) e a memória de longo prazo (o conteúdo mais relevante deixado mais próximo de 1)&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="artigos-e-links-recomendados"&gt;
&lt;h2&gt;artigos e links recomendados&lt;/h2&gt;
&lt;p&gt;[1] &lt;cite&gt;Learning to Forget: Continual Prediction with LSTM ( Felix A. Gers , Jürgen Schmidhuber , Fred Cummins ) &amp;lt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.5709&amp;gt;&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;cite&gt;Long short-term memory (Sepp Hochreiter; Jürgen Schmidhuber) &amp;lt;https://www.researchgate.net/profile/Sepp_Hochreiter/publication/13853244_Long_Short-term_Memory/links/5700e75608aea6b7746a0624/Long-Short-term-Memory.pdf?origin=publication_detail&amp;gt;&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;cite&gt;Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation ( Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua) &amp;lt;https://arxiv.org/pdf/1406.1078v3.pdf&amp;gt;&lt;/cite&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="system-messages section"&gt;
&lt;h2&gt;Docutils System Messages&lt;/h2&gt;
&lt;div class="system-message" id="id3"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;tt class="docutils"&gt;&amp;lt;string&amp;gt;&lt;/tt&gt;, line 6); &lt;em&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
Unknown target name: "1".&lt;/div&gt;
&lt;div class="system-message" id="id4"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;tt class="docutils"&gt;&amp;lt;string&amp;gt;&lt;/tt&gt;, line 6); &lt;em&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/#id2"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
Unknown target name: "2".&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/gru-e-lstm/</guid><pubDate>Mon, 24 Dec 2018 05:13:54 GMT</pubDate></item><item><title>Seq2Seq - Implementação</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-implementacao/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;p&gt;Escreva o seu comentário aqui.&lt;/p&gt;</description><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-implementacao/</guid><pubDate>Mon, 24 Dec 2018 05:13:25 GMT</pubDate></item><item><title>Seq2Seq - Introdução</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-introducao/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Tenho certeza que todos ao menos uma vez se perguntaram, pelo menos nas primeiras vezes que usaram o google translate, "como é que isso funciona? é magica?", até mesmo pelo que escrevi aqui até agora, todos os conteúdos estão bastante distantes de algo que trate tão intensamente com linguagem do que o desta anotação. O seq2seq nos permite criar redes que aprendam a sequência em que as palavras estão dispostas num texto de modo que fique fácil gerar textos, por hora, para simplificar esse assunto bastante extendo, traterei aqui apenas de explicar cada passo praticamente sem o código e na próxima anotação terá uma implementação completa.&lt;/p&gt;
&lt;div class="section" id="encoder-decoder"&gt;
&lt;h2&gt;encoder - decoder&lt;/h2&gt;
&lt;p&gt;O grande "truque" está no mecanismo de codificação-decodificação, na prática são 2 redes neurais recorrentes bem simples que compartilham uma mesma camada oculta e não tem camada de ativação, só uma célula &lt;cite&gt;GRU ou LSTM &amp;lt;link://filename/posts/gru-e-lstm.rst&amp;gt;_&lt;/cite&gt; que realiza o processamento.&lt;/p&gt;
&lt;p&gt;A informação que dá sentido à ambas as redes é a camada oculta, é sobre ela que incide o treinamento, portanto essa é a camada responsável por fazer a relação entre as saídas de cada rede neural.&lt;/p&gt;
&lt;p&gt;Mas por não haver uma classificação, precisaremos de mais uma rede neural (3 até agora) que será como a rede de decodificação mas com funções de ativação para que sobre ela seja realizado o treinamento da camada oculta.&lt;/p&gt;
&lt;p&gt;Então o que temos até o momento é:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;encoder:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;gru&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;decoder:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;gru&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;att decoder:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;gru&lt;/li&gt;
&lt;li&gt;função linear&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="treinamento"&gt;
&lt;h2&gt;Treinamento&lt;/h2&gt;
&lt;p&gt;Como já disse que tudo está em torno da camada oculta compartilhada, é criando este array que se inicia o treinamento, que incide mais sobre a camada de decodificação que sobre a de encodificação, é feito desse modo pela camada de decodificação ser a usada para calcular a perda já que é ela que nos fornecerá a saída final do algoritmo.&lt;/p&gt;
&lt;p&gt;Procedimento:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;hidden layer&lt;/li&gt;
&lt;li&gt;encoder_output, hidden_layer = encoder(input, hidden_layer)&lt;/li&gt;
&lt;li&gt;decoder_output, hidden_layer = decoder(encoder_output, hidden_layer)&lt;/li&gt;
&lt;li&gt;loss(decoder_output, target)&lt;/li&gt;
&lt;li&gt;backward&lt;/li&gt;
&lt;li&gt;step&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Na próxima anotação sobre o seq2seq, diante do código, tudo ficará mais claro.&lt;/p&gt;
&lt;p&gt;Resolvi não colocar imagens ilustrativas aqui pois no 1º link das leituras recomendadas há um monte de animações explicando bem detalhadamente todo o processo, das 2 leituras recomendadas, essa é a que mais recomendo.&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="leituras-recomendadas"&gt;
&lt;h2&gt;leituras recomendadas&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"&gt;http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://google.github.io/seq2seq/"&gt;https://google.github.io/seq2seq/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>modelagem</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/seq2seq-introducao/</guid><pubDate>Mon, 24 Dec 2018 05:13:03 GMT</pubDate></item><item><title>Classificação 1</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-1/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;O problema fundamental da classificação de textos reside no fato da dificuldade de representar o texto a nível numérico, ainda que o word2vec, o glove, o seq2seq sejam realmente úteis assim como vários outros algoritmos que não incluí aqui mas que são facilmente encontrados em buscas no google, eles por si só não conseguem ir além da representação semântica ou de alguma outra lógica sobre as palavras, tanto para gerar textos quanto para classificá-los precisamos do auxílio de outros algoritmos. O objetivo desta anotação é identificar os desafions inerentes a esta tarefa.&lt;/p&gt;
&lt;div class="section" id="apenas-vocabulario"&gt;
&lt;h2&gt;Apenas vocabulário&lt;/h2&gt;
&lt;p&gt;Esse seria o caminho mais óbvio, tendo em vista que temos uma representação espacial da disposição das palavras num hiperplano, então faz sentido imaginar que textos sobre diferentes assuntos necessariamente tem diferentes vocabulários. Façamos um teste:&lt;/p&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;peguei 2 textos da wikipedia:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol class="loweralpha simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://pt.wikipedia.org/wiki/Lutefisk"&gt;Lutefisk&lt;/a&gt; - um prato da culinária norueguesa&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://pt.wikipedia.org/wiki/Erhu"&gt;Erhu&lt;/a&gt; - um instrumento tradicional chinês&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;fiz o pré-processamento das palavras como já descrito em outro post, ficando apenas com o vocabulário&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;peguei as posições correspondentes a cada palavra no skip-gram já treinado&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;reduzi as dimensões e plotei o gráfico&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;img alt="/images/classificacao_1_scatter_voc.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/classificacao_1_scatter_voc.png"&gt;
&lt;p&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-1/#id1"&gt;&lt;span class="problematic" id="id2"&gt;**&lt;/span&gt;&lt;/a&gt;explicando as cores:&lt;/p&gt;
&lt;div class="system-message" id="id1"&gt;
&lt;p class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;&amp;lt;string&amp;gt;&lt;/tt&gt;, line 19); &lt;em&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-1/#id2"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
Inline strong start-string without end-string.&lt;/div&gt;
&lt;!-- vermelho: vocabulário do texto 1 --&gt;
&lt;!-- ciano: vocabulário do texto 2 --&gt;
&lt;!-- branco: vocabulário em comum a ambos --&gt;
&lt;p&gt;Como é possível perceber acima, não identificamos um nível de separação consistente entre os vocabulários, olhando a densidade de concentração do vocabulário nos dois textos, descartando as palavras em comum, temos a imagem abaixo:&lt;/p&gt;
&lt;img alt="/images/classificação_1_kde_voc.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/classifica%C3%A7%C3%A3o_1_kde_voc.png"&gt;
&lt;p&gt;Os centros estão muito próximos, ou seja, mesmo identificando as regiões mais densas nos vocabulários, ao tentar usar esta região como métrica, a similaridade de sentido entre os termos para o texto 1 e para o texto 2 continuam muito próximas tornando essa estratégia bem ineficaz.&lt;/p&gt;
&lt;p&gt;A solução está na compreensão que um texto não são apenas palavras soltas, mas o sentifo extrapola a simples junção de palavras, então a ordem do que está escrito importa, a disposição das palavras no texto importa muito.&lt;/p&gt;
&lt;p&gt;Nas próximas anotações abordarei sobre o uso de redes convolucionais e redes neurais recorrentes, diferentes formas de tentar burlar as dificuldades aqui apresentadas.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/classificacao-1/</guid><pubDate>Mon, 24 Dec 2018 05:12:26 GMT</pubDate></item><item><title>Distância Euclidiama vs Similaridade de Cossenos</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Indo direto ao ponto a principal diferença entre os cálculos é que enquanto na distância euclidiana é como se fizéssemos uma medição com uma régua entre 2 pontos, na similaridade de cossenos analisamos a distância angular entre 2 pontos a partir da origem, isso ficará mais claro no gráfico perto do final desta anotação.&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
dist\_eucl = \sqrt{\sum{(a-b)^2}}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
cosine\_sim = \frac{\sqrt{\sum{a * b}}}{\sqrt{\sum{a^2}} * \sqrt{\sum{b^2}}}
\end{equation*}
&lt;/div&gt;
&lt;div class="section" id="comparando-resultados"&gt;
&lt;h2&gt;Comparando resultados&lt;/h2&gt;
&lt;p&gt;Primeiro vamos implementar cada cálculo e depois uma função que receba uma matriz, normalize os dados, e indike os "k" pontos mais próximos a alguma coordenada que a gente escolher. Como usaremos em outras anotações, escrevi mais linhas do que um código simples e didático deveria ter:&lt;/p&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-12"&gt;12&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-13"&gt;13&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-14"&gt;14&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-15"&gt;15&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-16"&gt;16&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-17"&gt;17&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-18"&gt;18&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-19"&gt;19&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-20"&gt;20&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-21"&gt;21&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-22"&gt;22&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-23"&gt;23&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-24"&gt;24&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-25"&gt;25&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-26"&gt;26&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-27"&gt;27&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/#rest_code_7361953c4e44408fbac34e1192d4a6f5-28"&gt;28&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-1"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.spatial.distance&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;euclidean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cosine&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"cos"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s2"&gt;"coord"&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"coord"&lt;/span&gt;&lt;span class="p"&gt;]])))&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-12"&gt;&lt;/a&gt;        &lt;span class="n"&gt;ata_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;coord_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-14"&gt;&lt;/a&gt;    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;data_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-16"&gt;&lt;/a&gt;        &lt;span class="n"&gt;coord_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"pos"&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-18"&gt;&lt;/a&gt;    &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-19"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data_norm&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-20"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s2"&gt;"cos"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-21"&gt;&lt;/a&gt;            &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cosine&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-22"&gt;&lt;/a&gt;        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-23"&gt;&lt;/a&gt;            &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;euclidean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-24"&gt;&lt;/a&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-25"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s2"&gt;"cos"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-26"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-27"&gt;&lt;/a&gt;    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_7361953c4e44408fbac34e1192d4a6f5-28"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Visualizando a diferença de resultados entre as medições, gerei esse gráfico abaixo:&lt;/p&gt;
&lt;a class="reference external image-reference" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/eucl_vs_cos.png"&gt;&lt;img alt="/images/eucl_vs_cos.thumbnail.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/eucl_vs_cos.thumbnail.png" style="width: 500px;"&gt;&lt;/a&gt;
&lt;p&gt;explicando: os pontos vermelhos representam os pontos mais próximos desse ponto amarelo cortado por uma seta são os pontos mais próximos considerando a distância euclidiana, os pontos azuis é pela similaridade de cossenos e os roxos são os que as duas métricas coincidem ao listar os mais próximos, a seta indica a inclinação do ponto amarelo em relação a origem, e é isso que a similaridade de cossenos leva em consideração, perceba que um dos pontos azuis ficou bem distante mas projetando a seta vemos que se mantém mais próximo ao ângulo do ponto amarelo que o ponto vemelho.&lt;/p&gt;
&lt;p&gt;O motivo de preferirmos usar a similaridade de cossenos a usar distância euclidiana ou outras métricas para medir distâncias é que quando trabalhamos com NLP e ainda mais quando fazemos uma redução de dimensionalidade (onde ficou claro que há rotação e distorção) os ângulos ficam mais bem preservados que as distâncias.&lt;/p&gt;
&lt;p&gt;obs: É muito comum a similaridade é calculada com 1 passo a mais do que o demonstrado aqui, a distância angular é dada por:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
dist\_angular = \frac{cos^-1(cos\_similarity)}{\pi}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
angular\_similarity = 1-dist\_angular
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Outras vezes apenas fazem &lt;strong&gt;1-similaridade&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/knn_eucl_cos.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>utils</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/distancia-euclidiama-vs-similaridade-de-cossenos/</guid><pubDate>Fri, 07 Dec 2018 07:04:17 GMT</pubDate></item><item><title>Estatística: TF-IDF e LSA</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Antes da popularidade de métodos baseados em IA, muito também devido à capacidade dos computadores da época, o que restava para análises de texto era quantificar as palavras e buscar extrair estatísticas, o mais básico e fundamental talvez seja o TF-IDF e por isso este post.&lt;/p&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name"&gt;
&lt;col class="field-body"&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;tf-idf:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;em&gt;frequency-inverse document frequency&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Este método se resume a contar a frequência de uso de palavras e realizar um cálculo que gere uma estimativa de uso/importância da palavra no texto, de certa forma ele se conecta à &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law"&gt;Lei de Zipf&lt;/a&gt; que trata justamente de uma análise da frequência de palavras.&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
TF(t) = \frac{nº\ de\ vezes\ que\ t\ aparece\ no\ texto}{total\ de\ termos\ no\ texto}
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
IDF(t) = log_e(\frac{quantidade\ total\ de\ textos}{numero\ de\ textos\ em\ que\ t\ aparece})
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Recomendo bastante a wikipédia em inglês, há bastante exemplos de cálculos variantes: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Logicamente há inconsistências, afinal apenas a frequência não alcança o uso das palavras, não indica necessariamente as mais significativas se uma pessoa em vez de fazer referências a uma palavra ficar repetindo a mesma coisa o tempo todo. ex.:&lt;/p&gt;
&lt;!--  --&gt;
&lt;blockquote&gt;
&lt;p&gt;"há filmes bons, ruins e medianos, mas o filme em questão é o pior de todos, o filme é tão chato e cansativo que todos dormem assistindo os primeiros minutos do filme"&lt;/p&gt;
&lt;p&gt;"há filmes bons, ruins e medianos, mas este em questão é o pior de todos, tão chato e cansativo que todos dormem aos primeiros minutos"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;É bem claro que apesar do sentido do texto ser o mesmo, a importância dada à palavra "filme" seria diferente. E de fato, o TF-IDF funciona melhor para textos que seguem as regras de coesão e coerência, então vamos usar publicações da wikipédia.&lt;/p&gt;
&lt;p&gt;Apesar do cálculo ser bastante simples, vou preferir usar o sklearn pois neste caso o mais importante é ter uma ideia geral sobre um recurso básico e servir como uma introdução básica sobre NLP, especialmente sobre vertorização de textos&lt;/p&gt;
&lt;div class="section" id="tf-idf"&gt;
&lt;h2&gt;TF-IDF&lt;/h2&gt;
&lt;p&gt;Como quase tudo no sklearn...&lt;/p&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-12"&gt;12&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-13"&gt;13&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/#rest_code_34e6594eb158474ab1576df2ac03ec24-14"&gt;14&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-1"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;wikipedia&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.corpus&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;stopwords&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-3"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;stopw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stopwords&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"portuguese"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;\
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stopwords&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"english"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-8"&gt;&lt;/a&gt;&lt;span class="n"&gt;wikipedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_lang&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"pt"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-9"&gt;&lt;/a&gt;&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;wikipedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Alan_Turing"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;tfidf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stop_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;stopw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-13"&gt;&lt;/a&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tfidf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitlines&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;a name="rest_code_34e6594eb158474ab1576df2ac03ec24-14"&gt;&lt;/a&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Na penúltima linha usei o &lt;cite&gt;splitlines&lt;/cite&gt; para dividir o texto em parágrafos, assim podemos posteriormente coletar informações sobre os termos relevantes para cada parágrafo, mas admito esta forma ser demasiadamente simplista pois neste caso acabo considerando subtítulos como parágrafos.&lt;/p&gt;
&lt;p&gt;Internamente, o objeto que criamos, durante o treinamento, armazena um dicionário com as palavras e um "id", vamos usar isso para converter os termos:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_13bdd8d59ba844dcbc93ff2b1ac1af6c-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;
&lt;a name="rest_code_13bdd8d59ba844dcbc93ff2b1ac1af6c-2"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="n"&gt;x664&lt;/span&gt; &lt;span class="n"&gt;sparse&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt; &lt;span class="s1"&gt;'&amp;lt;class '&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="s1"&gt;'&amp;gt;'&lt;/span&gt;
&lt;a name="rest_code_13bdd8d59ba844dcbc93ff2b1ac1af6c-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mi"&gt;862&lt;/span&gt; &lt;span class="n"&gt;stored&lt;/span&gt; &lt;span class="n"&gt;elements&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;Compressed&lt;/span&gt; &lt;span class="n"&gt;Sparse&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt; &lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;A matriz esparsa tem diversas vantagens quando tratamos com longos arrays rechados de zeros, talvez o produto principal nessa implementação seja exatamente essa matriz que indica em cada parágrafo quais os termos presentes e a sua frequência, que é o ponto principal do TF-IDF.&lt;/p&gt;
&lt;img alt="visualização da matriz resultante" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/lsa.png"&gt;
&lt;p&gt;E é exatamente sobre essa matriz que chegamos no LSA (Latent Semantic Analysis), mas antes vamos ver quais as palavras mais relevantes do primeiro parágrafo:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;ft_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tfidf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_feature_names&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-2"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;top_tfidf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toarray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)[::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;top_tfidf&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-4"&gt;&lt;/a&gt;        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ft_name&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;computação&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;cheshire&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-8"&gt;&lt;/a&gt;&lt;span class="n"&gt;junho&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-9"&gt;&lt;/a&gt;&lt;span class="n"&gt;ciência&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-10"&gt;&lt;/a&gt;&lt;span class="n"&gt;influente&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;algoritmo&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-12"&gt;&lt;/a&gt;&lt;span class="n"&gt;east&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-13"&gt;&lt;/a&gt;&lt;span class="n"&gt;lógico&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-14"&gt;&lt;/a&gt;&lt;span class="n"&gt;desenvolvimento&lt;/span&gt;
&lt;a name="rest_code_160098d8b64746b1997153fbce992da6-15"&gt;&lt;/a&gt;&lt;span class="n"&gt;desempenhando&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;O ft_name é a lista de termos que irá converter para string a posição do termo indicada quando ordenamos o array comtendo o valor calculado para cada termo devolvendo as respectivas posições.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lsa"&gt;
&lt;h2&gt;LSA&lt;/h2&gt;
&lt;p&gt;O LSA é nada mais que usar o &lt;a class="reference external" href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/svd-vs-pca"&gt;SVD&lt;/a&gt; mas em vez de diminuir as dimensões vamos manter o tamanho da matriz:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-2"&gt;&lt;/a&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;664&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-4"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;lsa&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TruncatedSVD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-5"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;lsa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;TruncatedSVD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;algorithm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'randomized'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_e85c4ba69569446fa6efd42b17e1cce0-7"&gt;&lt;/a&gt;   &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;O real poder do LSA vem desse tratamento dado à matriz formada a partir do TF-IDF, o código abaixo indica as palavras mais relevantes para cada parágrafo:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;comp&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lsa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;components_&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;terms_in_comp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ft_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;comp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;sorted_terms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;terms_in_comp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-4"&gt;&lt;/a&gt;                          &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-6"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"paragrafo: {i}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sorted_terms&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_cf73a290231c404e81b2744e9deead7d-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"-"&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Pegando apenas o parágrafo 0, o resultado que temos é:&lt;/p&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name"&gt;
&lt;col class="field-body"&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;paragrafo 0:&lt;/th&gt;&lt;td class="field-body"&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;turing&lt;/li&gt;
&lt;li&gt;máquina&lt;/li&gt;
&lt;li&gt;alan&lt;/li&gt;
&lt;li&gt;prêmio&lt;/li&gt;
&lt;li&gt;memorial&lt;/li&gt;
&lt;li&gt;guerra&lt;/li&gt;
&lt;li&gt;enigma&lt;/li&gt;
&lt;li&gt;bletchley&lt;/li&gt;
&lt;li&gt;park&lt;/li&gt;
&lt;li&gt;computação&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="off-topic"&gt;
&lt;h2&gt;off-topic&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;E para gerar estatísticas de relevância de um texto inteiro, basta não dividir em parágrafos&lt;/li&gt;
&lt;li&gt;E para gerarmos aquele bag of words que está na moda temos algumas opções, dependendo do caso aplicamos só o &lt;strong&gt;TF&lt;/strong&gt; para gerar um ranking, para outros casos o &lt;strong&gt;TF-IDF&lt;/strong&gt; funciona melhor, especialmente quando juntamos vários textos como uma análise geral de várias páginas de blogs, o LSA tende a ser melhor em usos mais específicos porém nada impede de usa-lo para gerar o ranking de termos para um livro, por exemplo.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/estatistica-tf-idf-e-lsa.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>utils</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/estatistica-tf-idf-e-lsa/</guid><pubDate>Fri, 07 Dec 2018 04:47:59 GMT</pubDate></item><item><title>word2vec 3: skip-gram</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-3-skip-gram/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Como já dito antes, o skip-gram faz um treinamento meio que ao contrário do cbow, no treinamento a rede neural recebe as palavras centrais para tentar prever as palavras de contexto e assim ajusta os pesos das camadas da rede neural aproximando valores para palavras semelhantes no hiperplano.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;text_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-8"&gt;&lt;/a&gt;           &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-9"&gt;&lt;/a&gt;       &lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-11"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;center_word_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-14"&gt;&lt;/a&gt;        &lt;span class="n"&gt;context_word_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;pair_ids&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;center_word_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context_word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_d7a929b15f6a469cada19f39350511fd-16"&gt;&lt;/a&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;A única diferença do código acima para criar os pares de ids está na ordem: primeiro a palavra central e depois a palavra de contexto:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;central&lt;/th&gt;
&lt;th class="head"&gt;contexto&lt;/th&gt;
&lt;th class="head"&gt;central&lt;/th&gt;
&lt;th class="head"&gt;contexto&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;máquina&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;302&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;computação&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;máquina&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;604&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;máquina&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;409&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;td&gt;importante&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;277&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;papel&lt;/td&gt;
&lt;td&gt;desempenhando&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;O modelo da rede neural não se difere muito da usada no cbow, a única diferença fica por conta do tamanho da entrada da primeira função linear, já que passaremos 1 id por vez e não 4 como no cbow.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-3"&gt;&lt;/a&gt;        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-5"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-7"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# única diferença aqui&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-8"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-10"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LogSoftmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-16"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-19"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-20"&gt;&lt;/a&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-21"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_word_emb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word_id&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-22"&gt;&lt;/a&gt;        &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LongTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_bfb053bc8fe84bd6bc826f8dc55d430a-23"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;De modo geral o nível de erro (ou perda, nunca sei ao certo como traduzir "loss" neste contexto) no skip-gram é maior que no cbow, mas repito que o importante é que esteja havendo um aprendizado e não que a rede neural se adapte ao ponto de prever todas as palavras relacionadas ainda que ocasionalmente isso ocorra, para nós interessa o seguinte movimento: numa época a rede neural elevar os valores das palavras próximas na saída e afastar as mais distantes, assim naturalmente ela vai aprendendo a agrupar palavras em regiões de um hiperplano aproximando ou afastando de acordo com o modo como as palavras são usadas, tendendo a manter um distanciamento relacionado ao seu valor semântico.&lt;/p&gt;
&lt;img alt="/images/word2vec-skipgram-loss.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-skipgram-loss.png"&gt;
&lt;p&gt;Reduzindo as dimensões para visualizar a distribuição...&lt;/p&gt;
&lt;img alt="/images/word2vec-skipgram-1.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-skipgram-1.png" style="width: 500px;"&gt;
&lt;p&gt;Logicamente dessa forma como implementei, o custo/perda/loss é mais alto que na implementação feita do cbow, afinal vamos aos poucos ajustando 4 resultados possíveis para cada termo. Neste exemplo aumentei a quantidade de épocas para 2500 e ainda assim ficou imensamente distante do resultado da implementação do cbow neste aspecto, porém a relação entre as palavras se mostrou um pouco melhor ainda que longe do ideal.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;rank sim cos&lt;/th&gt;
&lt;th class="head"&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;
&lt;/li&gt;&lt;/ul&gt;
&lt;/th&gt;
&lt;th class="head"&gt;rank dist eucl&lt;/th&gt;
&lt;th class="head"&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;
&lt;/li&gt;&lt;/ul&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;muitos&lt;/td&gt;
&lt;td&gt;0.14544&lt;/td&gt;
&lt;td&gt;muitos&lt;/td&gt;
&lt;td&gt;0.07375&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;poderia&lt;/td&gt;
&lt;td&gt;0.26087&lt;/td&gt;
&lt;td&gt;code&lt;/td&gt;
&lt;td&gt;0.08692&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;ceruzzi&lt;/td&gt;
&lt;td&gt;0.28141&lt;/td&gt;
&lt;td&gt;ceruzzi&lt;/td&gt;
&lt;td&gt;0.08939&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;code&lt;/td&gt;
&lt;td&gt;0.28206&lt;/td&gt;
&lt;td&gt;condados&lt;/td&gt;
&lt;td&gt;0.09595&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;britânica&lt;/td&gt;
&lt;td&gt;0.28430&lt;/td&gt;
&lt;td&gt;mortem&lt;/td&gt;
&lt;td&gt;0.09709&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;mortem&lt;/td&gt;
&lt;td&gt;0.33544&lt;/td&gt;
&lt;td&gt;atos&lt;/td&gt;
&lt;td&gt;0.10284&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;condenado&lt;/td&gt;
&lt;td&gt;0.33660&lt;/td&gt;
&lt;td&gt;teórica&lt;/td&gt;
&lt;td&gt;0.10357&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;comerciantes&lt;/td&gt;
&lt;td&gt;0.33929&lt;/td&gt;
&lt;td&gt;condenado&lt;/td&gt;
&lt;td&gt;0.10376&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;cabeceira&lt;/td&gt;
&lt;td&gt;0.34548&lt;/td&gt;
&lt;td&gt;rápido&lt;/td&gt;
&lt;td&gt;0.10433&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;condados&lt;/td&gt;
&lt;td&gt;0.36041&lt;/td&gt;
&lt;td&gt;prazer&lt;/td&gt;
&lt;td&gt;0.10648&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;img alt="/images/word2vec-skipgram-rank.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-skipgram-rank.png"&gt;
&lt;p&gt;Só lembrando que segui o mesmo padrão de cores:&lt;/p&gt;
&lt;table class="docutils field-list" frame="void" rules="none"&gt;
&lt;col class="field-name"&gt;
&lt;col class="field-body"&gt;
&lt;tbody valign="top"&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;amarelo:&lt;/th&gt;&lt;td class="field-body"&gt;Palavra escolhida&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;vermelho:&lt;/th&gt;&lt;td class="field-body"&gt;Termos mais próximos pela similaridade de cossenos&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;azul:&lt;/th&gt;&lt;td class="field-body"&gt;Termos mais próximos pela distância euclidiana&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="field"&gt;&lt;th class="field-name"&gt;roxo:&lt;/th&gt;&lt;td class="field-body"&gt;Termos que ambas as métricas concordam&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/word2vec-3-skipgram.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>word2vec</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-3-skip-gram/</guid><pubDate>Fri, 07 Dec 2018 04:43:36 GMT</pubDate></item><item><title>SVD vs PCA</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/svd-vs-pca/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Não vou tratar aqui de como se implementa o PCA e o SVD, prefiro indicar esses tutoriais abaixo, eles foram muito bem escritos e são muito claros sobre como são os cálculos usados:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://sebastianraschka.com/Articles/2014_pca_step_by_step.html"&gt;tutorial PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/"&gt;tutorial SVD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Embora esses métodos possam ser usados para compressão de dados, análises populacionais e uma infinidade de análises envolvendo dados organizados em matrizes, aqui prefiro comparar cada método e discutir o uso voltado à redução de dimensões a fim que possamos visualizar os dados dessas anotações,&lt;/p&gt;
&lt;p&gt;mas antes de chegar nas discussões, vamos ver alguns gráficos mostrando o que o SVD e o PCA retornaram quando os usamos para reduzir dimensões de matrizes:&lt;/p&gt;
&lt;img alt="/images/svd_pca_0_3d.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_0_3d.png"&gt;
&lt;img alt="/images/svd_pca_1_3dreduction.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_1_3dreduction.png"&gt;
&lt;p&gt;curiosamente vemos que ocorreu uma rotação no gráfico do PCA e que o gráfico do SVD mantém uma certa similaridade visual com o gráfico original em 3D. Só compreendi melhor vendo &lt;a class="reference external" href="https://www.quora.com/What-is-the-difference-between-PCA-and-SVD/answer/Adarsh-131"&gt;esta resposta no Quora&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Geometrically PCA corresponds to “centering the dataset”, and then rotating it to align the axis of highest variance with the principle axis."&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Geometricamente, PCA corresponde a "centralização do dataset", e depois rotaciona para alinhar o eixo de maior variância com o eixo principal&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Lógico que nem sempre acontece de ambos as representações ficarem tão diferentes, para observar melhor isso resolvi seguir um &lt;a class="reference external" href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py"&gt;exemplo da documentação do sklearn&lt;/a&gt;&lt;/p&gt;
&lt;img alt="/images/svd_pca_2_64reduction.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_2_64reduction.png"&gt;
&lt;p&gt;A imagem acima mostra que deve ter coincidido a forma como o SVD reduziu as dimensões e a rotação feita pelo PCA, só lembrando o que está de forma muito explícita no link para o Quora: o PCA usa o SVD para criar um ranking, afinal PCA significa "análise do componente principal" e o SVD fornece um dos passos para chegar ao componente pricipal.&lt;/p&gt;
&lt;p&gt;Mas o KMeans realiza um aprendizado não supervisionado, e ainda especialmente neste caso onde a redução de 64 dimensões para 2 com certeza não deu margem para que os dados fossem linearmente separáveis, resolvi usar o SVM para desenhar o espaço para cada classe.&lt;/p&gt;
&lt;img alt="/images/svd_pca_3_svm.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/svd_pca_3_svm.png"&gt;
&lt;p&gt;Algo que se deve ressaltar no gráfico acima é que os pontos semi-transparentes que adicionei ao gráfico são os que os classificadores treinados erraram, sobre isso repare no resultado abaixo:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-1"&gt;&lt;/a&gt;erros SVD: 704 de 1797
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-2"&gt;&lt;/a&gt;erros PCA: 704 de 1797
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-3"&gt;&lt;/a&gt;erros normal: 0 de 1797
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-4"&gt;&lt;/a&gt;-----------------------
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-5"&gt;&lt;/a&gt;percentuais de acertos:
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-6"&gt;&lt;/a&gt;&amp;gt; SVD: 60.824%
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-7"&gt;&lt;/a&gt;&amp;gt; pca: 60.824%
&lt;a name="rest_code_27e20de9dad84d098793720e7653c63b-8"&gt;&lt;/a&gt;&amp;gt; normal: 100.000%
&lt;/pre&gt;&lt;p&gt;Considerei "normal" como a aplicação do SVM sem reduzir as dimensões. Estes resultados mostram que a sobreposição de dados na redução de dimensões assim como a distorção que ocorre nas transformações feitas com as matrizes, tende a dificultar o trabalho dos algoritmos, mesmo mantendo um certo nível de fidelidade com a distribuição original dos dadosm o melhor é usar essa redução mais para visualizar do que para aplicar métricas ou classificadores, e por isso também que nas notas onde uso distância euclidiana e similaridade de cossenos, ao reduzir as dimensões os resultados parecem errados ainda que nas dimensões originais esteja correto.&lt;/p&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/SVD-PCA.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>utils</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/svd-vs-pca/</guid><pubDate>Fri, 07 Dec 2018 04:26:29 GMT</pubDate></item><item><title>Word2Vec 2: CBOW</title><link>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/</link><dc:creator>Lincoln de Macêdo</dc:creator><description>&lt;div&gt;&lt;p&gt;Na anotação anterior vimos de forma mais ou menos prática o sentido da coisa, implementamos o Word2Vec com o objetivo de identificar a proximidade semântica entre palavras com base no uso em textos, este post é fundamentalmente teórico e a implementação do cbow aqui demonstrada está muito longe de ser algo pronto para produção, é apenas um exemplo que tenta ser didático.&lt;/p&gt;
&lt;div class="section" id="preparacao-dos-dados"&gt;
&lt;h2&gt;preparação dos dados&lt;/h2&gt;
&lt;p&gt;Como nosso objetivo é fazer com que uma rede neural receba as palavras de contexto e indique a palavra central, e na anotação anterior fiz uma pequena observação dizendo que sempre teremos $2w$ palavras de contexto para cada palavra central e assim faremos, vamos modificar um pouco o código que cria os pares do word2vec:&lt;/p&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-12"&gt;12&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_b681d05195e54117935d1c2ab3132375-13"&gt;13&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;pair_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;text_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;center_word_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;context_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word2id&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;corpus_text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;center_word&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_b681d05195e54117935d1c2ab3132375-13"&gt;&lt;/a&gt;    &lt;span class="n"&gt;pair_ids&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;context_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;center_word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Assim feito, teremos algo como:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;contexto&lt;/th&gt;
&lt;th class="head"&gt;central&lt;/th&gt;
&lt;th class="head"&gt;contexto&lt;/th&gt;
&lt;th class="head"&gt;central&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;[155, 77, 577, 495]&lt;/td&gt;
&lt;td&gt;544&lt;/td&gt;
&lt;td&gt;['armazenado', 'ace', 'turing', 'interessou']&lt;/td&gt;
&lt;td&gt;posteriormente&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[77, 544, 495, 233]&lt;/td&gt;
&lt;td&gt;577&lt;/td&gt;
&lt;td&gt;['ace', 'posteriormente', 'interessou', 'química']&lt;/td&gt;
&lt;td&gt;turing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[544, 577, 233, 308]&lt;/td&gt;
&lt;td&gt;495&lt;/td&gt;
&lt;td&gt;['posteriormente', 'turing', 'química', 'escreveu']&lt;/td&gt;
&lt;td&gt;interessou&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[577, 495, 308, 446]&lt;/td&gt;
&lt;td&gt;233&lt;/td&gt;
&lt;td&gt;['turing', 'interessou', 'escreveu', 'artigo']&lt;/td&gt;
&lt;td&gt;química&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[495, 233, 446, 537]&lt;/td&gt;
&lt;td&gt;308&lt;/td&gt;
&lt;td&gt;['interessou', 'química', 'artigo', 'sobre']&lt;/td&gt;
&lt;td&gt;escreveu&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[233, 308, 537, 323]&lt;/td&gt;
&lt;td&gt;446&lt;/td&gt;
&lt;td&gt;['química', 'escreveu', 'sobre', 'base']&lt;/td&gt;
&lt;td&gt;artigo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[308, 446, 323, 233]&lt;/td&gt;
&lt;td&gt;537&lt;/td&gt;
&lt;td&gt;['escreveu', 'artigo', 'base', 'química']&lt;/td&gt;
&lt;td&gt;sobre&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[446, 537, 233, 504]&lt;/td&gt;
&lt;td&gt;323&lt;/td&gt;
&lt;td&gt;['artigo', 'sobre', 'química', 'morfogênese']&lt;/td&gt;
&lt;td&gt;base&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[537, 323, 504, 506]&lt;/td&gt;
&lt;td&gt;233&lt;/td&gt;
&lt;td&gt;['sobre', 'base', 'morfogênese', 'previu']&lt;/td&gt;
&lt;td&gt;química&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[323, 233, 506, 492]&lt;/td&gt;
&lt;td&gt;504&lt;/td&gt;
&lt;td&gt;['base', 'química', 'previu', 'reações']&lt;/td&gt;
&lt;td&gt;morfogênese&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[233, 504, 492, 8]&lt;/td&gt;
&lt;td&gt;506&lt;/td&gt;
&lt;td&gt;['química', 'morfogênese', 'reações', 'químicas']&lt;/td&gt;
&lt;td&gt;previu&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="a-rede-neural"&gt;
&lt;h2&gt;A rede neural&lt;/h2&gt;
&lt;p&gt;O que importa na rede neural neste método e no skip-gram é a camada &lt;em&gt;Embedding&lt;/em&gt;&lt;/p&gt;
&lt;table class="codetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-1"&gt; 1&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-2"&gt; 2&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-3"&gt; 3&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-4"&gt; 4&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-5"&gt; 5&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-6"&gt; 6&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-7"&gt; 7&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-8"&gt; 8&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-9"&gt; 9&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-10"&gt;10&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-11"&gt;11&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-12"&gt;12&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-13"&gt;13&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-14"&gt;14&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-15"&gt;15&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-16"&gt;16&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-17"&gt;17&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-18"&gt;18&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-19"&gt;19&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-20"&gt;20&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-21"&gt;21&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-22"&gt;22&lt;/a&gt;
&lt;a href="http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/#rest_code_711fe1ba88cc4cbaafb806418ef3610c-23"&gt;23&lt;/a&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code python"&gt;&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-3"&gt;&lt;/a&gt;        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-5"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-7"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;emb_size&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;context_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-8"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-10"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LogSoftmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-15"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-16"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-19"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-20"&gt;&lt;/a&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-21"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_word_emb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word_id&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-22"&gt;&lt;/a&gt;        &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LongTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;word_id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_711fe1ba88cc4cbaafb806418ef3610c-23"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;O treinamento será demorado, afinal como já dito, este não é um código para produção, é apenas um código didático, então enquanto ocorre o treinamento, não é má idéia ir tomar um chá e caminhar um pouco.&lt;/p&gt;
&lt;p&gt;Algo que preciso ressaltar aqui é que predizer a palavra central corretamente não importa tanto, o importante é que esteja ocorrendo o aprendizado já queo que nos interessa é que os valores da camada incorporada se aproximem em palavras próximas e se distanciem para palavras distantes, então é de se esperar um gráfico horrível mostrando a evolução da perda.&lt;/p&gt;
&lt;img alt="/images/word2vec-cbow-loss.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-cbow-loss.png"&gt;
&lt;p&gt;Para visualizar a distribuição das palavras num plano cartesiano, faremos o mesmo que com o Gensim, usaremos a implementação do PCA disponível no slearn.&lt;/p&gt;
&lt;img alt="/images/word2vec-cbow-1.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-cbow-1.png"&gt;
&lt;p&gt;Observando a similaridade, que não é lá tão boa neste caso devido a total falta de otimização em tudo no código:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;rank sim cos&lt;/th&gt;
&lt;th class="head"&gt; &lt;/th&gt;
&lt;th class="head"&gt;rank dist eucl&lt;/th&gt;
&lt;th class="head"&gt; &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;novas&lt;/td&gt;
&lt;td&gt;0.28059&lt;/td&gt;
&lt;td&gt;novas&lt;/td&gt;
&lt;td&gt;0.09326&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;equivalia&lt;/td&gt;
&lt;td&gt;0.31309&lt;/td&gt;
&lt;td&gt;polonesa&lt;/td&gt;
&lt;td&gt;0.09989&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;pioneiro&lt;/td&gt;
&lt;td&gt;0.31798&lt;/td&gt;
&lt;td&gt;neve&lt;/td&gt;
&lt;td&gt;0.10029&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;afirma&lt;/td&gt;
&lt;td&gt;0.32445&lt;/td&gt;
&lt;td&gt;andrew&lt;/td&gt;
&lt;td&gt;0.10191&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;neve&lt;/td&gt;
&lt;td&gt;0.33447&lt;/td&gt;
&lt;td&gt;pioneiro&lt;/td&gt;
&lt;td&gt;0.10310&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;polonesa&lt;/td&gt;
&lt;td&gt;0.33585&lt;/td&gt;
&lt;td&gt;afirma&lt;/td&gt;
&lt;td&gt;0.10484&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;massachusetts&lt;/td&gt;
&lt;td&gt;0.34675&lt;/td&gt;
&lt;td&gt;conduzida&lt;/td&gt;
&lt;td&gt;0.10508&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;conduzida&lt;/td&gt;
&lt;td&gt;0.34768&lt;/td&gt;
&lt;td&gt;bombas&lt;/td&gt;
&lt;td&gt;0.10641&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;andrew&lt;/td&gt;
&lt;td&gt;0.35143&lt;/td&gt;
&lt;td&gt;manipular&lt;/td&gt;
&lt;td&gt;0.10718&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;hastings&lt;/td&gt;
&lt;td&gt;0.35665&lt;/td&gt;
&lt;td&gt;homossexuais&lt;/td&gt;
&lt;td&gt;0.11074&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Observando onde cada termo está com as dimensões da camada incorporada da rede neural reduzida a 2d temos:&lt;/p&gt;
&lt;img alt="/images/word2vec-cbow-rank.png" src="http://demacdolincoln.github.io/anotacoes-nlp/posts/images/word2vec-cbow-rank.png"&gt;
&lt;p&gt;É compreensível ver estas distâncias tão em desarcodo pelo fato das distorções da redução de dimensões, de 10 para 2.&lt;/p&gt;
&lt;div class="notebook"&gt;
    &lt;a class="notebook-link" href="http://nbviewer.jupyter.org/github/demacdolincoln/anotacoes-nlp/blob/src/files/word2vec-2-cbow.ipynb"&gt;code&lt;/a&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>word2vec</category><guid>http://demacdolincoln.github.io/anotacoes-nlp/posts/posts/word2vec-2-cbow/</guid><pubDate>Fri, 07 Dec 2018 03:23:12 GMT</pubDate></item></channel></rss>